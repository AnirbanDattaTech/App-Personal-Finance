# --- 1. File Description ---

This file provides context for LLM assistants about the 'app-personal-finance' project.
It includes:
A. A file tree showing the project structure (excluding specified cache/build/git dirs).
B. Details for key files:
   - Python (.py): Full, whitespace/comment-compressed code (docstrings preserved).
   - Essential Config/Metadata/Reference: AI summary + truncated snippet.
   - data/expenses.csv: Head rows only.
The goal is to provide meaningful context focusing on application logic, tests, essential configuration, and reference materials.

# --- 2. Folder and File structure tree ---

app-personal-finance/
    check_env.py
    requirements-v2.0.txt
    assistant/
        finance-assistant/
            .codespellignore
            .env.example
            langgraph.json
            LICENSE
            Makefile
            pyproject.toml
            README.md
            app/
                server.py
            src/
                agent/
                    __init__.py
                    configuration.py
                    graph.py
                    state.py
            static/
                studio_ui.png
            tests/
                integration_tests/
                    __init__.py
                    test_graph.py
                unit_tests/
                    __init__.py
                    test_configuration.py
    data/
        expenses.csv
        expenses.db
    metadata/
        expense_metadata.json
        expenses_metadata_detailed.yaml
    reference/
        _temp_tree.txt
        agentic_ai_guidelines.txt
        data_analysis.ipynb
        expenses_sample.csv
        generate_data.py
        generate_db.py
        generate_metadata_details.py
        generate_summary.py
        instructions_advanced_question_types.txt
        instructions_agentic_ai.txt
        instructions_code_details.txt
        instructions_metadata.txt
        test_llm.py
        test_llm_gemini.py
        test_llm_openai.py
        img/
    streamlit/
        __init__.py
        db_utils.py
        main.py
        style_utils.py
        styles.css
        tabs/
            __init__.py
            add_expense.py
            assistant.py
            reports.py
            visuals.py

# --- 3. Code file summary ---
# --- 3A. .py files (Full Compressed Code) ---

#PY File: server.py
@path: assistant/finance-assistant/app/server.py
@summary: Full Python code (compressed) included below.
@code:
import os,sys
from pathlib import Path
import logging
from typing import Optional, Any
import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel, Field
from langserve import add_routes
try:
    current_dir = Path(__file__).resolve().parent # This is the 'app' directory
    src_dir = current_dir.parent / "src"        # Go up one level to 'finance-assistant' then into 'src'
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', # Added logger name
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    logger = logging.getLogger(__name__) # Logger for this server file
    if src_dir.is_dir() and str(src_dir) not in sys.path:
        sys.path.insert(0, str(src_dir))
        logger.info(f"Added src directory to sys.path: {src_dir}")
    elif not src_dir.is_dir():
         logger.error(f"Calculated src directory does not exist or is not a directory: {src_dir}. Imports might fail.")
    else:
         logger.debug(f"Src directory already in sys.path or not added: {src_dir}")
except Exception as path_ex:
     print(f"ERROR setting up paths/logging: {path_ex}")
     sys.exit("Fatal error during initial path setup.")
try:
    from agent.graph import graph as finance_assistant_graph # Rename for clarity
    from agent.state import AgentState # Needed for confirming AgentState structure conceptually
    logger.info("Successfully imported graph and AgentState from src.agent")
except ImportError as e:
    logger.error(f"CRITICAL ERROR: Failed to import graph or AgentState from src.agent. "
                 f"Check PYTHONPATH and ensure '{src_dir}' exists and contains 'agent/'. Error: {e}", exc_info=True)
    raise ImportError("Could not import agent graph or state. Server cannot start.") from e
except Exception as e:
    logger.error(f"An unexpected error occurred during agent import: {e}", exc_info=True)
    raise # Re-raise other unexpected errors
class AssistantInput(BaseModel):
    """Input schema for the Finance Assistant Agent."""
    original_query: str = Field(..., description="The user's natural language query.")
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "original_query": "What was my total grocery spend last month?"
                }
            ]
        }
    }
class AssistantOutput(BaseModel):
    """Output schema for the Finance Assistant Agent."""
    final_response: Optional[str] = Field(None, description="The final text response generated for the user.")
    chart_json: Optional[str] = Field(None, description="Plotly chart JSON representation, if one was generated.")
    sql_query: Optional[str] = Field(None, description="The SQL query generated by the agent, if applicable.")
    error: Optional[str] = Field(None, description="Any error message captured during the agent's execution.")
    model_config = {
        "json_schema_extra": {
             "examples": [
                 {
                     "final_response": "Your total grocery spend last month was INR 5,432.10.",
                     "chart_json": "{ ...plotly json... }",
                     "sql_query": "SELECT category, SUM(amount) FROM expenses WHERE strftime('%Y-%m', date) = '2023-03' AND category = 'Grocery' GROUP BY category;",
                     "error": None
                 }
            ]
        }
    }
logger.info("Initializing FastAPI application...")
app = FastAPI(
    title="Personal Finance Assistant Agent API",
    version="0.1.1", # Bumped version for the change
    description="API endpoint for interacting with the LangGraph-based Personal Finance Assistant.",
)
@app.get("/", tags=["Health Check"])
async def read_root():
    """Simple health check endpoint."""
    logger.debug("Root endpoint '/' accessed.")
    return {"status": "ok", "message": "Finance Assistant API is running."}
logger.info(f"Adding LangServe routes for agent graph '{finance_assistant_graph.__class__.__name__}'...")
add_routes(
    app=app,
    runnable=finance_assistant_graph,    # The imported compiled LangGraph runnable
    path="/assistant",                   # Base path for agent endpoints (e.g., POST /assistant/invoke)
    input_type=AssistantInput,           # Pydantic model for input validation
    output_type=AssistantOutput,         # Pydantic model for output structuring (NOW INCLUDES sql_query)
    config_keys=["configurable"]         # Keys to make configurable at runtime (e.g., session_id, thread_id)
)
logger.info(f"LangServe routes added successfully at path '/assistant'. Playground at /assistant/playground/")
if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000)) # Allow port configuration via environment variable
    host = os.getenv("HOST", "0.0.0.0") # Default to accessible on network
    logger.info(f"Starting Uvicorn server directly on {host}:{port}...")
    logger.warning("Running directly with 'python app/server.py'. Use 'uvicorn' command or 'langgraph dev' for development features like hot-reloading.")
    uvicorn.run(app, host=host, port=port)


#PY File: __init__.py
@path: assistant/finance-assistant/src/agent/__init__.py
@summary: Full Python code (compressed) included below.
@code:
"""New LangGraph Agent.

This module defines a custom graph.
"""
from agent.graph import graph
__all__ = ["graph"]


#PY File: configuration.py
@path: assistant/finance-assistant/src/agent/configuration.py
@summary: Full Python code (compressed) included below.
@code:
"""Define the configurable parameters for the agent."""
from __future__ import annotations
from dataclasses import dataclass, fields
from typing import Optional
from langchain_core.runnables import RunnableConfig
@dataclass(kw_only=True)
class Configuration:
    """The configuration for the agent."""
    my_configurable_param: str = "changeme"
    @classmethod
    def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> Configuration:
        """Create a Configuration instance from a RunnableConfig object."""
        configurable = (config.get("configurable") or {}) if config else {}
        _fields = {f.name for f in fields(cls) if f.init}
        return cls(**{k: v for k, v in configurable.items() if k in _fields})


#PY File: graph.py
@path: assistant/finance-assistant/src/agent/graph.py
@summary: Full Python code (compressed) included below.
@code:
"""Define the graph for the finance assistant agent."""
import os
import json
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from langchain_google_genai import ChatGoogleGenerativeAI # Use Gemini
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END
from sqlalchemy import create_engine, text           # For database interaction
from pathlib import Path
import logging
from dotenv import load_dotenv
from typing import List, Dict, Any
import yaml # To load the metadata YAML
from agent.state import AgentState
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__) # Use a specific logger for this module
project_root = Path(__file__).resolve().parent.parent.parent.parent.parent
env_path = project_root / '.env'
if env_path.exists():
    load_dotenv(dotenv_path=env_path)
    logger.info(f"Loaded environment variables from: {env_path}")
else:
    logger.warning(f".env file not found at {env_path}. Relying on system environment variables.")
DB_PATH = project_root / "data" / "expenses.db"
if not DB_PATH.exists():
    logger.error(f"CRITICAL: DATABASE NOT FOUND at expected location: {DB_PATH}")
    raise FileNotFoundError(f"Database file not found at {DB_PATH}. Ensure data/expenses.db exists in the project root.")
DB_URI = f"sqlite:///{DB_PATH.resolve()}"
try:
    engine = create_engine(DB_URI) #, connect_args={"check_same_thread": False})
    logger.info(f"Database engine created for: {DB_URI}")
    with engine.connect() as conn:
         logger.info("Database connection test successful.")
except Exception as e:
    logger.error(f"Failed to create database engine or connect: {e}", exc_info=True)
    raise # Stop execution if DB setup fails
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    logger.error("CRITICAL: GOOGLE_API_KEY not found in environment variables.")
    raise ValueError("GOOGLE_API_KEY environment variable must be set in the .env file.")
try:
    LLM = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        google_api_key=google_api_key,
        temperature=0.1, # Lower temperature for more deterministic tasks initially
        convert_system_message_to_human=True # Important for Gemini compatibility
    )
    logger.info("ChatGoogleGenerativeAI model initialized (gemini-1.5-flash-latest).")
except Exception as e:
    logger.error(f"Failed to initialize ChatGoogleGenerativeAI: {e}", exc_info=True)
    raise # Stop execution if LLM setup fails
metadata_path = project_root / "metadata" / "expenses_metadata_detailed.yaml"
SCHEMA_METADATA = "" # Initialize as empty string
try:
    if metadata_path.exists():
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata_content = yaml.safe_load(f)
            SCHEMA_METADATA = json.dumps(metadata_content, indent=2)
            logger.info(f"Successfully loaded metadata from {metadata_path}")
    else:
        logger.warning(f"Metadata file not found at {metadata_path}. SQL generation accuracy may be reduced.")
        SCHEMA_METADATA = """
         Fallback Schema:
         Table: expenses
         Columns: id(TEXT PK), date(DATE 'YYYY-MM-DD'), year(INT), month(TEXT 'YYYY-MM'), week(TEXT 'YYYY-Www'), day_of_week(TEXT), account(TEXT), category(TEXT), sub_category(TEXT), type(TEXT), user(TEXT 'Anirban'|'Puspita'), amount(REAL INR).
         """
except Exception as e:
    logger.error(f"Failed to load or parse metadata YAML: {e}", exc_info=True)
    # Proceed with fallback schema or raise error depending on desired robustness
    SCHEMA_METADATA = """
     Fallback Schema: Table: expenses. Columns: id, date, year, month, week, day_of_week, account, category, sub_category, type, user, amount.
     """

# ==========================================================================
#                       NODE FUNCTIONS Will Go Here
# ==========================================================================
def classify_query_node(state: AgentState) -> dict:
    """
    Classifies the user's original query into 'simple', 'advanced', or 'irrelevant'.
    Args:
        state (AgentState): The current state of the graph. Must contain 'original_query'.
    Returns:
        dict: A dictionary containing the 'classification' key with the determined category,
              or an 'error' key if classification fails.
    """
    logger.info("--- Executing Node: classify_query_node ---")
    query = state.get('original_query') # Use .get() for safety

    if not query:
        logger.error("Original query is missing in state for classification.")
        # Return error and default classification
        return {"error": "Missing user query.", "classification": "irrelevant"}

    logger.debug(f"Classifying query: '{query}'")

    # Define the prompt for the classification task
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Your primary task is to classify user questions about personal finance data into one of three categories. Respond ONLY with a single word: 'simple', 'advanced', or 'irrelevant'.

Definitions:
- simple: Directly answerable with a standard SQL query on the 'expenses' table. Examples: totals, averages, filtering by date/category/user, specific lookups like 'What was my total spend last month?', 'Show my grocery expenses in Jan 2024', 'List expenses over 1000 INR'.
- advanced: Requires complex analysis beyond direct SQL (e.g., forecasting, prediction, anomaly detection, clustering, complex multi-step calculations). Examples: 'Predict my spending next week', 'Cluster my spending habits', 'Find unusual spending patterns'. FOR THIS INITIAL IMPLEMENTATION, TREAT 'advanced' THE SAME AS 'simple' downstream (proceed to SQL generation).
- irrelevant: Unrelated to the user's personal finance data stored in the expenses table. Examples: 'What is the weather like?', 'Who won the game?', 'General knowledge questions'.
"""),
        # Explicitly tell the LLM what to do with the user's query
        ("user", f"Classify the following user query: {query}")
    ])

    # Create the chain: Prompt -> LLM
    chain = prompt | LLM

    try:
        # Invoke the LLM
        response = chain.invoke({}) # Provide empty input as context is in the prompt
        # Clean the LLM response: lower case, remove extra characters/whitespace
        classification = response.content.strip().lower().replace("'", "").replace('"', '').replace(".", "")
        logger.info(f"Gemini raw classification response: '{response.content}' -> Cleaned: '{classification}'")

        # Validate the classification output
        valid_classifications = ['simple', 'advanced', 'irrelevant']
        if classification not in valid_classifications:
            logger.warning(f"LLM returned an unexpected classification: '{classification}'. Defaulting to 'simple'.")
            # Fallback to a safe default if the LLM response is malformed
            classification = 'simple'

    except Exception as e:
        logger.error(f"LLM call failed during query classification for query '{query}': {e}", exc_info=True)
        # If the LLM call fails, set an error and default classification
        return {"error": f"Failed to classify query due to LLM error: {e}", "classification": "simple"}

    # Return the classification in the expected dictionary format to update the state
    return {"classification": classification}

def generate_sql_node(state: AgentState) -> dict:
    """
    Generates an SQLite query based on the user's original query and schema metadata.
    Uses prompt variables to safely inject schema and query.
    Args:
        state (AgentState): The current state graph. Must contain 'original_query'.
                           Uses the pre-loaded SCHEMA_METADATA constant.
    Returns:
        dict: A dictionary containing the 'sql_query' key with the generated query string,
              or an 'error' key if SQL generation fails.
    """
    logger.info("--- Executing Node: generate_sql_node ---")
    query = state.get('original_query')

    if state.get('error'):
         logger.warning(f"Skipping SQL generation due to previous error: {state['error']}")
         return {}
    if not query:
        logger.error("Original query is missing in state for SQL generation.")
        return {"error": "Missing user query.", "sql_query": None}

    logger.debug(f"Generating SQL for query: '{query}'")

    if not SCHEMA_METADATA:
        logger.error("Schema metadata (SCHEMA_METADATA) is empty or was not loaded.")
        return {"error": "Database schema metadata is unavailable.", "sql_query": None}

    template_string = """System: You are an expert SQLite query generator. Your task is to generate a precise and syntactically correct SQLite query for the 'expenses' table based ONLY on the user's question and the provided schema metadata.

Schema Metadata:
------
{schema_info}
------

Query Generation Instructions:
1.  Analyze the user's question and the detailed schema metadata.
2.  Identify the relevant columns, filters, aggregations (SUM, AVG, COUNT), and grouping clauses needed.
3.  Use the 'purpose_for_llm' descriptions in the metadata to understand column usage and handle potential ambiguities (e.g., prefer 'Household.Electricity Bill' for bill payments, consider context for 'Amazon').
4.  Construct a single, valid SQLite query.
5.  Interpret timeframes relative to a plausible current date (e.g., assume mid-2024 or later for 'last month', 'this year'). Use specific 'YYYY-MM-DD' date formats in WHERE clauses (e.g., `date BETWEEN '2024-01-01' AND '2024-01-31'`).
6.  Respond ONLY with the raw SQL query. Do NOT include any explanations, comments, or markdown formatting (like ```sql or ```).
7.  Ensure the query terminates correctly (no trailing semicolon needed typically for execution libraries).

User: Generate the SQLite query for the following question: {user_query}
"""
    prompt = ChatPromptTemplate.from_template(template_string)
    logger.debug(f"Prompt expected input variables: {prompt.input_variables}")
    chain = prompt | LLM
    try:
        input_data = {"schema_info": SCHEMA_METADATA, "user_query": query}
        logger.debug(f"Invoking SQL generation chain with input_data keys: {list(input_data.keys())}")
        logger.debug(f"Value for 'user_query' (first 50 chars): {str(input_data.get('user_query', 'MISSING'))[:50]}")
        logger.debug(f"Value for 'schema_info' present: {bool(input_data.get('schema_info'))}")
        response = chain.invoke(input_data)
        sql_query = response.content.strip().replace("```sql", "").replace("```", "").strip()
        if sql_query.endswith(';'):
            sql_query = sql_query[:-1].strip()
        logger.info(f"Gemini raw SQL response: '{response.content}' -> Cleaned: '{sql_query}'")
        if not sql_query or not sql_query.lower().startswith("select"):
             logger.error(f"Generated query is empty or does not start with SELECT: '{sql_query}'")
             return {"error": "Failed to generate a valid SELECT query.", "sql_query": sql_query or "Empty response"}
    except KeyError as e:
        logger.error(f"KeyError during chain invocation in SQL generation. This usually means the input dict didn't match prompt variables. Error: {e}", exc_info=True)
        return {"error": f"Internal error matching input to prompt variables: {e}"}
    except Exception as e:
        logger.error(f"LLM call failed during SQL generation for query '{query}': {e}", exc_info=True)
        return {"error": f"Failed to generate SQL query due to LLM error: {e}", "sql_query": None}
    return {"sql_query": sql_query}
def execute_sql_node(state: AgentState) -> dict:
    """
    Executes the generated SQL query against the SQLite database using SQLAlchemy.
    Returns results as a string and a serializable list of dictionaries.

    Args:
        state (AgentState): The current graph state. Must contain 'sql_query' if no prior error.

    Returns:
        dict: A dictionary containing 'sql_results_list' (List[Dict]) and
              'sql_results_str' (string representation), or an 'error' key if execution fails.
    """
    logger.info("--- Executing Node: execute_sql_node ---")
    sql_query = state.get('sql_query')
    if state.get('error'):
        logger.warning(f"Skipping SQL execution due to previous error: {state['error']}")
        return {"sql_results_list": [], "sql_results_str": f"Error: {state['error']}"}
    if not sql_query:
        logger.error("No SQL query found in state to execute.")
        return {"error": "SQL query generation failed or was missing.", "sql_results_list": [], "sql_results_str": "Error: No SQL query found."}
    logger.info(f"Attempting to execute SQL query: [{sql_query}]")
    try:
        with engine.connect() as connection:
            df = pd.read_sql(sql=text(sql_query), con=connection)
        logger.info(f"SQL query executed successfully. Number of rows returned: {len(df)}")
        results_list: List[Dict[str, Any]] = [] # Initialize as empty list
        results_str: str = ""                   # Initialize as empty string
        if not df.empty:
            for col in df.columns:
                if 'date' in col.lower():
                    try: df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d')
                    except Exception: pass # Ignore formatting errors
                elif 'amount' in col.lower() and pd.api.types.is_numeric_dtype(df[col]):
                    try: df[col] = df[col].round(2)
                    except Exception: pass # Ignore formatting errors
            results_list = df.to_dict('records') # Convert DataFrame to list of dictionaries
            results_str = df.to_string(index=False, na_rep='<NA>') # Create string version
        else:
            results_list = [] # Explicitly set to empty list
            results_str = "Query returned no results."
        return {"sql_results_list": results_list, "sql_results_str": results_str}
    except Exception as e:
        logger.error(f"SQL execution failed for query [{sql_query}]: {e}", exc_info=True)
        error_msg = f"Failed to execute SQL query. Error: {e}. Query Attempted: [{sql_query}]"
        return {"error": error_msg, "sql_results_list": [], "sql_results_str": f"Error executing SQL."}
def generate_chart_node(state: AgentState) -> dict:
    """
    Generates a Plotly chart JSON based on the SQL query results (List of Dicts).
    Uses simple heuristics to determine the most appropriate chart type.

    Args:
        state (AgentState): The current graph state. Needs 'sql_results_list'.

    Returns:
        dict: A dictionary containing 'chart_json' (Plotly JSON string) if successful,
              otherwise None.
    """
    logger.info("--- Executing Node: generate_chart_node ---")
    results_list = state.get('sql_results_list')
    error = state.get('error')
    query = state.get('original_query', 'Unknown query')
    if error:
        logger.warning(f"Skipping chart generation due to previous error: {error}")
        return {"chart_json": None}
    if results_list is None: # Check if None (might happen if execute_sql had severe issue before returning)
        logger.warning("Skipping chart generation as results list is None.")
        return {"chart_json": None}
    if not results_list: # Check if empty list
        logger.info("Skipping chart generation as results list is empty.")
        return {"chart_json": None}
    try:
        df = pd.DataFrame(results_list)
        if df.empty: # Double check after conversion
            logger.warning("DataFrame created from results list is empty. Skipping chart generation.")
            return {"chart_json": None}
        logger.info(f"Reconstructed DataFrame for charting, shape: {df.shape}")
        logger.debug(f"DataFrame columns: {df.columns.tolist()}")
        fig = None
        num_rows, num_cols = df.shape
        col_names_lower = df.columns.str.lower()
        numeric_cols = [
            col for col in df.columns
            if pd.api.types.is_numeric_dtype(df[col])
            and col.lower() not in ['id', 'year']
        ]
        logger.debug(f"Identified numeric columns for plotting: {numeric_cols}")
        date_col_present = 'date' in col_names_lower or 'month' in col_names_lower or 'yearmonth' in col_names_lower
        if date_col_present and len(numeric_cols) == 1:
            if 'date' in col_names_lower: time_col = df.columns[col_names_lower.tolist().index('date')]
            elif 'month' in col_names_lower: time_col = df.columns[col_names_lower.tolist().index('month')]
            else: time_col = df.columns[col_names_lower.tolist().index('yearmonth')]
            value_col = numeric_cols[0]
            logger.info(f"Attempting Line Chart: X='{time_col}', Y='{value_col}'")
            try:
                try:
                    df[time_col] = pd.to_datetime(df[time_col])
                except Exception as time_conv_err:
                    logger.warning(f"Could not convert time column '{time_col}' to datetime: {time_conv_err}. Sorting may be string-based.")
                df_sorted = df.sort_values(by=time_col)
                fig = px.line(df_sorted, x=time_col, y=value_col, title=f"{value_col.capitalize()} Trend", markers=True)
                fig.update_layout(xaxis_title=time_col.capitalize(), yaxis_title=value_col.capitalize())
            except TypeError as sort_err:
                 logger.warning(f"TypeError during sorting/plotting line chart (column '{time_col}' might not be sortable): {sort_err}. Skipping line chart.")
            except Exception as line_err:
                 logger.warning(f"Could not generate line chart: {line_err}. Skipping line chart.")
        elif num_cols == 2 and len(numeric_cols) == 1:
            value_col = numeric_cols[0]
            cat_col = next((col for col in df.columns if col != value_col), None)
            if cat_col:
                logger.info(f"Attempting Bar Chart: Category='{cat_col}', Value='{value_col}'")
                try:
                    max_bars = 15
                    df_agg = df.sort_values(by=value_col, ascending=False)
                    df_chart = df_agg.head(max_bars)
                    title_suffix = f" (Top {max_bars})" if len(df_agg) > max_bars else ""
                    title = f"{value_col.capitalize()} by {cat_col.capitalize()}{title_suffix}"
                    fig = px.bar(df_chart, x=cat_col, y=value_col, title=title, text_auto='.2s')
                    fig.update_traces(textangle=0, textposition="outside")
                    fig.update_layout(xaxis_title=cat_col.capitalize(), yaxis_title=value_col.capitalize())
                except Exception as bar_err:
                    logger.warning(f"Could not generate bar chart: {bar_err}. Skipping bar chart.")
            else:
                logger.warning("Bar chart condition met but failed to identify categorical column.")
        elif num_rows == 1 and len(numeric_cols) == 1:
            logger.info("Single numeric value result. Skipping graphical chart generation.")
            return {"chart_json": None}
        if fig is None:
            logger.info("No specific chart type matched heuristics. Skipping chart generation.")
            return {"chart_json": None}
        fig.update_layout(
            margin=dict(l=40, r=20, t=60, b=40),
            title_x=0.5,
            legend_title_text=None
        )
        chart_json = fig.to_json()
        logger.info("Plotly chart JSON generated successfully.")
        return {"chart_json": chart_json}
    except Exception as e:
        logger.error(f"Unexpected error during chart generation: {e}", exc_info=True)
        return {"chart_json": None}
from langchain_core.prompts import PromptTemplate # Or keep ChatPromptTemplate
def generate_response_node(state: AgentState) -> dict:
    """
    Generates the final natural language response. Uses PromptTemplate.
    Instructs the LLM to use INR as the currency.

    Args:
        state (AgentState): The current graph state. Needs 'original_query'.
                           Uses 'sql_results_str', 'error', 'classification',
                           and checks for 'chart_json'.

    Returns:
        dict: A dictionary containing the 'final_response' string.
    """
    logger.info("--- Executing Node: generate_response_node ---")
    query = state.get('original_query', "An unspecified query")
    sql_results_str = state.get('sql_results_str', '') # Use the string representation
    error = state.get('error')
    classification = state.get('classification')
    chart_available = state.get('chart_json') is not None
    final_response = ""
    if error:
        logger.warning(f"Generating response based on detected error: {error}")
        error_str = str(error)
        if "Failed to execute SQL" in error_str or "syntax error" in error_str.lower():
            final_response = f"I encountered an issue trying to retrieve the data ({error_str}). Perhaps try asking differently?"
        elif "Failed to generate SQL" in error_str:
             final_response = f"I had trouble understanding how to fetch the data ({error_str}). Could you please rephrase?"
        elif "Failed to classify query" in error_str:
             final_response = f"I had trouble understanding your request type ({error_str}). Could you clarify?"
        elif "Missing user query" in error_str or "metadata is unavailable" in error_str:
             final_response = f"There was an internal setup issue preventing me from processing your request: {error_str}"
        else: # Generic error
            final_response = f"I'm sorry, I encountered an issue: {error_str}. Please try again."
    elif classification == 'irrelevant':
         final_response = "This question doesn't seem related to your financial expenses. Could you ask something about your spending?"
         logger.info("Generated response for 'irrelevant' classification.")
    elif not error and (not sql_results_str or sql_results_str == "Query returned no results."):
         final_response = "I looked through your expense records based on your query, but couldn't find any matching transactions."
         logger.info("Generated response for query with no results.")
    elif not error and sql_results_str:
        logger.info("Generating summary response using PromptTemplate.")
        chart_mention = "I've also prepared a chart to visualize this." if chart_available else ""
        template_string = """System: You are a friendly financial assistant summarizing financial data for users based in India.
Instructions:
*   Answer the user's question ({user_query}) using the key information from the data below.
*   Summarize findings/trends if data is tabular. State single numbers clearly. Do NOT just repeat the raw data.
*   {chart_mention_instruction} Mention the chart briefly if applicable.
*   Keep the tone conversational (use "You"/"Your"). Avoid jargon.
*   IMPORTANT: Always use 'INR' (Indian Rupees) when referring to monetary values. Do NOT use '$' or other currency symbols.
Retrieved Data:
{sql_data}
Assistant: """
        prompt = PromptTemplate(
            template=template_string,
            input_variables=["user_query", "sql_data", "chart_mention_instruction"]
        )
        chain = prompt | LLM
        try:
            if LLM is None: raise ValueError("LLM client is not initialized.")
            input_dict = {
                "user_query": query,
                "sql_data": sql_results_str, # Pass the string representation
                "chart_mention_instruction": chart_mention
            }
            response = chain.invoke(input_dict)
            final_response = response.content.strip()
            logger.info("Successfully generated summary response from LLM.")
        except Exception as e:
            logger.error(f"LLM call failed during final response generation: {e}", exc_info=True)
            final_response = f"I retrieved the data:\n{sql_results_str}\nBut I had trouble summarizing it. Please note the currency is INR." # Add fallback currency note
    else:
        logger.error("Reached end of generate_response_node without generating a response. State: %s", {k:v for k,v in state.items() if k != 'sql_results_list'}) # Log state excluding list
        final_response = "I'm sorry, I wasn't able to determine a response for your query."
    if not final_response:
        logger.warning("Final response was empty after processing, providing default.")
        final_response = "I'm sorry, I couldn't process that request properly."
    return {"final_response": final_response}
def should_continue(state: AgentState) -> str:
    """
    Determines the next node to execute based on the current state,
    specifically the query classification or presence of errors.

    Args:
        state (AgentState): The current graph state.

    Returns:
        str: The name of the next node to call ('generate_sql', 'generate_response'),
             or potentially END (though current logic routes errors to response).
    """
    logger.info("--- Evaluating Edge: should_continue ---")
    classification = state.get('classification')
    error = state.get('error') # Check if ANY previous node set an error
    if error:
        logger.warning(f"Error detected in state ('{error}'), routing directly to 'generate_response'.")
        return "generate_response"
    elif classification == 'irrelevant':
        logger.info("Classification is 'irrelevant', routing to 'generate_response'.")
        return "generate_response"
    elif classification in ['simple', 'advanced']:
        logger.info(f"Classification is '{classification}', routing to 'generate_sql'.")
        return "generate_sql"
    else:
        logger.error(f"Unknown or missing classification ('{classification}') in state. Routing to generate_response.")
        return "generate_response"
logger.info("Defining LangGraph workflow structure...")
workflow = StateGraph(AgentState)
logger.debug("StateGraph initialized.")
workflow.add_node("classify_query", classify_query_node)
workflow.add_node("generate_sql", generate_sql_node)
workflow.add_node("execute_sql", execute_sql_node)
workflow.add_node("generate_chart", generate_chart_node)
workflow.add_node("generate_response", generate_response_node)
logger.info("Nodes added to the graph: classify_query, generate_sql, execute_sql, generate_chart, generate_response")
workflow.set_entry_point("classify_query")
logger.info("Graph entry point set to 'classify_query'.")
workflow.add_conditional_edges(
    source="classify_query",  # keyword: source
    path=should_continue,     # keyword: path
    path_map={                # keyword: path_map
        "generate_sql": "generate_sql",
        "generate_response": "generate_response"
    }
)
logger.info("Conditional edge added from 'classify_query' based on 'should_continue' logic.")
workflow.add_edge("generate_sql", "execute_sql")
workflow.add_edge("execute_sql", "generate_chart")
workflow.add_edge("generate_chart", "generate_response")
logger.info("Linear edges defined: generate_sql -> execute_sql -> generate_chart -> generate_response.")
workflow.add_edge("generate_response", END) # END is a special marker from langgraph.graph
logger.info("Final edge added from 'generate_response' to END.")
graph = workflow.compile()
logger.info("LangGraph workflow compiled successfully. Runnable 'graph' object created.")
if __name__ == "__main__":
    logger.info("--- Running Direct Script Test ---")
    test_queries = [
        "What was my total spend last month?",
        "Show my grocery expenses",
        "what is the weather?",
        "Compare spending between Anirban and Puspita for Rent",
        "Generate a query with syntax error deliberately" # Example for testing error path
    ]
    test_input = {"original_query": test_queries[0]} # Change index to test different queries
    logger.info(f"Test Input: {test_input}")
    try:
        for output_chunk in graph.stream(test_input, {"recursion_limit": 10}):
            node_name = list(output_chunk.keys())[0]
            node_data = output_chunk[node_name]
            logger.info(f"--- Step Output: {node_name} ---")
            log_output = {k: (v[:100] + '...' if isinstance(v, str) and len(v) > 100 else v)
                          for k, v in node_data.items() if k != 'sql_results_df'} # Exclude df for brevity
            logger.info(f"{log_output}")
    except Exception as e:
        logger.error(f"Test execution failed: {e}", exc_info=True)
    logger.info("--- Direct Script Test Complete ---")


#PY File: state.py
@path: assistant/finance-assistant/src/agent/state.py
@summary: Full Python code (compressed) included below.
@code:
"""Define the state structures for the agent."""
from __future__ import annotations # Ensures compatibility with type hints
from typing import TypedDict, Optional, List, Dict, Any
import pandas as pd
class AgentState(TypedDict):
    """Represents the state shared across the agent graph."""
    original_query: str           # The initial question from the user
    classification: Optional[str]   # 'simple', 'advanced', 'irrelevant'
    sql_query: Optional[str]        # Generated SQL query
    sql_results_str: Optional[str]  # SQL results as a formatted string
    sql_results_list: Optional[List[Dict[str, Any]]] # SQL results as a list of dictionaries, store results as serializable list of dicts
    chart_json: Optional[str]       # Plotly figure JSON representation
    final_response: Optional[str]   # Final text response for the user
    error: Optional[str]            # To capture errors during execution


#PY File: __init__.py
@path: assistant/finance-assistant/tests/integration_tests/__init__.py
@summary: Full Python code (compressed) included below.
@code:
"""Define any integration tests you want in this directory."""


#PY File: test_graph.py
@path: assistant/finance-assistant/tests/integration_tests/test_graph.py
@summary: Full Python code (compressed) included below.
@code:
import pytest
from langsmith import unit
from agent import graph
@pytest.mark.asyncio
@unit
async def test_agent_simple_passthrough() -> None:
    res = await graph.ainvoke({"changeme": "some_val"})
    assert res is not None


#PY File: __init__.py
@path: assistant/finance-assistant/tests/unit_tests/__init__.py
@summary: Full Python code (compressed) included below.
@code:
"""Define any unit tests you may want in this directory."""


#PY File: test_configuration.py
@path: assistant/finance-assistant/tests/unit_tests/test_configuration.py
@summary: Full Python code (compressed) included below.
@code:
from agent.configuration import Configuration
def test_configuration_empty() -> None:
    Configuration.from_runnable_config({})


#PY File: check_env.py
@path: check_env.py
@summary: Full Python code (compressed) included below.
@code:
import sys
import os
import importlib.util
print(f"--- Python Executable ---")
print(sys.executable) # Shows which python is running this script
print("-" * 25)
print("\n--- sys.path ---")
for p in sys.path:
    print(p) # Shows where Python looks for modules
print("-" * 25)
print("\n--- Checking Pandas Import ---")
try:
    spec = importlib.util.find_spec("pandas")
    if spec is not None:
        print(f"Pandas found at: {spec.origin}")
        import pandas as pd
        print(f"Pandas successfully imported. Version: {pd.__version__}")
    else:
        print("Pandas specification NOT found by importlib.")
except ImportError:
    print("Pandas import FAILED.")
except Exception as e:
    print(f"An unexpected error occurred during pandas check: {e}")
print("-" * 25)
print("\n--- PATH Environment Variable ---")
path_var = os.environ.get('PATH', '')
path_list = path_var.split(os.pathsep)
print("Entries in PATH:")
for entry in path_list:
    print(entry)
print("-" * 25)
print("\n--- Checking if genai paths are in PATH ---")
genai_scripts = r"E:\Code\venv\conda\genai\Scripts"
genai_lib_bin = r"E:\Code\venv\conda\genai\Library\bin" # Common on Windows
genai_base = r"E:\Code\venv\conda\genai"
if any(p.lower().startswith(genai_scripts.lower()) for p in path_list):
     print(f"✅ Found path starting with: {genai_scripts}")
else:
     print(f"❌ Did NOT find expected Scripts path starting with: {genai_scripts}")
if any(p.lower().startswith(genai_lib_bin.lower()) for p in path_list):
     print(f"✅ Found path starting with: {genai_lib_bin}")
else:
     print(f"❌ Did NOT find expected Library\\bin path starting with: {genai_lib_bin}")
if any(p.lower().startswith(genai_base.lower()) for p in path_list if not p.lower().startswith(genai_scripts.lower()) and not p.lower().startswith(genai_lib_bin.lower())):
     print(f"✅ Found other path starting with: {genai_base}")
else:
     print(f"❌ Did NOT find other base path starting with: {genai_base}")


#PY File: generate_data.py
@path: reference/generate_data.py
@summary: Full Python code (compressed) included below.
@code:
"""
Generates realistic expense data based on predefined rules and constraints.

Reads rules from 'sample_data_generation.csv' (in project root) and
outputs transactions to 'dummy_expenses_generated.csv' (in project root)
covering the period from 2023-01-01 to 2025-04-20.
"""
import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta
from pathlib import Path
import logging
from tqdm import tqdm # For progress bar
from typing import List, Dict, Any, Optional
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
RULES_FILE = PROJECT_ROOT / "sample_data_generation.csv"
OUTPUT_FILE = PROJECT_ROOT / "dummy_expenses_generated.csv"
METADATA_FILE = PROJECT_ROOT / "expense_metadata.json" # Optional, for validation/reference
START_DATE = datetime(2023, 1, 1)
END_DATE = datetime(2025, 4, 20)
MONTHLY_MIN_TOTAL = 60000
MONTHLY_MAX_TOTAL = 120000
MONTHLY_MAX_ROWS = 100
AVG_ADHOC_PER_DAY = 4 # Average number of ad-hoc transactions per day
ADHOC_RANGE = (1, 7) # Min/Max ad-hoc transactions per day (adjust as needed)
def load_rules(filepath: Path) -> Optional[pd.DataFrame]:
    """Loads and preprocesses the ruleset CSV."""
    if not filepath.exists():
        logging.error(f"Rules file not found: {filepath}")
        return None
    try:
        df_rules = pd.read_csv(filepath)
        df_rules.columns = [col.strip() for col in df_rules.columns]
        df_rules['Valid-expense-types'] = df_rules['Valid-expense-types'].str.split('|')
        for col in ['Min-expenses-amount', 'Max-expenses-amount', 'Max-times-per-month']:
            df_rules[col] = pd.to_numeric(df_rules[col], errors='coerce')
        df_rules['Max-times-per-month'].fillna(5, inplace=True)
        df_rules['Max-times-per-month'] = df_rules['Max-times-per-month'].astype(int)
        df_rules.dropna(subset=['Category', 'Sub-category', 'User', 'Account', 'Expense-Frequency', 'Min-expenses-amount', 'Max-expenses-amount'], inplace=True)
        logging.info(f"Loaded {len(df_rules)} rules from {filepath}")
        return df_rules
    except Exception as e:
        logging.error(f"Error loading or processing rules file {filepath}: {e}", exc_info=True)
        return None
def get_date_parts(date_obj: datetime) -> Dict[str, Any]:
    """Calculates derived date columns."""
    return {
        "date_dt": date_obj,
        "date": date_obj.strftime('%d-%m-%Y'),
        "year": date_obj.year,
        "month": date_obj.strftime('%Y-%m'),
        "week": date_obj.strftime('%Y-W%V'),
        "day_of_week": date_obj.strftime('%A')
    }
def generate_fixed_transaction(rule: pd.Series, date_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Generates a dictionary for a fixed transaction based on a rule."""
    if rule['Min-expenses-amount'] != rule['Max-expenses-amount']:
        logging.warning(f"Rule marked as fixed frequency but Min!=Max: {rule.to_dict()}")
        return None
    amount = rule['Min-expenses-amount']
    valid_types = rule['Valid-expense-types']
    selected_type = valid_types[0] if isinstance(valid_types, list) and valid_types else "Fixed Expense"
    return {
        **date_info, "account": rule['Account'], "category": rule['Category'],
        "sub_category": rule['Sub-category'], "type": selected_type,
        "user": rule['User'], "amount": amount
    }
def generate_adhoc_transaction(rule: pd.Series, date_info: Dict[str, Any]) -> Dict[str, Any]:
    """Generates a dictionary for an ad-hoc transaction based on a rule."""
    amount = round(random.uniform(rule['Min-expenses-amount'], rule['Max-expenses-amount']), 2)
    selected_type = random.choice(rule['Valid-expense-types']) if isinstance(rule['Valid-expense-types'], list) and rule['Valid-expense-types'] else "Ad-hoc Expense"
    return {
        **date_info, "account": rule['Account'], "category": rule['Category'],
        "sub_category": rule['Sub-category'], "type": selected_type,
        "user": rule['User'], "amount": amount
    }
def check_fixed_conditions(rule: pd.Series, current_date: datetime) -> bool:
    """Checks if a fixed/recurring rule should trigger on the current date."""
    freq = rule['Expense-Frequency']
    day = current_date.day
    month = current_date.month
    if freq == 'monthly':
        if rule['Category'] == 'Rent' and day == 1: return True
        if rule['Category'] == 'Household' and rule['Sub-category'] == 'Maid' and day == 1: return True
        if rule['Category'] == 'Investment' and rule['Sub-category'] == 'SIP' and day == 5: return True
        if rule['Category'] == 'Insurance Premium' and rule['Sub-category'] == 'ULIP' and day == 10: return True
        if rule['Category'] == 'Insurance Premium' and rule['Sub-category'] == 'Health Insurance' and day == 15: return True
        if rule['Category'] == 'Connectivity' and rule['Sub-category'] == 'Netflix' and day == 20: return True # Example day for monthly connectivity
        if rule['Category'] == 'Utilities' and rule['Sub-category'] == 'Water' and day == 7: return True # Example day
        if rule['Category'] == 'Utilities' and rule['Sub-category'] == 'Maintenance' and day == 6: return True # Example day
        if rule['Category'] == 'Utilities' and rule['Sub-category'] == 'Garbage Collection' and day == 3: return True # Example day
        return False
    elif freq == 'bi-monthly': # Odd months, day 2
        return month % 2 != 0 and day == 2
    elif freq == 'once every 3 months': # Jan, Apr, Jul, Oct, day 3
        return month in [1, 4, 7, 10] and day == 3
    elif freq == 'once every 6 months': # Jan, Jul, day 4
        return month in [1, 7] and day == 4
    elif freq == 'bi-annually': # Mar 20, Sep 20
        return (month == 3 and day == 20) or (month == 9 and day == 20)
    elif freq == 'annually': # Jan 15
        if rule['Category'] == 'Insurance Premium' and rule['Sub-category'] == 'Vehicle Insurance': return month == 2 and day == 25 # Example Date
        if rule['Category'] == 'Connectivity' and rule['Sub-category'] == 'Prime Video': return month == 1 and day == 15 # Example Date
        if rule['Category'] == 'Connectivity' and rule['Sub-category'] == 'Disney+ Hotstar': return month == 1 and day == 16 # Example Date
        return False # Only trigger specific annuals
    return False
def generate_data():
    """Main function to generate the expense data."""
    logging.info("--- Starting Data Generation ---")
    logging.info(f"Looking for rules file at: {RULES_FILE}")
    logging.info(f"Output will be saved to: {OUTPUT_FILE}")
    df_rules = load_rules(RULES_FILE)
    if df_rules is None:
        return
    fixed_rules = df_rules[df_rules['Expense-Frequency'] != 'ad-hoc'].copy()
    adhoc_rules = df_rules[df_rules['Expense-Frequency'] == 'ad-hoc'].copy()
    all_transactions = []
    current_date = START_DATE
    total_days = (END_DATE - START_DATE).days + 1
    pbar = tqdm(total=total_days, desc="Generating Daily Transactions")
    current_month_str = ""
    current_month_total = 0.0
    current_month_rows = 0
    monthly_rule_counts: Dict[int, int] = {}
    while current_date <= END_DATE:
        date_info = get_date_parts(current_date)
        if date_info['month'] != current_month_str:
            if current_month_str:
                logging.info(f"Month {current_month_str} Summary: Rows={current_month_rows}, Total=₹{current_month_total:.2f}")
                if current_month_total < MONTHLY_MIN_TOTAL: logging.warning(f"Month {current_month_str} total ₹{current_month_total:.2f} BELOW target minimum ₹{MONTHLY_MIN_TOTAL}")
                if current_month_total > MONTHLY_MAX_TOTAL: logging.warning(f"Month {current_month_str} total ₹{current_month_total:.2f} ABOVE target maximum ₹{MONTHLY_MAX_TOTAL}")
                if current_month_rows > MONTHLY_MAX_ROWS: logging.warning(f"Month {current_month_str} rows {current_month_rows} EXCEEDED target maximum {MONTHLY_MAX_ROWS}")
            current_month_str = date_info['month']
            current_month_total = 0.0; current_month_rows = 0; monthly_rule_counts = {}
            logging.debug(f"Starting generation for month: {current_month_str}")
        for index, rule in fixed_rules.iterrows():
            if check_fixed_conditions(rule, current_date):
                transaction = generate_fixed_transaction(rule, date_info)
                if transaction and current_month_rows < MONTHLY_MAX_ROWS + 5:
                     all_transactions.append(transaction)
                     current_month_total += transaction['amount']
                     current_month_rows += 1
                     monthly_rule_counts[index] = monthly_rule_counts.get(index, 0) + 1
                     logging.debug(f"Generated fixed: {transaction['category']}/{transaction['sub_category']} on {date_info['date']}")
                elif transaction:
                     logging.warning(f"Skipped fixed {rule['Category']}/{rule['Sub-category']} on {date_info['date']} (Monthly row limit: {current_month_rows})")
        num_adhoc_today = random.randint(ADHOC_RANGE[0], ADHOC_RANGE[1])
        adhoc_added_today = 0
        weights = adhoc_rules['Account'].apply(lambda x: 1.5 if x in ['Anirban-ICICI', 'Puspita-SBI'] else 1.0).values
        if weights.sum() > 0: weights = weights / weights.sum()
        else: weights = None
        for _ in range(num_adhoc_today):
            if current_month_total >= MONTHLY_MAX_TOTAL or current_month_rows >= MONTHLY_MAX_ROWS:
                logging.debug(f"Stopping ad-hoc for {date_info['date']} due to limits.")
                break
            if weights is None or adhoc_rules.empty: continue
            rule_selected = False
            for attempt in range(5):
                 try:
                     selected_rule_series = adhoc_rules.sample(n=1, weights=weights).iloc[0]
                     rule_index = selected_rule_series.name
                 except ValueError as e: logging.warning(f"Adhoc sample error: {e}"); continue
                 current_rule_count = monthly_rule_counts.get(rule_index, 0)
                 max_allowed = selected_rule_series['Max-times-per-month']
                 if current_rule_count < max_allowed:
                     transaction = generate_adhoc_transaction(selected_rule_series, date_info)
                     if current_month_total + transaction['amount'] <= MONTHLY_MAX_TOTAL * 1.05: # Allow slight overshoot
                         all_transactions.append(transaction)
                         current_month_total += transaction['amount']
                         current_month_rows += 1
                         monthly_rule_counts[rule_index] = current_rule_count + 1
                         adhoc_added_today += 1
                         rule_selected = True
                         logging.debug(f"Generated adhoc: {transaction['category']}/{transaction['sub_category']} on {date_info['date']}")
                         break # Exit retry loop
                     else:
                         logging.debug(f"Skipping adhoc {selected_rule_series['Category']}/{selected_rule_series['Sub-category']} to avoid exceeding monthly total drastically.")
                 else:
                     logging.debug(f"Rule {rule_index} hit monthly limit ({max_allowed}). Retrying...")
            if not rule_selected: logging.debug(f"Could not find valid ad-hoc rule for {date_info['date']} after retries.")
        current_date += timedelta(days=1)
        pbar.update(1)
    pbar.close()
    if current_month_str:
        logging.info(f"Month {current_month_str} Summary: Rows={current_month_rows}, Total=₹{current_month_total:.2f}")
        if current_month_total < MONTHLY_MIN_TOTAL: logging.warning(f"Month {current_month_str} total ₹{current_month_total:.2f} BELOW target minimum ₹{MONTHLY_MIN_TOTAL}")
        if current_month_total > MONTHLY_MAX_TOTAL: logging.warning(f"Month {current_month_str} total ₹{current_month_total:.2f} ABOVE target maximum ₹{MONTHLY_MAX_TOTAL}")
        if current_month_rows > MONTHLY_MAX_ROWS: logging.warning(f"Month {current_month_str} rows {current_month_rows} EXCEEDED target maximum {MONTHLY_MAX_ROWS}")
    if not all_transactions:
        logging.warning("No transactions were generated.")
        return
    df_final = pd.DataFrame(all_transactions)
    output_columns = ['date', 'year', 'month', 'week', 'day_of_week', 'account', 'category', 'sub_category', 'type', 'user', 'amount']
    df_final = df_final[output_columns]
    logging.info(f"--- Data Generation Complete ---")
    logging.info(f"Total transactions generated: {len(df_final)}")
    try:
        df_final.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')
        logging.info(f"Successfully saved generated data to: {OUTPUT_FILE}")
    except Exception as e:
        logging.error(f"Error saving output CSV file {OUTPUT_FILE}: {e}", exc_info=True)
if __name__ == "__main__":
    generate_data()


#PY File: generate_db.py
@path: reference/generate_db.py
@summary: Full Python code (compressed) included below.
@code:
"""
This script reads 'data/expenses.csv', reconstructs missing 'date' entries using 'year', 'month', and 'day_of_week',
adds UUIDs, and saves the result into 'data/expenses.db'.
"""
import sqlite3
import pandas as pd
import uuid
from pathlib import Path
import calendar
from datetime import datetime
ROOT_DIR = Path(__file__).parent
DATA_DIR = ROOT_DIR / "data"
CSV_PATH = DATA_DIR / "expenses.csv"
DB_PATH = DATA_DIR / "expenses.db"
DATA_DIR.mkdir(parents=True, exist_ok=True)
df = pd.read_csv(CSV_PATH)
df.columns = (
    df.columns
    .str.strip()
    .str.lower()
    .str.replace("-", "_")
    .str.replace(" ", "_")
)
if 'date' not in df.columns:
    df['date'] = pd.NaT
df['date'] = pd.to_datetime(df['date'], errors='coerce')
missing_date_mask = df['date'].isnull()
def reconstruct_date(row):
    year = int(row['year'])
    month = int(row['month'].split('-')[1]) if isinstance(row['month'], str) and '-' in row['month'] else int(row['month'])
    day_name = row['day_of_week']
    month_calendar = calendar.monthcalendar(year, month)
    for week in month_calendar:
        for i, day in enumerate(week):
            if day != 0 and calendar.day_name[i] == day_name:
                return datetime(year, month, day)
    return pd.NaT
df.loc[missing_date_mask, 'date'] = df[missing_date_mask].apply(reconstruct_date, axis=1)
still_missing = df['date'].isnull().sum()
if still_missing > 0:
    print(f"⚠️ Warning: {still_missing} 'date' entries could not be reconstructed and will be dropped.")
    df = df.dropna(subset=['date'])
df['month'] = df['date'].dt.to_period('M').astype(str)
df['week'] = df['date'].dt.strftime('%G-W%V')  # ISO week format
df['day_of_week'] = df['date'].dt.day_name()
df['date'] = df['date'].dt.strftime('%Y-%m-%d')
df['id'] = [str(uuid.uuid4()) for _ in range(len(df))]
required_columns = {
    'date', 'account', 'category', 'sub_category', 'type',
    'user', 'amount', 'month', 'week', 'day_of_week'
}
missing_cols = required_columns - set(df.columns)
if missing_cols:
    raise ValueError(f"❌ Missing required column(s) after processing: {missing_cols}")
with sqlite3.connect(DB_PATH) as conn:
    df.to_sql("expenses", conn, if_exists="replace", index=False)
print(f"\n✅ Database successfully created at: {DB_PATH.resolve()}")
print(f"📊 Total records written: {len(df)}")


#PY File: generate_metadata_details.py
@path: reference/generate_metadata_details.py
@summary: Full Python code (compressed) included below.
@code:
"""
generate_metadata_details.py

Reads specified metadata files (YAML and JSON) from the 'metadata/' directory,
includes the head of CSV files found in the 'data/' directory,
and compiles their content into a single text file
'reference/instructions_metadata.txt' for LLM context.
"""
import logging
from pathlib import Path
import json
import yaml # Requires PyYAML to be installed (pip install pyyaml)
import pandas as pd # Required for reading CSV head
from typing import Optional, List
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
try:
    SCRIPT_DIR = Path(__file__).resolve().parent
    ROOT_DIR = SCRIPT_DIR.parent
    REFERENCE_DIR = ROOT_DIR / "reference"
    METADATA_DIR = ROOT_DIR / "metadata"
    DATA_DIR = ROOT_DIR / "data" # Define data directory path
    OUTPUT_PATH = REFERENCE_DIR / "instructions_metadata.txt"
    YAML_METADATA_PATH = METADATA_DIR / "expenses_metadata_detailed.yaml"
    JSON_METADATA_PATH = METADATA_DIR / "expense_metadata.json" # Phase 1 metadata
    CSV_HEAD_ROWS = 5
    logger.info(f"Root directory: {ROOT_DIR}")
    logger.info(f"Metadata directory: {METADATA_DIR}")
    logger.info(f"Data directory: {DATA_DIR}")
    logger.info(f"Output file path: {OUTPUT_PATH}")
except Exception as e:
    logger.exception(f"Error setting up script paths: {e}")
    raise SystemExit("Could not determine project structure paths.") from e
def read_file_content(file_path: Path) -> Optional[str]:
    """Reads the entire content of a given text file (non-CSV)."""
    logger.debug(f"Attempting to read text file: {file_path.relative_to(ROOT_DIR)}")
    if not file_path.is_file():
        logger.error(f"File not found: {file_path}")
        return None
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        logger.info(f"Successfully read {len(content)} characters from {file_path.name}")
        return content
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}", exc_info=True)
        return None
def read_csv_head(file_path: Path, num_rows: int) -> Optional[str]:
    """Reads the head of a CSV file using pandas."""
    logger.debug(f"Attempting to read head of CSV: {file_path.relative_to(ROOT_DIR)}")
    if not file_path.is_file():
        logger.error(f"CSV file not found: {file_path}")
        return None
    try:
        df = pd.read_csv(file_path, nrows=num_rows, on_bad_lines='skip')
        logger.info(f"Successfully read head ({len(df)} rows) of CSV: {file_path.name}")
        if df.empty:
             return "(CSV file is empty)"
        return df.to_string(index=False)
    except pd.errors.EmptyDataError:
        logger.warning(f"CSV file is empty: {file_path}")
        return "(CSV file is empty)"
    except Exception as e:
        logger.error(f"Error reading CSV file {file_path}: {e}", exc_info=True)
        return f"# ERROR: Could not read CSV head from {file_path.name} due to {type(e).__name__}\n"
def find_csv_files(directory: Path) -> List[Path]:
    """Finds all .csv files directly within the specified directory."""
    csv_files: List[Path] = []
    if not directory.is_dir():
        logger.warning(f"Data directory not found: {directory}")
        return csv_files
    try:
        csv_files = sorted(list(directory.glob('*.csv')))
        logger.info(f"Found {len(csv_files)} CSV files in {directory.name}/: {[f.name for f in csv_files]}")
    except Exception as e:
        logger.error(f"Error searching for CSV files in {directory}: {e}")
    return csv_files
def main() -> None:
    """
    Reads metadata files and CSV heads, generates the consolidated instructions_metadata.txt file.
    """
    logger.info(f"--- Starting Metadata & Data Head Consolidation for {OUTPUT_PATH.name} ---")
    try:
        REFERENCE_DIR.mkdir(parents=True, exist_ok=True)
        logger.debug(f"Ensured reference directory exists: {REFERENCE_DIR}")
    except Exception as e:
        logger.error(f"Could not create reference directory {REFERENCE_DIR}: {e}")
        return
    yaml_content = read_file_content(YAML_METADATA_PATH)
    json_content = read_file_content(JSON_METADATA_PATH)
    csv_files_in_data_dir = find_csv_files(DATA_DIR)
    csv_head_contents = {}
    for csv_file in csv_files_in_data_dir:
        head_content = read_csv_head(csv_file, CSV_HEAD_ROWS)
        if head_content is not None:
            csv_head_contents[csv_file] = head_content
    try:
        with open(OUTPUT_PATH, 'w', encoding='utf-8') as outfile:
            logger.info(f"Opened output file for writing: {OUTPUT_PATH}")
            outfile.write("# Metadata and Data Sample Context for LLM Assistant\n\n")
            outfile.write("This file contains the content of key metadata files and the head rows of key data files\n")
            outfile.write("used in the 'app-personal-finance' project.\n\n")
            outfile.write("-" * 70)
            outfile.write("\n\n")
            outfile.write(f"# --- Content from: {YAML_METADATA_PATH.relative_to(ROOT_DIR).as_posix()} ---\n\n")
            if yaml_content is not None:
                outfile.write(yaml_content)
                logger.info(f"Wrote content from {YAML_METADATA_PATH.name}")
            else:
                outfile.write(f"# ERROR: Could not read content from {YAML_METADATA_PATH.name}\n")
                logger.error(f"Failed to write content for {YAML_METADATA_PATH.name} as it could not be read.")
            outfile.write("\n\n")
            outfile.write("-" * 70)
            outfile.write("\n\n")
            outfile.write(f"# --- Content from: {JSON_METADATA_PATH.relative_to(ROOT_DIR).as_posix()} ---\n\n")
            if json_content is not None:
                try:
                    parsed_json = json.loads(json_content)
                    outfile.write(json.dumps(parsed_json, indent=4))
                except json.JSONDecodeError:
                    logger.warning(f"Could not parse JSON from {JSON_METADATA_PATH.name}, writing raw content.")
                    outfile.write(json_content)
                logger.info(f"Wrote content from {JSON_METADATA_PATH.name}")
            else:
                outfile.write(f"# ERROR: Could not read content from {JSON_METADATA_PATH.name}\n")
                logger.error(f"Failed to write content for {JSON_METADATA_PATH.name} as it could not be read.")
            outfile.write("\n\n")
            outfile.write("-" * 70)
            outfile.write("\n\n")
            outfile.write(f"# --- Data Samples (Top {CSV_HEAD_ROWS} Rows) from CSV files in {DATA_DIR.relative_to(ROOT_DIR).as_posix()}/ ---\n\n")
            if not csv_head_contents:
                 outfile.write("# (No CSV files found or readable in the data directory)\n")
            else:
                for csv_file, head_content in csv_head_contents.items():
                     outfile.write(f"# --- Head of: {csv_file.relative_to(ROOT_DIR).as_posix()} ---\n\n")
                     outfile.write(head_content)
                     outfile.write("\n\n")
                     logger.info(f"Wrote head content from {csv_file.name}")
            outfile.write("\n") # Final newline
        logger.info(f"✅ Successfully generated metadata & data sample context file: {OUTPUT_PATH}")
    except Exception as e:
        logger.exception(f"An critical error occurred during file generation: {e}")
        print(f"❌ Error generating context file. Check logs. Error: {e}")
    logger.info("--- Metadata & Data Head Consolidation Complete ---")
if __name__ == "__main__":
    try:
        import yaml
        import pandas
    except ImportError as import_err:
        print(f"Error: Missing required library - {import_err.name}")
        print("Please install required libraries using:")
        print("pip install pyyaml pandas")
        exit(1)
    main()


#PY File: test_llm.py
@path: reference/test_llm.py
@summary: Full Python code (compressed) included below.
@code:
"""
test_llm.py - Lists models or tests a specific LLM.
# (Keep the rest of the docstring)
"""
import os
import logging
import argparse
from pathlib import Path
from typing import Optional, List
import sys
try: from langchain_openai import ChatOpenAI; OPENAI_LC_INSTALLED = True
except ImportError: ChatOpenAI = None; OPENAI_LC_INSTALLED = False
from dotenv import load_dotenv
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
DATA_DIR = ROOT_DIR / "data"
CSV_PATH = DATA_DIR / "expenses.csv"
ENV_PATH = ROOT_DIR / ".env"
CSV_HEAD_ROWS = 5
load_dotenv()
def list_google_models() -> List[str]:
    logger.info("Attempting to list Google Generative AI models...")
    model_names = []
    try: import google.generativeai as genai
    except ImportError: print("\nERROR: 'google-generativeai' not installed."); return model_names
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key: logger.error("GOOGLE_API_KEY not found"); print("\nERROR: GOOGLE_API_KEY not set."); return model_names
    try:
        genai.configure(api_key=api_key); logger.debug("Google AI SDK configured.")
        for model in genai.list_models():
            if 'generateContent' in model.supported_generation_methods or \
               'models/gemini' in model.name or 'models/gemma' in model.name: model_names.append(model.name)
        logger.info(f"Found {len(model_names)} potential Google models.")
    except Exception as e: logger.error(f"Failed list: {e}", exc_info=True); print(f"\nERROR: Google list: {e}")
    print("\n--- Available Google Models (Gemini/Gemma families) ---")
    if model_names: [print(f"- {name}") for name in sorted(model_names)]
    else: print("No Google models found or error occurred.")
    print("-" * 55); return model_names
def list_openai_models() -> List[str]:
    logger.info("Attempting to list OpenAI models...")
    model_names = []
    try: from openai import OpenAI, OpenAIError
    except ImportError: print("\nERROR: 'openai' not installed."); return model_names
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key: logger.error("OPENAI_API_KEY not found"); print("\nERROR: OPENAI_API_KEY not set."); return model_names
    try:
        client = OpenAI(api_key=api_key); logger.debug("OpenAI client initialized.")
        models_response = client.models.list(); logger.debug("Received OpenAI model list.")
        for model in models_response:
             if hasattr(model, 'id') and \
                ('gpt' in model.id or 'text-' in model.id) and \
                not any(x in model.id for x in ['embed','whisper','dall-e','tts','vision','image']):
                  model_names.append(model.id)
        logger.info(f"Found {len(model_names)} potential OpenAI text models.")
    except OpenAIError as e: logger.error(f"OpenAI API error: {e}", exc_info=True); print(f"\nERROR: OpenAI API list: {e}")
    except Exception as e: logger.error(f"Failed list: {e}", exc_info=True); print(f"\nERROR: OpenAI list: {e}")
    print("\n--- Available OpenAI Models (GPT/Text families) ---")
    if model_names: [print(f"- {name}") for name in sorted(model_names)]
    else: print("No relevant OpenAI models found or error occurred.")
    print("-" * 55); return model_names
def load_csv_sample(file_path: Path, num_rows: int) -> Optional[str]:
    """Loads the head of the CSV file and returns it as a plain string."""
    try: import pandas as pd
    except ImportError: logger.error("Pandas failed import in load_csv_sample."); print("\nERROR: Pandas required."); return None
    logger.info(f"Loading first {num_rows} rows from {file_path}...")
    if not file_path.is_file(): logger.error(f"CSV not found: {file_path}"); print(f"\nERROR: Data file not found: {file_path}"); return None
    try:
        df = pd.read_csv(file_path, nrows=num_rows, on_bad_lines='warn')
        logger.info(f"Read head ({len(df)} rows).");
        if df.empty: logger.warning("CSV empty."); return "(CSV Sample is Empty)"
        return df.to_string(index=False)
    except pd.errors.EmptyDataError: logger.warning(f"CSV empty (read error): {file_path}"); return "(CSV Sample is Empty)"
    except Exception as e: logger.error(f"Error reading CSV: {e}", exc_info=True); print(f"\nERROR: Read CSV: {e}"); return None
def test_model_with_data(model_name: str, data_sample_str: str) -> None:
    """Tests the specified LLM model by asking it to sum amounts."""
    try: from langchain_google_genai import ChatGoogleGenerativeAI
    except ImportError: ChatGoogleGenerativeAI = None
    try: from langchain_openai import ChatOpenAI
    except ImportError: ChatOpenAI = None
    try: from langchain_core.exceptions import LangChainException
    except ImportError: LangChainException = Exception # Fallback
    logger.info(f"Testing model '{model_name}' with data sample.")
    llm = None; api_key = None; provider = "Unknown"
    if ('gemini' in model_name or 'gemma' in model_name):
        provider = "Google"
        if not ChatGoogleGenerativeAI: print(f"\nERROR: LangChain Google wrapper missing for {model_name}."); return
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key: logger.error("GOOGLE_API_KEY not found"); print("\nERROR: GOOGLE_API_KEY not set."); return
        try: llm = ChatGoogleGenerativeAI(model=model_name.replace('models/', ''), google_api_key=api_key, temperature=0); logger.info(f"Initialized Google LC client: {model_name}")
        except Exception as e: logger.error(f"Failed init: {e}", exc_info=True); print(f"\nERROR: Init Google client: {e}"); return
    elif ('gpt' in model_name or 'text-' in model_name):
        provider = "OpenAI"
        if not ChatOpenAI: print(f"\nERROR: LangChain OpenAI wrapper missing for {model_name}."); return
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key: logger.error("OPENAI_API_KEY not found"); print("\nERROR: OPENAI_API_KEY not set."); return
        try: llm = ChatOpenAI(model=model_name, openai_api_key=api_key, temperature=0); logger.info(f"Initialized OpenAI LC client: {model_name}")
        except Exception as e: logger.error(f"Failed init: {e}", exc_info=True); print(f"\nERROR: Init OpenAI client: {e}"); return
    else: logger.error(f"Unknown provider/wrapper: {model_name}"); print(f"\nERROR: Unknown model/lib: '{model_name}'."); return
    if llm:
        print(f"\n--- Testing Model: {model_name} ({provider}) ---")
        prompt = f"""Given the following data sample as plain text:
{data_sample_str}
Calculate the sum of the 'amount' column from the data shown above.
Respond ONLY with the final numerical total. Do not include explanations, currency symbols, commas, or any other text.
"""
        print("Sending request...")
        try:
            response = llm.invoke(prompt)
            if hasattr(response, 'content'): llm_response_content = response.content
            elif isinstance(response, str): llm_response_content = response
            else: llm_response_content = str(response)
            llm_response_content = llm_response_content.strip()
            logger.info(f"Received response from {model_name}: '{llm_response_content}'")
            print("\n--- LLM Response ---"); print(llm_response_content); print("-" * 20)
            try: float(llm_response_content.replace(',', '')); print("✅ Numerical.")
            except ValueError: print("⚠️ Not numerical.")
        except LangChainException as e: logger.error(f"LangChain error: {e}", exc_info=True); print(f"\nERROR: LangChain call: {e}")
        except Exception as e: logger.error(f"Unexpected error: {e}", exc_info=True); print(f"\nERROR: Unexpected call: {e}")
        print("-" * 55)

# --- Main Execution Logic ---
def main():
    logger.info(f"--- Running test_llm.py using Python: {sys.executable} ---")
    parser = argparse.ArgumentParser(
        description="List available LLMs or test a specific LLM with sample data.",
        usage="%(prog)s [<model_name>]"
    )
    parser.add_argument(
        "model_name", nargs='?', type=str,
        help="Optional: Model name to test (e.g., 'models/gemini-1.5-flash-latest', 'gpt-4o')."
    )
    args = parser.parse_args()

    if args.model_name:
        # Mode 2: Test model - Check Pandas dependency HERE
        try:
             import pandas as pd # Attempt import right before use
             logger.info("Pandas imported successfully within main() for testing.")
        except ImportError:
             logger.error("Pandas import failed within main().")
             print("\nError: Pandas library is required for testing mode but failed to import.")
             print("Please ensure 'pandas' is installed correctly in the environment.")
             return # Exit if pandas cannot be imported

        print(f"Attempting to test model: {args.model_name}")
        data_sample = load_csv_sample(CSV_PATH, CSV_HEAD_ROWS)
        if data_sample:
            test_model_with_data(args.model_name, data_sample)
        else:
            print("\nCannot perform model test because data sample failed to load (or Pandas missing).")
    else:
        # Mode 1: List models
        print("No model specified...")
        list_google_models()
        list_openai_models()
        print("\nTo test a model, run: python test_llm.py \"<model_name>\"")

if __name__ == "__main__":
    main()
    print("\n--- Script Finished ---")


#PY File: test_llm_gemini.py
@path: reference/test_llm_gemini.py
@summary: Full Python code (compressed) included below.
@code:
"""
Basic test script using LangGraph and Gemini. Includes model listing.
"""
import pandas as pd
import os
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, TypedDict, Annotated
import operator
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
ENV_PATH = PROJECT_ROOT / ".env"
if ENV_PATH.exists(): load_dotenv(dotenv_path=ENV_PATH); logging.info(".env file loaded.")
else: logging.warning(f".env file not found at {ENV_PATH}.")
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    logging.error("GOOGLE_API_KEY not found. Exiting.")
    exit()
else:
    try:
        genai.configure(api_key=api_key)
        logging.info("GOOGLE_API_KEY found and Google AI SDK configured.")
    except Exception as e:
        logging.error(f"Failed to configure Google AI SDK: {e}", exc_info=True)
        exit()
print("\n--- Available Google Generative AI Models (for generateContent) ---")
print("-" * 60)
models_available_for_chat = []
try:
    models_listed = False
    for model in genai.list_models():
        if 'generateContent' in model.supported_generation_methods:
            models_listed = True
            print(f"- {model.name} ({model.display_name})")
            models_available_for_chat.append(model.name) # Store usable model names
    if not models_listed:
        print("No models supporting 'generateContent' found for your API key.")
except Exception as e:
    logging.error(f"Failed to list models: {e}", exc_info=True)
    print(f"\nError: Could not retrieve model list.")
print("-" * 60)
DATA_FILE = PROJECT_ROOT / "data/expenses.csv"
df = pd.DataFrame()
df_sample = "" # Initialize
try:
    if DATA_FILE.exists():
        df = pd.read_csv(DATA_FILE)
        df_sample = df[['date', 'category', 'sub_category', 'user', 'amount']].head().to_markdown(index=False)
        logging.info(f"Loaded data sample from {DATA_FILE}")
    else:
        logging.error(f"Data file not found at {DATA_FILE}. Cannot proceed with LLM test.")
except Exception as e:
    logging.error(f"Error loading data file {DATA_FILE}: {e}", exc_info=True)
class SimpleAgentState(TypedDict):
    question: str
    data_summary: str
    llm_response: str
def get_user_question(state: SimpleAgentState) -> Dict:
    logging.info("Node: get_user_question")
    return {}
def call_gemini(state: SimpleAgentState) -> Dict:
    logging.info("Node: call_gemini")
    question = state.get('question', '')
    data_summary = state.get('data_summary', '')
    if not question: return {"llm_response": "Error: Question missing."}
    if not data_summary: logging.warning("Data summary is missing.")
    model_to_use = "gemini-1.5-flash" # LangChain often handles the 'models/' prefix
    full_model_name_check = f"models/{model_to_use}" # Check with prefix
    if models_available_for_chat and full_model_name_check not in models_available_for_chat:
         logging.warning(f"Model '{model_to_use}' (as {full_model_name_check}) was not found in the list of models supporting 'generateContent'. LangChain might still work or might default.")
    try:
        llm = ChatGoogleGenerativeAI(model=model_to_use)
        logging.info(f"Initialized LangChain Gemini model: {llm.model}")
    except Exception as e:
        logging.error(f"Failed to initialize ChatGoogleGenerativeAI model: {e}", exc_info=True)
        return {"llm_response": f"Error: Could not initialize LangChain LLM - {e}"}
    prompt = f"""
You are a helpful assistant analyzing personal expense data.
Answer the following question based *only* on the provided data summary.
Data Summary (First 5 Rows):
{data_summary}
Question: {question}
Answer:
"""
    logging.info(f"Sending prompt to {model_to_use}...")
    try:
        response = llm.invoke(prompt)
        logging.info(f"Received response from {model_to_use}.")
        llm_response_content = response.content if hasattr(response, 'content') else str(response)
        return {"llm_response": llm_response_content}
    except Exception as e:
        logging.error(f"Error during LLM call with {model_to_use}: {e}", exc_info=True)
        return {"llm_response": f"Error: Failed to get response from LLM - {e}"}

# --- Define Graph ---
workflow = StateGraph(SimpleAgentState)
workflow.add_node("fetch_question", get_user_question)
workflow.add_node("generate_answer", call_gemini)
workflow.set_entry_point("fetch_question")
workflow.add_edge("fetch_question", "generate_answer")
workflow.add_edge("generate_answer", END)
app = workflow.compile()
logging.info("LangGraph compiled.")

# --- Run the Test ---
if __name__ == "__main__":
    print("\n" + "-" * 30)
    print("--- Basic LangGraph Gemini Test ---")
    print("-" * 30)

    # Only proceed if data was loaded successfully for the test part
    if df_sample:
        test_question = "List the categories present in the sample data."
        print(f"Test Question: {test_question}")
        initial_state: SimpleAgentState = {
            "question": test_question,
            "data_summary": df_sample,
            "llm_response": ""
        }
        print("\nInvoking LangGraph...")
        try:
            final_state = app.invoke(initial_state)
            print("\n--- Final LLM Response ---")
            print(final_state.get("llm_response", "No response generated."))
        except Exception as e:
            print(f"\nError invoking LangGraph: {e}")
            logging.error("Error during graph invocation", exc_info=True)
    else:
        print("\nSkipping LangGraph invocation as data sample failed to load.")

    print("-" * 30)


#PY File: test_llm_openai.py
@path: reference/test_llm_openai.py
@summary: Full Python code (compressed) included below.
@code:
import os
from openai import OpenAI
from dotenv import load_dotenv
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
def list_openai_models():
    """Connects to OpenAI using the API key from environment variables
    and lists the available models.
    """
    load_dotenv()
    logging.info("Attempting to load environment variables from .env file.")
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logging.error("FATAL: OPENAI_API_KEY environment variable not set.")
        print("\nError: OPENAI_API_KEY not found.")
        print("Please ensure you have the key set in your environment variables or in a .env file.")
        return # Stop execution if key is missing
    logging.info("OPENAI_API_KEY found.")
    try:
        client = OpenAI(api_key=api_key)
        logging.info("OpenAI client initialized successfully.")
    except Exception as e:
        logging.error(f"Failed to initialize OpenAI client: {e}", exc_info=True)
        print(f"\nError initializing OpenAI client: {e}")
        return # Stop execution if client fails
    try:
        logging.info("Attempting to fetch the list of available models...")
        models_response = client.models.list()
        logging.info(f"Successfully received model list response.") # Type: {type(models_response)}
        print("\nAvailable OpenAI Models:")
        print("-" * 25)
        count = 0
        for model in models_response:
            if hasattr(model, 'id'):
                print(f"- {model.id}")
                count += 1
            else:
                logging.warning(f"Encountered model data without an 'id': {model}")
        print("-" * 25)
        logging.info(f"Successfully listed {count} models.")
        if count == 0:
             print("No models were listed. Check API key permissions or connection.")
    except Exception as e:
        logging.error(f"An error occurred while fetching or listing models: {e}", exc_info=True)
        print(f"\nAn error occurred while fetching models: {e}")
        print("Please check your API key, network connection, and OpenAI service status.")
if __name__ == "__main__":
    print("--- Testing OpenAI Model Listing ---")
    list_openai_models()
    print("\n--- Test Complete ---")


#PY File: __init__.py
@path: streamlit/__init__.py
@summary: Full Python code (compressed) included below.
@code:



#PY File: db_utils.py
@path: streamlit/db_utils.py
@summary: Full Python code (compressed) included below.
@code:
import sqlite3
import pandas as pd
from uuid import uuid4
from pathlib import Path # Use pathlib
import logging
from typing import Optional, Dict, Any, List
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
PROJECT_ROOT = Path(__file__).parent.parent
DB_PATH = PROJECT_ROOT / "data" / "expenses.db"
if not DB_PATH.exists():
    logging.error(f"DATABASE NOT FOUND at expected location: {DB_PATH.resolve()}")
    logging.error(f"(Calculated from: {__file__})")
else:
    logging.info(f"Using database at: {DB_PATH.resolve()}")
def get_connection() -> Optional[sqlite3.Connection]:
    """
    Establishes a connection to the SQLite database.

    Returns:
        Optional[sqlite3.Connection]: A connection object or None if connection fails.
    """
    try:
        conn = sqlite3.connect(DB_PATH, check_same_thread=False)
        conn.row_factory = sqlite3.Row  # Return rows as dictionary-like objects
        logging.debug(f"Database connection established to {DB_PATH.resolve()}")
        return conn
    except sqlite3.Error as e:
        logging.error(f"Database connection error to {DB_PATH.resolve()}: {e}", exc_info=True)
        return None
def fetch_all_expenses() -> pd.DataFrame:
    """Fetches all expenses, ordered by date descending."""
    conn = get_connection()
    if conn is None:
        logging.error("Cannot fetch expenses: Database connection failed.")
        return pd.DataFrame()
    try:
        query = "SELECT id, date, year, month, week, day_of_week, account, category, sub_category, type, user, amount FROM expenses ORDER BY date DESC"
        df = pd.read_sql(query, conn)
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        logging.info(f"Fetched {len(df)} expenses.")
        return df
    except (sqlite3.Error, pd.errors.DatabaseError) as e:
        logging.error(f"Error fetching all expenses: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()
def fetch_expense_by_id(expense_id: str) -> Optional[Dict[str, Any]]:
    """Fetches a single expense by its ID."""
    conn = get_connection()
    if conn is None:
        logging.error(f"Cannot fetch expense {expense_id}: Database connection failed.")
        return None
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM expenses WHERE id = ?", (expense_id,))
        record = cursor.fetchone()
        logging.debug(f"Fetched expense for ID {expense_id}: {'Found' if record else 'Not Found'}")
        return dict(record) if record else None
    except sqlite3.Error as e:
        logging.error(f"Error fetching expense by ID {expense_id}: {e}", exc_info=True)
        return None
    finally:
        if conn: conn.close()
def insert_expense(data: Dict[str, Any]) -> bool:
    """
    Inserts a new expense record into the database.

    Args:
        data (Dict[str, Any]): Dictionary containing expense details.
                               Must include all required fields.

    Returns:
        bool: True if insertion was successful, False otherwise.
    """
    conn = get_connection()
    if conn is None:
        logging.error("Cannot insert expense: Database connection failed.")
        return False
    required_fields = ['date', 'year', 'month', 'week', 'day_of_week',
                       'account', 'category', 'sub_category', 'type',
                       'user', 'amount']
    missing_fields = [field for field in required_fields if field not in data]
    if missing_fields:
        logging.error(f"Missing required fields for inserting expense: {', '.join(missing_fields)}. Data provided: {list(data.keys())}")
        return False
    sql = """
    INSERT INTO expenses (
        id, date, year, month, week, day_of_week,
        account, category, sub_category, type, user, amount
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """

    try:
        cursor = conn.cursor()
        new_id = str(uuid4()) # Generate UUID here
        # Prepare values in the correct order, ensuring types
        values = (
            new_id,
            data['date'], # Assume already string in 'YYYY-MM-DD' format
            int(data['year']),
            data['month'],
            data['week'],
            data['day_of_week'],
            data['account'],
            data['category'],
            # Handle potentially missing/empty sub_category gracefully
            data.get('sub_category', ''), # Use .get for optional fields if applicable
            data['type'],
            data['user'],
            float(data['amount'])
        )
        cursor.execute(sql, values)
        conn.commit()
        logging.info(f"✅ Expense inserted with ID: {new_id}")
        return True
    except (sqlite3.Error, ValueError, TypeError) as e: # Catch potential type errors too
        logging.error(f"Error inserting expense: {e}", exc_info=True)
        conn.rollback()
        return False
    finally:
        if conn:
            conn.close()

def update_expense(expense_id: str, data: Dict[str, Any]) -> bool:
    """Updates an existing expense based on its ID."""
    conn = get_connection()
    if conn is None:
        logging.error(f"Cannot update expense {expense_id}: Database connection failed.")
        return False

    # Define fields allowed for update (excluding id, maybe others)
    updatable_fields = ['date', 'year', 'month', 'week', 'day_of_week', 'account', 'category', 'sub_category', 'type', 'user', 'amount']
    # Construct SET clause dynamically from provided data, only using allowed fields
    set_parts = []
    values = []
    for field in updatable_fields:
        if field in data:
            set_parts.append(f"{field} = ?")
            # Basic type conversion/validation (more robust needed for production)
            if field in ['year']:
                values.append(int(data[field]))
            elif field in ['amount']:
                values.append(float(data[field]))
            else:
                values.append(data[field])

    if not set_parts:
        logging.warning(f"No valid fields provided for updating expense ID {expense_id}.")
        return False

    set_clause = ", ".join(set_parts)
    sql = f"UPDATE expenses SET {set_clause} WHERE id = ?"
    values.append(expense_id) # Add the ID for the WHERE clause

    try:
        cursor = conn.cursor()
        cursor.execute(sql, tuple(values))
        conn.commit()
        if cursor.rowcount == 0:
            logging.warning(f"No expense found with ID {expense_id} to update.")
            return False
        logging.info(f"Expense {expense_id} updated successfully.")
        return True
    except (sqlite3.Error, ValueError, TypeError) as e:
        logging.error(f"Error updating expense {expense_id}: {e}", exc_info=True)
        conn.rollback()
        return False
    finally:
        if conn: conn.close()

def delete_expense(expense_id: str) -> bool:
    """Deletes an expense record by its ID."""
    conn = get_connection()
    if conn is None:
        logging.error(f"Cannot delete expense {expense_id}: Database connection failed.")
        return False

    sql = "DELETE FROM expenses WHERE id = ?"
    try:
        cursor = conn.cursor()
        cursor.execute(sql, (expense_id,))
        conn.commit()
        if cursor.rowcount == 0:
            logging.warning(f"No expense found with ID {expense_id} to delete.")
            return False
        logging.info(f"Expense {expense_id} deleted successfully.")
        return True
    except sqlite3.Error as e:
        logging.error(f"Error deleting expense {expense_id}: {e}", exc_info=True)
        conn.rollback()
        return False
    finally:
        if conn: conn.close()

def fetch_last_expenses(n: int = 10) -> pd.DataFrame:
    """Fetches the last N expenses, ordered by date then rowid descending."""
    conn = get_connection()
    if conn is None:
        logging.error(f"Cannot fetch last {n} expenses: Database connection failed.")
        return pd.DataFrame()
    try:
        # Order by date descending first, then rowid descending as a tie-breaker
        # Explicitly list columns
        query = f"""
            SELECT id, date, year, month, week, day_of_week, account, category, sub_category, type, user, amount
            FROM expenses
            ORDER BY date DESC, rowid DESC
            LIMIT ?
        """
        df = pd.read_sql(query, conn, params=(n,))
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        logging.info(f"Fetched last {len(df)} expenses (requested {n}).")
        return df
    except (sqlite3.Error, pd.errors.DatabaseError) as e:
        logging.error(f"Error fetching last {n} expenses: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()


#PY File: main.py
@path: streamlit/main.py
@summary: Full Python code (compressed) included below.
@code:
"""
Main Streamlit application file for the Personal Expense Tracker.
Handles page navigation and calls rendering functions for each tab.
"""
import streamlit as st
import pandas as pd
import logging
from pathlib import Path # Good practice for path handling
try:
    from tabs import add_expense, reports, visuals, assistant
    from style_utils import load_css
    from db_utils import fetch_all_expenses  # For CSV download
    st.session_state['imports_successful'] = True
    logging.info("Successfully imported UI tabs and utils.")
except ImportError as e:
    st.error(f"Failed to import necessary application components: {e}. "
             f"Please check the file structure and ensure main.py is run from the correct directory "
             f"or that the 'streamlit' package is correctly installed/recognized.")
    logging.exception("ImportError during initial module loading.")
    st.session_state['imports_successful'] = False
    st.stop() # Stop execution if core modules fail
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
st.set_page_config(
    layout="wide",
    page_title="Personal Expense Tracker",
    page_icon="💰"
)
if st.session_state.get('imports_successful', False):
    load_css()
else:
    st.warning("Could not load CSS due to import errors.")
st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "Go to",
    ["Add Expenses", "Reports", "Visualizations", "Assistant"], # Assistant is now a valid option
    label_visibility="collapsed",
    key="main_nav"
)
st.sidebar.markdown("---")
st.sidebar.header("Data Management")
if st.session_state.get('imports_successful', False): # Check if db_utils import worked
    try:
        df_all = fetch_all_expenses()
        if not df_all.empty:
            df_export = df_all.drop(columns=["id"], errors="ignore")
            csv_bytes = df_export.to_csv(index=False).encode("utf-8")
            st.sidebar.download_button(
                label="Download Data Backup (.csv)",
                data=csv_bytes,
                file_name="expenses_backup.csv",
                mime="text/csv",
                help="Download the full dataset as a CSV file"
            )
        else:
            st.sidebar.info("No expense data available to download.")
    except Exception as e:
        st.sidebar.error("Error loading data for CSV backup.")
        logging.exception("Sidebar CSV export error: %s", e)
else:
    st.sidebar.warning("Data management unavailable due to import errors.")
if st.session_state.get('imports_successful', False):
    if page == "Add Expenses":
        add_expense.render()
    elif page == "Reports":
        reports.render()
    elif page == "Visualizations":
        visuals.render()
    elif page == "Assistant":
         assistant.render() # **** CALL THE RENAMED MODULE'S FUNCTION ****
    else:
        st.error("Invalid page selected.")
else:
    pass


#PY File: style_utils.py
@path: streamlit/style_utils.py
@summary: Full Python code (compressed) included below.
@code:
import streamlit as st
import logging
from pathlib import Path # Use pathlib for robust path handling
CSS_FILE = Path(__file__).parent / "styles.css"
def load_css():
    """Loads CSS from the styles.css file located in the same directory."""
    if CSS_FILE.is_file():
        try:
            with open(CSS_FILE, "r") as f:
                css = f.read()
            st.markdown(f"<style>{css}</style>", unsafe_allow_html=True)
        except Exception as e:
            logging.error(f"Error reading CSS file {CSS_FILE}: {e}")
            st.error("Failed to load page styles.")
    else:
        logging.warning(f"CSS file not found at expected location: {CSS_FILE}")


#PY File: __init__.py
@path: streamlit/tabs/__init__.py
@summary: Full Python code (compressed) included below.
@code:



#PY File: add_expense.py
@path: streamlit/tabs/add_expense.py
@summary: Full Python code (compressed) included below.
@code:
import streamlit as st
import pandas as pd
from db_utils import insert_expense, fetch_last_expenses # Use direct import based on previous findings
import json
import datetime
from typing import Dict, Any, Optional
import logging
import time
from pathlib import Path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
METADATA_FILE_PATH = PROJECT_ROOT / "metadata" / "expense_metadata.json"
@st.cache_data
def load_metadata() -> Optional[Dict[str, Any]]:
    """Loads metadata from the project's metadata directory."""
    if not METADATA_FILE_PATH.is_file():
        logging.error(f"Metadata file not found at: {METADATA_FILE_PATH}")
        st.error(f"Critical application error: Metadata configuration file not found at {METADATA_FILE_PATH}. Please ensure it exists.")
        return None
    try:
        with open(METADATA_FILE_PATH, "r") as f:
            metadata = json.load(f)
            logging.info(f"Metadata loaded successfully from {METADATA_FILE_PATH}")
            return metadata
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding JSON from {METADATA_FILE_PATH}: {e}", exc_info=True)
        st.error(f"Critical application error: Metadata file ({METADATA_FILE_PATH.name}) seems corrupted.")
        return None
    except Exception as e:
        logging.exception(f"Failed to load or parse metadata from {METADATA_FILE_PATH}: {e}")
        st.error("Critical application error: An unexpected error occurred while loading metadata.")
        return None
def render():
    """Renders the Add Expense page."""
    if "trigger_rerun" in st.session_state and time.time() > st.session_state["trigger_rerun"]:
        st.session_state.pop("trigger_rerun", None)
        st.rerun()
    st.subheader("Add New Expense")
    metadata = load_metadata()
    if metadata is None:
        return
    all_accounts = metadata.get("Account", [])
    category_map = metadata.get("categories", {})
    all_categories = sorted(list(category_map.keys()))
    user_map = metadata.get("User", {})
    if not all_accounts or not all_categories or not category_map or not user_map:
        st.error("Metadata structure is invalid or incomplete. Cannot proceed.")
        logging.error("Invalid metadata structure detected after loading.")
        return
    expense_date = st.date_input("Date of Expense", value=datetime.date.today(), key="add_date")
    selected_category = st.selectbox("Category", options=all_categories, index=0, key="add_category")
    available_subcategories = sorted(category_map.get(selected_category, []))
    with st.form("expense_form", clear_on_submit=True):
        col1, col2 = st.columns(2)
        with col1:
            selected_account = st.selectbox("Account", options=all_accounts, key="add_account")
            subcat_disabled = not bool(available_subcategories)
            selected_sub_category = st.selectbox(
                "Sub-category",
                options=available_subcategories,
                key="add_sub_category", # Key remains the same
                disabled=subcat_disabled,
                help="Select a sub-category if applicable." if not subcat_disabled else "No sub-categories for this category."
            )
        with col2:
            expense_type = st.text_input("Type (Description)", max_chars=60, key="add_type", help="Enter a brief description of the expense.")
            expense_amount = st.number_input("Amount (INR)", min_value=0.01, format="%.2f", step=10.0, key="add_amount") # Key remains the same
        submitted = st.form_submit_button("Add Expense")
        if submitted:
            is_valid = True
            expense_user = user_map.get(selected_account, "Unknown") # Derive user here
            if not expense_type.strip():
                st.toast("⚠️ Please enter a Type/Description.", icon="⚠️"); is_valid = False
            if expense_amount <= 0:
                 st.toast("⚠️ Amount must be greater than zero.", icon="⚠️"); is_valid = False
            if available_subcategories and not selected_sub_category:
                st.toast("⚠️ Please select a Sub-category.", icon="⚠️"); is_valid = False
            if is_valid:
                final_sub_category = selected_sub_category if available_subcategories else ""
                dt = pd.to_datetime(expense_date)
                expense_data = {
                    "date": dt.strftime("%Y-%m-%d"), "year": dt.year,
                    "month": dt.to_period("M").strftime("%Y-%m"), "week": dt.strftime("%G-W%V"),
                    "day_of_week": dt.day_name(), "account": selected_account,
                    "category": selected_category, "sub_category": final_sub_category,
                    "type": expense_type.strip(), "user": expense_user, "amount": expense_amount
                }
                success = insert_expense(expense_data)
                if success:
                    st.toast("✅ Expense added successfully!", icon="✅")
                    st.session_state["last_added"] = expense_data
                    st.session_state["highlight_time"] = time.time()
                else:
                    st.toast("❌ Failed to save expense to the database.", icon="❌")
    if "last_added" in st.session_state and "highlight_time" in st.session_state:
         if time.time() - st.session_state["highlight_time"] <= 5:
              st.success("Entry saved successfully!") # Show success message briefly
         else:
              st.session_state.pop("last_added", None)
              st.session_state.pop("highlight_time", None)
    st.markdown("---")
    st.subheader("Last 10 Expenses Added")
    try:
        df = fetch_last_expenses(10)
        if df.empty:
            st.info("No recent expenses recorded yet.")
        else:
            highlight_index = None
            last_added_data = st.session_state.get("last_added")
            highlight_start_time = st.session_state.get("highlight_time")
            if highlight_start_time and (time.time() - highlight_start_time > 5):
                 st.session_state.pop("last_added", None)
                 st.session_state.pop("highlight_time", None)
                 last_added_data = None
            if last_added_data:
                match = df[
                    (df["date"].dt.strftime('%Y-%m-%d') == last_added_data["date"]) &
                    (df["account"] == last_added_data["account"]) &
                    (df["category"] == last_added_data["category"]) &
                    (df["sub_category"].fillna("") == last_added_data["sub_category"]) &
                    (df["type"] == last_added_data["type"]) &
                    (df["user"] == last_added_data["user"]) &
                    (df["amount"].round(2) == round(float(last_added_data["amount"]), 2))
                ]
                if not match.empty:
                    highlight_index = match.index[0]
            display_df = df.drop(columns=["id", "year", "month", "week", "day_of_week"], errors="ignore").rename(columns={
                "date": "Date", "account": "Account", "category": "Category",
                "sub_category": "Sub Category", "type": "Type", "user": "User", "amount": "Amount (INR)"
            })
            def highlight_row_conditionally(row):
                is_highlighted = row.name == highlight_index
                return ['background-color: #d1ffd6' if is_highlighted else '' for _ in row]
            st.dataframe(
                display_df.style
                    .format({"Date": "{:%Y-%m-%d}", "Amount (INR)": "₹{:.2f}"})
                    .apply(highlight_row_conditionally, axis=1),
                use_container_width=True, height=380, hide_index=True
            )
    except Exception as e:
        logging.exception("Failed to display recent expenses table")
        st.error(f"Error loading recent expenses: {e}")


#PY File: assistant.py
@path: streamlit/tabs/assistant.py
@summary: Full Python code (compressed) included below.
@code:
import streamlit as st
import requests
import json
import plotly.io as pio
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
LANGSERVE_INVOKE_URL = "http://localhost:8000/assistant/invoke" # Use invoke endpoint
def call_assistant_api(query: str) -> dict | None:
    """Sends the query to the LangServe invoke endpoint and returns the parsed output."""
    payload = {"input": {"original_query": query}}
    headers = {"Content-Type": "application/json", "Accept": "application/json"}
    try:
        logger.info(f"Sending request to API: {LANGSERVE_INVOKE_URL} with query: '{query}'")
        response = requests.post(LANGSERVE_INVOKE_URL, json=payload, headers=headers, timeout=120)
        response.raise_for_status()
        response_data = response.json()
        logger.info(f"API Raw Response: {response_data}")
        if "output" in response_data:
            api_output = response_data.get("output", {})
            logger.info(f"Extracted API Output: {api_output}")
            if not isinstance(api_output, dict):
                 logger.error(f"API 'output' key did not contain a dictionary. Response: {response_data}")
                 st.error("Received an unexpected response format from the assistant (output is not a dict).")
                 return None
            return api_output
        else:
            logger.error(f"API response missing 'output' key. Response: {response_data}")
            st.error("Received an unexpected response format from the assistant.")
            return None
    except requests.exceptions.RequestException as e:
        logger.error(f"API call failed: {e}", exc_info=True)
        st.error(f"Failed to connect to the assistant backend: {e}")
        return None
    except json.JSONDecodeError:
        logger.error(f"Failed to decode API JSON response: {response.text}")
        st.error("Received an invalid response from the assistant (not valid JSON).")
        return None
    except Exception as e:
         logger.error(f"An unexpected error occurred during API call: {e}", exc_info=True)
         st.error(f"An unexpected error occurred: {e}")
         return None
def render():
    """Renders the Assistant tab using the invoke endpoint."""
    st.subheader("💰 Personal Finance Assistant")
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Greetings! How can I help you with your finances today?"}
        ]
    logger.debug(f"Displaying {len(st.session_state.messages)} messages from history.")
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"]) # Display only text content
    if prompt := st.chat_input("Ask me about your expenses..."):
        logger.info(f"User input received: '{prompt}'")
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("Thinking...")
            api_response = call_assistant_api(prompt)
            if api_response:
                final_response = api_response.get("final_response", "Sorry, I couldn't generate a response.")
                chart_json = api_response.get("chart_json")
                sql_query = api_response.get("sql_query")
                error_msg = api_response.get("error")
                if error_msg:
                    logger.error(f"Assistant API returned an error: {error_msg}")
                    final_response = f"An error occurred: {error_msg}"
                    message_placeholder.error(final_response)
                else:
                    message_placeholder.markdown(final_response) # Fill the text placeholder
                assistant_message_data = {
                    "role": "assistant",
                    "content": final_response,
                    "chart_json": chart_json,
                    "sql_query": sql_query
                }
                st.session_state.messages.append(assistant_message_data)
                if chart_json or sql_query: # Only create columns if there's something to show
                    viz_col, sql_col = st.columns([0.6, 0.4])
                    placeholder_viz = viz_col.empty()
                    placeholder_sql = sql_col.empty()
                    with placeholder_viz:
                        if chart_json and not error_msg:
                            try:
                                chart_fig = pio.from_json(chart_json)
                                st.plotly_chart(chart_fig, use_container_width=True)
                                logger.info("Chart displayed successfully.")
                            except Exception as e:
                                logger.error(f"Error rendering chart JSON: {e}")
                                st.warning("Could not display the generated chart.")
                    with placeholder_sql:
                        if sql_query and not error_msg :
                            with st.expander("View Generated SQL", expanded=False):
                                 st.code(sql_query, language="sql")
                                 logger.info("SQL query displayed.")
            else:
                 message_placeholder.error("Failed to get a response from the assistant.")
                 st.session_state.messages.append({
                     "role": "assistant",
                     "content": "Failed to get a response from the assistant."
                 })


#PY File: reports.py
@path: streamlit/tabs/reports.py
@summary: Full Python code (compressed) included below.
@code:
import streamlit as st
import pandas as pd
import datetime
import json
import logging
from typing import Dict, Any, Optional
from db_utils import fetch_all_expenses, fetch_expense_by_id, update_expense, delete_expense
from pathlib import Path
import time # Keep for short sleep after successful edit/delete
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
METADATA_FILE_PATH = PROJECT_ROOT / "metadata" / "expense_metadata.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
@st.cache_data
def load_metadata() -> Optional[Dict[str, Any]]:
    """Loads metadata from the project's metadata directory."""
    if not METADATA_FILE_PATH.is_file():
        logging.error(f"Metadata file not found at: {METADATA_FILE_PATH}")
        st.error(f"Critical application error: Metadata configuration file not found at {METADATA_FILE_PATH}. Please ensure it exists.")
        return None
    try:
        with open(METADATA_FILE_PATH, "r") as f:
            metadata = json.load(f)
            logging.info(f"Metadata loaded successfully from {METADATA_FILE_PATH}")
            return metadata
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding JSON from {METADATA_FILE_PATH}: {e}", exc_info=True)
        st.error(f"Critical application error: Metadata file ({METADATA_FILE_PATH.name}) seems corrupted. Please check its format.")
        return None
    except Exception as e:
        logging.exception(f"Failed to load or parse metadata from {METADATA_FILE_PATH}: {e}")
        st.error("Critical application error: An unexpected error occurred while loading metadata.")
        return None
@st.cache_data
def convert_df_to_csv(df: pd.DataFrame) -> bytes:
    """Converts a DataFrame to CSV bytes."""
    try:
        if 'Date' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Date']):
             df_copy = df.copy()
             df_copy['Date'] = df_copy['Date'].dt.strftime('%Y-%m-%d')
             return df_copy.to_csv(index=False).encode("utf-8")
        else:
             return df.to_csv(index=False).encode("utf-8")
    except Exception as e:
        logging.error(f"CSV conversion failed: {e}")
        st.error("Failed to generate CSV data.")
        return b""
def render():
    """Renders the Reports page, handling view, edit, and delete modes."""
    st.session_state.setdefault("edit_mode", False)
    st.session_state.setdefault("delete_confirm", False)
    st.session_state.setdefault("selected_expense_id", None)
    st.session_state.setdefault("force_refresh", False)
    metadata = load_metadata()
    if metadata is None:
        return
    if st.session_state.get("force_refresh", False):
        st.session_state["force_refresh"] = False # Reset the flag immediately
        st.cache_data.clear() # Clear cache to ensure fresh data fetch
    if st.session_state.edit_mode:
        if st.session_state.selected_expense_id:
            expense = fetch_expense_by_id(st.session_state.selected_expense_id)
            if expense:
                display_edit_form(expense, metadata)
            else:
                st.error(f"Could not load expense with ID {st.session_state.selected_expense_id} to edit.")
                st.session_state.edit_mode = False
                st.session_state.selected_expense_id = None
                if st.button("Back to Report"): st.rerun()
            return
    elif st.session_state.delete_confirm:
        if st.session_state.selected_expense_id:
            expense = fetch_expense_by_id(st.session_state.selected_expense_id)
            if expense:
                display_delete_confirmation(expense)
            else:
                st.error(f"Could not load expense with ID {st.session_state.selected_expense_id} to delete.")
                st.session_state.delete_confirm = False
                st.session_state.selected_expense_id = None
                if st.button("Back to Report"): st.rerun()
            return
    render_report_view(metadata)
def render_report_view(metadata: Dict[str, Any]):
    """Displays the main report view with filters and data table."""
    st.subheader("Expense Report")
    df_all = fetch_all_expenses()
    if df_all.empty:
        st.info("No expense data available to display.")
        return
    try:
        if not pd.api.types.is_datetime64_any_dtype(df_all['date']):
             df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')
             df_all.dropna(subset=['date'], inplace=True)
        if 'month' not in df_all.columns and 'date' in df_all.columns:
             df_all['month'] = df_all['date'].dt.strftime('%Y-%m')
        required_cols = ['date', 'month', 'account', 'category', 'sub_category', 'user', 'amount', 'id', 'type']
        if not all(col in df_all.columns for col in required_cols):
             missing = [col for col in required_cols if col not in df_all.columns]
             st.error(f"Database is missing required columns: {', '.join(missing)}. Cannot generate report.")
             logging.error(f"Missing columns in fetched data: {missing}")
             return
        all_months = ["All"] + sorted(df_all['month'].unique(), reverse=True)
        all_accounts = ["All"] + sorted(metadata.get("Account", []))
        all_categories = ["All"] + sorted(list(metadata.get("categories", {}).keys()))
        all_users = ["All"] + sorted(list(set(metadata.get("User", {}).values())))
        category_map = metadata.get("categories", {})
    except Exception as e:
         st.error(f"Error preparing data or filter options: {e}")
         logging.exception("Error during data preparation in reports tab.")
         return
    st.markdown("#### Filter Options")
    month_selected = st.selectbox(
        "Filter by Month", options=all_months, index=0, key="report_month_filter"
    )
    filter_col1, filter_col2 = st.columns(2)
    with filter_col1:
        accounts_selected = st.multiselect("Filter by Account(s)", options=all_accounts, default=["All"], key="report_account_filter")
        users_selected = st.multiselect("Filter by User(s)", options=all_users, default=["All"], key="report_user_filter")
    with filter_col2:
        categories_selected = st.multiselect("Filter by Category(s)", options=all_categories, default=["All"], key="report_category_filter")
        subcats_available = set()
        if "All" in categories_selected:
            for sublist in category_map.values(): subcats_available.update(sublist)
        else:
            for cat in categories_selected: subcats_available.update(category_map.get(cat, []))
        all_subcategories_options = ["All"] + sorted(list(subcats_available))
        subcategory_selected = st.selectbox(
            "Filter by Sub-category", options=all_subcategories_options, index=0, key="report_subcategory_filter",
            help="Available sub-categories depend on selected Categories."
        )
    try:
        df_filtered = df_all.copy()
        if month_selected != "All": df_filtered = df_filtered[df_filtered['month'] == month_selected]
        if "All" not in accounts_selected: df_filtered = df_filtered[df_filtered['account'].isin(accounts_selected)]
        if "All" not in categories_selected: df_filtered = df_filtered[df_filtered['category'].isin(categories_selected)]
        if subcategory_selected != "All": df_filtered = df_filtered[df_filtered['sub_category'] == subcategory_selected]
        if "All" not in users_selected: df_filtered = df_filtered[df_filtered['user'].isin(users_selected)]
    except Exception as e:
         st.error(f"Error applying filters: {e}")
         logging.exception("Error occurred while filtering DataFrame.")
         df_filtered = pd.DataFrame()
    st.markdown("---")
    total_filtered_expense = df_filtered['amount'].sum() if not df_filtered.empty else 0
    st.markdown(f"### Total Expense (Filtered): ₹{total_filtered_expense:,.2f}")
    if not df_filtered.empty:
        st.markdown("#### Summary Statistics (Filtered)")
        num_transactions = len(df_filtered)
        avg_transaction_amount = df_filtered['amount'].mean()
        top_category_series = df_filtered.groupby("category")["amount"].sum().nlargest(1)
        top_category_display = "N/A"
        if not top_category_series.empty:
             top_category_display = f"{top_category_series.index[0]} (₹{top_category_series.values[0]:,.0f})"
        stat_col1, stat_col2, stat_col3 = st.columns(3)
        stat_col1.metric("Transactions", f"{num_transactions:,}")
        stat_col2.metric("Avg. Transaction", f"₹{avg_transaction_amount:,.2f}")
        stat_col3.metric("Top Category", top_category_display)
    elif not df_all.empty:
        st.info("No transactions match the current filter criteria.")
    st.markdown("---")
    col_title, col_refresh = st.columns([4, 1])
    with col_title:
         st.markdown("### Detailed Transactions (Filtered)")
    with col_refresh:
        if st.button("🔄 Refresh Data", key="report_refresh_btn", help="Click to reload data from database"):
            st.session_state["force_refresh"] = True
            st.rerun() # Trigger rerun, flag will be checked at the top
    if not df_filtered.empty:
        display_columns = ["date", "account", "category", "sub_category", "type", "user", "amount"]
        existing_display_cols = [col for col in display_columns if col in df_filtered.columns]
        display_df = df_filtered[existing_display_cols + ['id']].copy()
        display_df = display_df.rename(columns={
            "date": "Date", "account": "Account", "category": "Category",
            "sub_category": "Sub Category", "type": "Type", "user": "User", "amount": "Amount (INR)"
        }).sort_values("Date", ascending=False)
        st.dataframe(
            display_df.drop(columns=['id']),
            column_config={
                 "Date": st.column_config.DateColumn("Date", format="YYYY-MM-DD"),
                 "Amount (INR)": st.column_config.NumberColumn("Amount (INR)", format="₹%.2f")
            },
            use_container_width=True, height=400, hide_index=True
        )
        st.markdown("---")
        st.markdown("#### Edit / Delete Expense")
        df_selectable = display_df.copy().head(500)
        def create_display_label(row):
             date_str = row['Date'].strftime('%Y-%m-%d') if pd.notna(row['Date']) else 'N/A'
             amt_str = f"₹{row['Amount (INR)']:.0f}"
             return f"{date_str} | {row['Category']} | {row.get('Sub Category', '')[:15]} | {row.get('Type', '')[:20]} | {amt_str}"
        selector_map = {"-- Select expense to modify --": None}
        for idx, row in df_selectable.iterrows():
             label = create_display_label(row)
             unique_label = f"{label} (ID: ...{row['id'][-6:]})"
             selector_map[unique_label] = row['id']
        selected_label = st.selectbox("Select Expense", options=list(selector_map.keys()), key="report_select_expense")
        selected_id = selector_map.get(selected_label)
        edit_col, delete_col = st.columns([1, 1])
        edit_disabled = selected_id is None
        delete_disabled = selected_id is None
        with edit_col:
            if st.button("Edit Selected", key="report_edit_btn", disabled=edit_disabled):
                st.session_state.selected_expense_id = selected_id
                st.session_state.edit_mode = True
                st.rerun()
        with delete_col:
            if st.button("Delete Selected", key="report_delete_btn", disabled=delete_disabled):
                st.session_state.selected_expense_id = selected_id
                st.session_state.delete_confirm = True
                st.rerun()
        st.markdown("---")
        csv_export_df = display_df.drop(columns=['id'])
        csv_data = convert_df_to_csv(csv_export_df)
        if csv_data:
            st.download_button(
                label="📥 Download Filtered Data (.csv)", data=csv_data,
                file_name="filtered_expenses.csv", mime="text/csv", key="report_download_csv"
            )
def display_edit_form(expense_data: Dict[str, Any], metadata: Dict[str, Any]):
    """Displays the form for editing a selected expense with dynamic sub-categories and rearranged layout."""
    expense_id = expense_data.get("id", "UNKNOWN")
    expense_id_short = f"...{expense_id[-6:]}" if expense_id != "UNKNOWN" else "N/A"
    st.subheader(f"Edit Expense (ID: {expense_id_short})")
    all_categories = sorted(metadata.get("categories", {}).keys())
    all_accounts = metadata.get("Account", [])
    user_map = metadata.get("User", {})
    category_map = metadata.get("categories", {})
    session_key_category = f"edit_category_{expense_id}"
    session_key_subcat_options = f"edit_subcat_options_{expense_id}"
    session_key_subcat_index = f"edit_subcat_index_{expense_id}"
    if session_key_category not in st.session_state:
        st.session_state[session_key_category] = expense_data.get("category", all_categories[0] if all_categories else None)
    def category_change_callback():
        new_category = st.session_state[f"edit_category_widget_{expense_id}"]
        st.session_state[session_key_category] = new_category
        new_subcat_options = sorted(category_map.get(new_category, []))
        st.session_state[session_key_subcat_options] = new_subcat_options
        st.session_state[session_key_subcat_index] = 0 # Reset index on category change
    current_edit_category = st.session_state[session_key_category]
    current_subcat_options = st.session_state.get(session_key_subcat_options, sorted(category_map.get(current_edit_category, [])))
    try:
        default_date = pd.to_datetime(expense_data["date"]).date()
        default_account_index = all_accounts.index(expense_data["account"]) if expense_data["account"] in all_accounts else 0
        initial_category_index = all_categories.index(current_edit_category) if current_edit_category in all_categories else 0
        initial_subcat = expense_data.get("sub_category", "")
        initial_subcat_index = 0
        if initial_subcat and initial_subcat in current_subcat_options:
             initial_subcat_index = current_subcat_options.index(initial_subcat)
        if session_key_subcat_index not in st.session_state:
             st.session_state[session_key_subcat_index] = initial_subcat_index
        default_type = expense_data.get("type", "")
        default_amount = float(expense_data.get("amount", 0.01))
        st.markdown("---") # Separator
        new_date_input = st.date_input("Date", value=default_date, key="edit_date_outside")
        st.selectbox(
            "Category",
            options=all_categories,
            index=initial_category_index,
            key=f"edit_category_widget_{expense_id}",
            on_change=category_change_callback
        )
        st.markdown("---") # Separator
        with st.form("edit_expense_form"):
            st.markdown("#### Modify Remaining Details")
            row1_col1, row1_col2 = st.columns(2)
            with row1_col1:
                new_account_input = st.selectbox( # Changed variable name
                    "Account",
                    options=all_accounts,
                    index=default_account_index,
                    key="edit_account"
                )
            with row1_col2:
                new_type_input = st.text_input( # Changed variable name
                    "Type",
                    value=default_type,
                    key="edit_type",
                    max_chars=60
                )
            row2_col1, row2_col2 = st.columns(2)
            with row2_col1:
                 subcat_disabled = not bool(current_subcat_options)
                 current_subcat_idx = st.session_state.get(session_key_subcat_index, 0)
                 if current_subcat_idx >= len(current_subcat_options): current_subcat_idx = 0
                 new_subcat_input = st.selectbox( # Changed variable name
                      "Sub-category",
                      options=current_subcat_options,
                      index=current_subcat_idx,
                      key="edit_subcat_widget",
                      disabled=subcat_disabled,
                      help="Options update based on Category selected above."
                 )
            with row2_col2:
                 new_amount_input = st.number_input( # Changed variable name
                     "Amount (INR)",
                     value=default_amount,
                     min_value=0.01,
                     format="%.2f",
                     key="edit_amount"
                 )
            derived_user = user_map.get(new_account_input, "Unknown")
            st.text(f"User: {derived_user}") # Display derived user
            submit_col, cancel_col = st.columns([1, 1])
            with submit_col: save_changes = st.form_submit_button("Save Changes")
            with cancel_col: cancel_edit = st.form_submit_button("Cancel")
            if save_changes:
                final_category = st.session_state[session_key_category]
                final_subcat_options = sorted(category_map.get(final_category, []))
                final_subcat_selection = new_subcat_input # Read from widget
                final_date = new_date_input # Read from widget outside form
                final_account = new_account_input # Read from widget
                final_type = new_type_input # Read from widget
                final_amount = new_amount_input # Read from widget
                is_valid = True
                if not final_type.strip(): st.warning("Type cannot be empty."); is_valid = False
                if final_amount <= 0: st.warning("Amount must be positive."); is_valid = False
                if final_subcat_options and not final_subcat_selection:
                    st.warning(f"Sub-category required for '{final_category}'."); is_valid = False
                if final_subcat_selection and final_subcat_selection not in final_subcat_options:
                     st.warning(f"'{final_subcat_selection}' is not valid for '{final_category}'."); is_valid = False
                if is_valid:
                     final_dt = pd.to_datetime(final_date)
                     updated_data = {
                        "date": final_dt.strftime("%Y-%m-%d"), "year": final_dt.year,
                        "month": final_dt.strftime("%Y-%m"), "week": final_dt.strftime("%G-W%V"),
                        "day_of_week": final_dt.day_name(), "account": final_account,
                        "category": final_category,
                        "sub_category": final_subcat_selection if final_subcat_options else "",
                        "type": final_type.strip(), "user": derived_user, "amount": final_amount
                     }
                     success = update_expense(expense_data["id"], updated_data)
                     if success:
                        st.success("Expense updated successfully!")
                        for key in [session_key_category, session_key_subcat_options, session_key_subcat_index, f"edit_category_widget_{expense_id}"]:
                            if key in st.session_state: del st.session_state[key]
                        st.session_state.edit_mode = False
                        st.session_state.selected_expense_id = None
                        st.session_state["force_refresh"] = True
                        time.sleep(0.5)
                        st.rerun()
                     else:
                         st.error("Failed to update expense in the database.")
            elif cancel_edit:
                 for key in [session_key_category, session_key_subcat_options, session_key_subcat_index, f"edit_category_widget_{expense_id}"]:
                     if key in st.session_state: del st.session_state[key]
                 st.session_state.edit_mode = False
                 st.session_state.selected_expense_id = None
                 st.rerun()
    except (ValueError, IndexError, KeyError, TypeError) as e:
         st.error(f"Error preparing edit form: {e}. Data might be inconsistent or type mismatch.")
         logging.exception(f"Error preparing edit form for ID {expense_id}: {e}")
         if st.button("Back to Report"):
              st.session_state.edit_mode = False; st.session_state.selected_expense_id = None; st.rerun()
def display_delete_confirmation(expense_data: Dict[str, Any]):
    """Displays the confirmation dialog for deleting an expense."""
    st.subheader("Confirm Deletion")
    st.warning(f"⚠️ Are you sure you want to permanently delete this expense?")
    details = {
        "Date": expense_data.get('date'), "Category": expense_data.get('category'),
        "Sub Category": expense_data.get('sub_category'), "Type": expense_data.get('type'),
        "Amount": f"₹{expense_data.get('amount', 0):,.2f}", "User": expense_data.get('user'),
        "Account": expense_data.get('account'), "ID": f"...{expense_data.get('id', '')[-6:]}"
    }
    st.json(details, expanded=True)
    confirm_col, cancel_col = st.columns(2)
    with confirm_col:
        if st.button("Yes, Delete Permanently", key="confirm_delete", type="primary"):
            success = delete_expense(expense_data["id"])
            if success:
                st.success("Expense deleted successfully.")
                st.session_state.delete_confirm = False
                st.session_state.selected_expense_id = None
                st.session_state["force_refresh"] = True # Trigger refresh
                time.sleep(0.5) # Brief pause
                st.rerun() # Rerun to show updated report
            else:
                 st.error("Failed to delete expense from the database.")
    with cancel_col:
        if st.button("Cancel", key="cancel_delete"):
            st.session_state.delete_confirm = False
            st.session_state.selected_expense_id = None
            st.rerun() # Go back to the report view


#PY File: visuals.py
@path: streamlit/tabs/visuals.py
@summary: Full Python code (compressed) included below.
@code:
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json
import datetime
from db_utils import fetch_all_expenses
from typing import Dict, Any, Optional
import logging
from pathlib import Path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
METADATA_FILE_PATH = PROJECT_ROOT / "metadata" / "expense_metadata.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
@st.cache_data
def load_metadata() -> Optional[Dict[str, Any]]:
    """Loads metadata from the project's metadata directory."""
    if not METADATA_FILE_PATH.is_file():
        logging.error(f"Metadata file not found at: {METADATA_FILE_PATH}")
        st.error(f"Critical application error: Metadata configuration file not found at {METADATA_FILE_PATH}. Please ensure it exists.")
        return None
    try:
        with open(METADATA_FILE_PATH, "r") as f:
            metadata = json.load(f)
            logging.info(f"Metadata loaded successfully from {METADATA_FILE_PATH}")
            return metadata
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding JSON from {METADATA_FILE_PATH}: {e}", exc_info=True)
        st.error(f"Critical application error: Metadata file ({METADATA_FILE_PATH.name}) seems corrupted.")
        return None
    except Exception as e:
        logging.exception(f"Failed to load or parse metadata from {METADATA_FILE_PATH}: {e}")
        st.error("Critical application error: An unexpected error occurred while loading metadata.")
        return None
def get_common_layout_args(chart_title: str, show_legend: bool = False) -> Dict[str, Any]:
    """Generates common layout arguments for Plotly charts."""
    return {
        "title_text": chart_title,
        "title_font_size": 16, "title_x": 0.5,
        "margin": dict(l=20, r=20, t=50, b=80 if show_legend else 40),
        "legend": dict(orientation="h", yanchor="bottom", y=-0.3, xanchor="center", x=0.5),
        "hovermode": "closest",
        "showlegend": show_legend
    }
def render():
    """Renders the 'Visualizations' page with a 2x2 grid of charts."""
    st.subheader("Expense Visualizations")
    metadata = load_metadata()
    if metadata is None: return
    df_all = fetch_all_expenses()
    if df_all.empty:
        st.info("No expense data available for visualization.")
        return
    try:
        if not pd.api.types.is_datetime64_any_dtype(df_all['date']):
             df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')
             df_all.dropna(subset=['date'], inplace=True)
        if 'month' not in df_all.columns and 'date' in df_all.columns:
             df_all['month'] = df_all['date'].dt.strftime('%Y-%m') # Use 'month' consistently
        if 'month' in df_all.columns and 'YearMonth' not in df_all.columns:
             df_all['YearMonth'] = df_all['month']
        required_cols = ['YearMonth', 'category', 'amount', 'date', 'account', 'user', 'type', 'sub_category']
        if not all(col in df_all.columns for col in ['YearMonth', 'category', 'amount', 'date', 'account', 'user', 'type']):
             missing = [col for col in required_cols if col not in df_all.columns]
             st.error(f"Required columns missing for visualizations: {missing}")
             return
        min_date = df_all['date'].min().date()
        max_date = df_all['date'].max().date()
        all_months = ["All"] + sorted(df_all['YearMonth'].unique(), reverse=True)
        all_categories = ["All"] + sorted(list(metadata.get("categories", {}).keys()))
        all_users = ["All"] + sorted(list(set(metadata.get("User", {}).values())))
        all_accounts = ["All"] + sorted(metadata.get("Account", []))
    except Exception as e:
        st.error(f"Error preparing data or filter options: {e}")
        logging.exception("Error during data preparation in visuals tab.")
        return
    if 'legends' not in st.session_state:
        st.session_state.legends = {'pie': False, 'bar': False, 'line': False, 'top': False}
    st.markdown("#### Overview Charts")
    row1_col1, row1_col2 = st.columns(2)
    row2_col1, row2_col2 = st.columns(2)
    with row1_col1:
        st.markdown("###### By Category (Proportion)")
        with st.expander("Pie Chart Filters", expanded=False):
            pie_month = st.selectbox("Month", all_months, 0, key="pie_month_filter")
            pie_cats = st.multiselect("Category", all_categories, ["All"], key="pie_cat_filter")
            pie_accounts = st.multiselect("Account", all_accounts, ["All"], key="pie_account_filter")
            pie_users = st.multiselect("User", all_users, ["All"], key="pie_user_filter")
        if st.button("Toggle Legend - Pie", key="pie_legend_btn"):
            st.session_state.legends['pie'] = not st.session_state.legends['pie']
        pie_df = df_all.copy()
        if pie_month != "All": pie_df = pie_df[pie_df['YearMonth'] == pie_month]
        if "All" not in pie_cats: pie_df = pie_df[pie_df['category'].isin(pie_cats)]
        if "All" not in pie_accounts: pie_df = pie_df[pie_df['account'].isin(pie_accounts)]
        if "All" not in pie_users: pie_df = pie_df[pie_df['user'].isin(pie_users)]
        pie_data = pie_df.groupby('category')['amount'].sum().reset_index()
        if not pie_data.empty and pie_data['amount'].sum() > 0:
            fig_pie = px.pie(pie_data, values='amount', names='category', hole=0.4)
            fig_pie.update_traces(textposition='inside', textinfo='percent+label', hoverinfo='label+percent+value')
            fig_pie.update_layout(**get_common_layout_args("Spending by Category", st.session_state.legends['pie']))
            st.plotly_chart(fig_pie, use_container_width=True)
        elif not pie_df.empty:
             st.info("No spending in selected categories/filters for Pie Chart.")
        else:
             st.info("No data matches filters for Pie Chart.")
    with row1_col2:
        st.markdown("###### By Category (Absolute)")
        with st.expander("Bar Chart Filters", expanded=False):
            bar_start = st.date_input("Start Date", min_date, key="bar_start_filter")
            bar_end = st.date_input("End Date", max_date, key="bar_end_filter")
            bar_accounts = st.multiselect("Account", all_accounts, ["All"], key="bar_account_filter")
            bar_users = st.multiselect("User", all_users, ["All"], key="bar_user_filter")
        if st.button("Toggle Legend - Bar", key="bar_legend_btn"):
            st.session_state.legends['bar'] = not st.session_state.legends['bar']
        if bar_start > bar_end:
            st.warning("Start date cannot be after end date for Bar Chart.")
            bar_df = pd.DataFrame()
        else:
             bar_df = df_all[(df_all['date'].dt.date >= bar_start) & (df_all['date'].dt.date <= bar_end)]
             if "All" not in bar_accounts: bar_df = bar_df[bar_df['account'].isin(bar_accounts)]
             if "All" not in bar_users: bar_df = bar_df[bar_df['user'].isin(bar_users)]
        bar_data = bar_df.groupby('category')['amount'].sum().reset_index()
        if not bar_data.empty and bar_data['amount'].sum() > 0:
            fig_bar = px.bar(bar_data, x='category', y='amount', color='category', text_auto='.2s')
            layout_bar = get_common_layout_args("Total Spending by Category", st.session_state.legends['bar'])
            layout_bar["yaxis_title"] = "Amount (INR)"
            layout_bar["xaxis_title"] = "Category"
            layout_bar["xaxis"] = dict(
                categoryorder='total descending',
                tickangle=-90  # Force vertical labels
            )
            fig_bar.update_layout(**layout_bar)
            fig_bar.update_traces(textposition='outside')
            st.plotly_chart(fig_bar, use_container_width=True)
        elif not bar_df.empty:
             st.info("No spending in selected categories/filters for Bar Chart.")
        else:
             st.info("No data matches filters for Bar Chart (check dates?).")
    with row2_col1:
        st.markdown("###### Trend Over Time")
        with st.expander("Line Chart Filters", expanded=False):
            line_start = st.date_input("Start Date", min_date, key="line_start_filter")
            line_end = st.date_input("End Date", max_date, key="line_end_filter")
            line_cats = st.multiselect("Category", all_categories, ["All"], key="line_cat_filter")
            line_accounts = st.multiselect("Account", all_accounts, ["All"], key="line_account_filter")
            line_users = st.multiselect("User", all_users, ["All"], key="line_user_filter")
            line_mode = st.radio("View", ["Daily", "Cumulative"], 0, horizontal=True, key="line_mode_filter")
        if st.button("Toggle Legend - Line", key="line_legend_btn"):
            st.session_state.legends['line'] = not st.session_state.legends['line']
        if line_start > line_end:
             st.warning("Start date cannot be after end date for Line Chart.")
             line_df = pd.DataFrame()
        else:
            line_df = df_all[(df_all['date'].dt.date >= line_start) & (df_all['date'].dt.date <= line_end)]
            if "All" not in line_cats: line_df = line_df[line_df['category'].isin(line_cats)]
            if "All" not in line_accounts: line_df = line_df[line_df['account'].isin(line_accounts)]
            if "All" not in line_users: line_df = line_df[line_df['user'].isin(line_users)]
        trend_data = line_df.groupby('date')['amount'].sum().reset_index().sort_values('date')
        fig_line = go.Figure()
        trace_added = False
        if not trend_data.empty:
            if line_mode == "Daily":
                fig_line.add_trace(go.Scatter(x=trend_data['date'], y=trend_data['amount'], mode='lines+markers', name='Daily Spend'))
                trace_added = True
            elif line_mode == "Cumulative":
                trend_data['cumulative'] = trend_data['amount'].cumsum()
                fig_line.add_trace(go.Scatter(x=trend_data['date'], y=trend_data['cumulative'], mode='lines+markers', name='Cumulative Spend', line=dict(dash='dot')))
                trace_added = True
        if trace_added:
             layout_line = get_common_layout_args(f"{line_mode} Spending Trend", st.session_state.legends['line'])
             layout_line["yaxis_title"] = "Amount (INR)"
             layout_line["xaxis_title"] = "Date"
             layout_line["xaxis"] = dict(rangeslider=dict(visible=True), type="date")
             layout_line["hovermode"] = "x unified"
             fig_line.update_layout(**layout_line)
             st.plotly_chart(fig_line, use_container_width=True)
        elif not line_df.empty:
             st.info("No spending in selected categories/filters for Line Chart.")
        else:
             st.info("No data matches filters for Line Chart (check dates?).")
    with row2_col2:
        st.markdown("###### Top 10 Expense Types")
        with st.expander("Top Expenses Filters", expanded=False): # Renamed for clarity
            top_start = st.date_input("Start Date", min_date, key="top_start_filter")
            top_end = st.date_input("End Date", max_date, key="top_end_filter")
            top_cats = st.multiselect("Category", all_categories, ["All"], key="top_cat_filter")
            top_accounts = st.multiselect("Account", all_accounts, ["All"], key="top_account_filter")
            top_users = st.multiselect("User", all_users, ["All"], key="top_user_filter")
        if top_start > top_end:
             st.warning("Start date cannot be after end date for Top Expenses.")
             top_df = pd.DataFrame()
        else:
            top_df = df_all[(df_all['date'].dt.date >= top_start) & (df_all['date'].dt.date <= top_end)]
            if "All" not in top_cats: top_df = top_df[top_df['category'].isin(top_cats)]
            if "All" not in top_accounts: top_df = top_df[top_df['account'].isin(top_accounts)]
            if "All" not in top_users: top_df = top_df[top_df['user'].isin(top_users)]
        if not top_df.empty and 'type' in top_df.columns:
            top_df_cleaned = top_df.dropna(subset=['type'])
            top_df_cleaned = top_df_cleaned[top_df_cleaned['type'].str.strip() != '']
            if not top_df_cleaned.empty:
                top_data = top_df_cleaned.groupby('type')['amount'].sum().reset_index().nlargest(10, 'amount').sort_values('amount', ascending=True)
                if not top_data.empty:
                    fig_top = px.bar(top_data, y='type', x='amount', orientation='h', text='amount', color='type', color_discrete_sequence=px.colors.qualitative.Pastel) # Example color sequence
                    layout_top = get_common_layout_args("Top 10 Expense Types by Amount", show_legend=False)
                    layout_top["xaxis_title"] = "Total Amount (INR)"
                    layout_top["yaxis_title"] = ""
                    layout_top["yaxis"] = {'categoryorder':'total ascending'}
                    fig_top.update_layout(**layout_top)
                    fig_top.update_traces(texttemplate="₹%{x:,.0f}", textposition="outside")
                    st.plotly_chart(fig_top, use_container_width=True)
                else:
                     st.info("No spending data found for 'Type' aggregation with current filters.")
            else:
                 st.info("No valid 'Type' entries found after cleaning filters.")
        elif not top_df.empty:
             st.info("No 'type' column found or no data after filtering for Top Expenses.")
        else:
             st.info("No data matches filters for Top Expenses (check dates?).")

# --- 3B. Non-.py files (Summaries + Snippets/Head) ---

#FILE File: langgraph.json
@path: assistant/finance-assistant/langgraph.json
@summary: The 'langgraph.json' file configures a finance assistant's environment by specifying dependencies, a graph path for the agent's functionality, and an environment file. It outlines the structure for integrating and managing the assistant's operational components.
@code:
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env"
}



#FILE File: pyproject.toml
@path: assistant/finance-assistant/pyproject.toml
@summary: The 'pyproject.toml' file configures a Python project named "agent" with version "0.0.1". It specifies dependencies, build requirements, and optional development tools. It includes metadata like author details and license type. The file also sets up linting rules using 'ruff' and defines package directories for setuptools.
@code:
[project]
name = "agent"
version = "0.0.1"
description = "Starter template for making a new agent LangGraph."
authors = [
    { name = "William Fu-Hinthorn", email = "13333726+hinthornw@users.noreply.github.com" },
]
readme = "README.md"
license = { text = "MIT" }
requires-python = ">=3.9"
dependencies = [
    "langgraph>=0.2.6",
    "python-dotenv>=1.0.1",
]


[project.optional-dependencies]
dev = ["mypy>=1.11.1", "ruff>=0.6.1"]

[build-system]
requires = ["setuptools>=73.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = ["langgraph.templates.agent", "agent"]
[tool.setuptools.package-dir]
"langgraph.templates.agent" = "src/agent"
"agent" = "src/agent"


[tool.setuptools.package-data]
"*" = ["py.typed"]

[tool.ruff]
lint.select = [
    "E",    # pycodestyle
    "F",    # pyflakes
    "I",    # isort
    "D",    # pydocstyle
    "D401", # First line should be in imperative mood
    "T201",
    "UP",
]
lint.ignore = [
    "UP006",
    "UP007",
    # We 
[...]


#FILE File: expenses.csv
@path: data/expenses.csv
@summary: Head (first 5 rows) of the expenses CSV data.
@code:
      date  year   month     week day_of_week       account   category     sub_category                       type    user   amount
2023-01-01  2023 2023-01 2023-W52      Sunday Anirban-ICICI       Rent       House Rent Monthly House Rent Payment Anirban 30000.00
2023-01-01  2023 2023-01 2023-W52      Sunday   Puspita-SBI  Household             Maid        Monthly Maid Salary Puspita  2500.00
2023-01-01  2023 2023-01 2023-W52      Sunday Anirban-ICICI    Grocery            Other         Meat/Fish Purchase Anirban   534.04
2023-01-01  2023 2023-01 2023-W52      Sunday   Puspita-SBI    Grocery Flipkart Grocery         FK Quick Groceries Puspita  2921.10
2023-01-01  2023 2023-01 2023-W52      Sunday Anirban-ICICI Restaurant         Takeaway               Lunch Parcel Anirban   848.85


#FILE File: expenses_metadata_detailed.yaml
@path: metadata/expenses_metadata_detailed.yaml
@summary: The file 'expenses_metadata_detailed.yaml' outlines the structure and purpose of an "expenses" table, detailing columns like date, category, and amount for tracking financial transactions of users Anirban and Puspita. It includes relationships, data types, constraints, and usage guidelines for analyzing spending patterns and answering financial queries.
@code:
table_name: expenses
description: "Stores all recorded financial transactions for users Anirban and Puspita. Each row represents a single expense event. Used to track spending patterns, budget adherence, and answer financial queries."
primary_key: id # Assuming 'id' is a UUID added during DB creation

# Explicit relationship mapping derived from expense_metadata.json
relationships:
  - type: derived
    from_column: account
    to_column: user
    mapping:
      "Anirban-SBI": "Anirban"
      "Anirban-ICICI": "Anirban"
      "Puspita-SBI": "Puspita"
      "Puspita-Bandhan": "Puspita"
    description: "The 'user' is strictly determined by the 'account' used in the transaction according to this fixed mapping."

columns:
  - name: id
    data_type: TEXT # Or UUID if supported natively
    description: "Unique identifier for each transaction record. Auto-generated, not typically used in user queries."
    constraints: "Primary Key, Not Null, Unique"
    purpose_for_llm: "Internal database 
[...]


#FILE File: _temp_tree.txt
@path: reference/_temp_tree.txt
@summary: The file 'reference/_temp_tree.txt' outlines the directory structure of a personal finance application. It includes scripts for environment checks, requirements, a finance assistant module, data files, metadata, reference documents, and a Streamlit interface with utilities and tabs for expenses, reports, and visuals.
@code:
app-personal-finance/
    check_env.py
    requirements-v2.0.txt
    assistant/
        finance-assistant/
            .codespellignore
            .env.example
            langgraph.json
            LICENSE
            Makefile
            pyproject.toml
            README.md
            app/
                server.py
            src/
                agent/
                    __init__.py
                    configuration.py
                    graph.py
                    state.py
            static/
                studio_ui.png
            tests/
                integration_tests/
                    __init__.py
                    test_graph.py
                unit_tests/
                    __init__.py
                    test_configuration.py
    data/
        expenses.csv
        expenses.db
    metadata/
        expense_metadata.json
        expenses_metadata_detailed.yaml
    reference/
        _temp_tree.txt
        agentic_ai_guidelines.txt
        data_analysis.ipynb
     
[...]


#FILE File: data_analysis.ipynb
@path: reference/data_analysis.ipynb
@summary: The 'data_analysis.ipynb' file is an exploratory data analysis (EDA) notebook for personal finance data. It analyzes a generated expense dataset ('dummy_expenses_generated.csv') to validate its structure and suitability for AI/ML tasks. The notebook includes data loading, basic checks, date parsing, null value checks, and transaction count validation against predefined rules.
@code:
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Personal Finance Data (v2)\n",
    "\n",
    "**Objective:** Analyze the generated expense data (`dummy_expenses_generated.csv`) to validate its structure, adherence to generation rules, realism, and suitability for downstream AI/ML tasks (Phase 2 Assistant). Plots will be displayed inline.\n",
    "\n",
    "**Data Source:** `../dummy_expenses_generated.csv` (Relative path from `reference/` to project root)\n",
    "**Ruleset Reference:** `../sample_data_generation.csv`\n",
    "**Metadata Reference:** `../expense_metadata.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcb1b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots im
[...]


#FILE File: expenses_sample.csv
@path: reference/expenses_sample.csv
@summary: The file 'expenses_sample.csv' records personal expenses, detailing transactions by date, account, category, sub-category, type, user, and amount. It includes expenses like rent, groceries, and travel, categorized by user (Anirban or Puspita) and account, with timestamps for precise tracking.
@code:
date,year,month,week,day_of_week,account,category,sub_category,type,user,amount
01-01-2023,2023,2023-01,2023-W52,Sunday,Anirban-ICICI,Rent,House Rent,Monthly House Rent Payment,Anirban,30000.0
01-01-2023,2023,2023-01,2023-W52,Sunday,Puspita-SBI,Household,Maid,Monthly Maid Salary,Puspita,2500.0
01-01-2023,2023,2023-01,2023-W52,Sunday,Anirban-ICICI,Grocery,Other,Meat/Fish Purchase,Anirban,534.04
01-01-2023,2023,2023-01,2023-W52,Sunday,Puspita-SBI,Grocery,Flipkart Grocery,FK Quick Groceries,Puspita,2921.1
01-01-2023,2023,2023-01,2023-W52,Sunday,Anirban-ICICI,Restaurant,Takeaway,Lunch Parcel,Anirban,848.85
02-01-2023,2023,2023-01,2023-W01,Monday,Puspita-SBI,Travel,Train,IRCTC Booking - Home,Puspita,1409.08
02-01-2023,2023,2023-01,2023-W01,Monday,Anirban-ICICI,Grocery,Local Store,Local Kirana - Daily Need,Anirban,1749.74
02-01-2023,2023,2023-01,2023-W01,Monday,Anirban-ICICI,Travel,Parking Fee,Parking @ Office,Anirban,67.58
02-01-2023,2023,2023-01,2023-W01,Monday,Anirban-ICICI,Household,Clea
[...]


#FILE File: instructions_advanced_question_types.txt
@path: reference/instructions_advanced_question_types.txt
@summary: The file outlines advanced question types for a Data Science Sub Agent (DSA) to categorize and answer within a personal finance advisor app. It details five machine learning categories: Regression, Forecasting, Classification, Segmentation, and Unsupervised Clustering, providing examples and reasoning for each, focusing on financial prediction and analysis tasks.
@code:
These are sample user questions.These are the questions which the DATA SCIENCE SUB AGENT (DSA) needs to answer. The DSA will categorize questions in one of the 5 advanced question types:Regression, Forecasting, Classification, Segmentation, Unsupervised Clustering. Here we have detailed reasoning for 5 example question across each of the 5 ML types (Regression, Forecasting, Classification, Segmentation, Unsupervised Clustering), specifically tailored for my 2-person personal finance advisor app.

1. Regression Questions (Predicting Specific Numerical Values)

	Goal: Predict a continuous numerical value (usually Amount) for a specific event or instance based on its characteristics.

	Why Regression (vs. Forecasting)? Focuses on the relationship between features of a single instance and its numerical outcome, rather than predicting the next value in a sequence based primarily on past values and time patterns.

	Question: "Predict the cost (Amount) of the next 'Household Repair' (Plumbing
[...]


#FILE File: instructions_metadata.txt
@path: reference/instructions_metadata.txt
@summary: The file provides metadata and sample data for the 'app-personal-finance' project, detailing the structure and purpose of the 'expenses' table. It includes descriptions of columns, relationships, and data types, focusing on financial transactions for users Anirban and Puspita, with categories and sub-categories for detailed expense analysis.
@code:
# Metadata and Data Sample Context for LLM Assistant

This file contains the content of key metadata files and the head rows of key data files
used in the 'app-personal-finance' project.

----------------------------------------------------------------------

# --- Content from: metadata/expenses_metadata_detailed.yaml ---

table_name: expenses
description: "Stores all recorded financial transactions for users Anirban and Puspita. Each row represents a single expense event. Used to track spending patterns, budget adherence, and answer financial queries."
primary_key: id # Assuming 'id' is a UUID added during DB creation

# Explicit relationship mapping derived from expense_metadata.json
relationships:
  - type: derived
    from_column: account
    to_column: user
    mapping:
      "Anirban-SBI": "Anirban"
      "Anirban-ICICI": "Anirban"
      "Puspita-SBI": "Puspita"
      "Puspita-Bandhan": "Puspita"
    description: "The 'user' is strictly determined by the 'account' used in the tr
[...]

