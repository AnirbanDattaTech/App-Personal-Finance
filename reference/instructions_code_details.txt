# Focused Project Context: Core File Tree & Code Details
# Generated by: generate_summary.py
# Purpose: Provides meaningful context (core logic, essential config, tests) for LLM assistants.
# Includes full code for .py files and AI summaries + truncated snippets for essential config/metadata.

# --- A. Project File Tree (Core Logic, Config, Tests) ---

app-personal-finance/
    assistant/
        finance-assistant/
            app/
                server.py
            langgraph.json
            src/
                agent/
                    __init__.py
                    configuration.py
                    graph.py
                    state.py
            tests/
                integration_tests/
                    test_graph.py
                unit_tests/
                    test_configuration.py
    metadata/
        expenses_metadata_detailed.yaml
    streamlit/
        __init__.py
        db_utils.py
        main.py
        style_utils.py
        tabs/
            __init__.py
            add_expense.py
            assistant.py
            reports.py
            visuals.py

# --- B. File Details (Python Code + Essential Config/Metadata) ---

#PY File: server.py
@path: assistant/finance-assistant/app/server.py
@summary: Full Python code included below.
@code:
# FILE: assistant/finance-assistant/app/server.py
# PURPOSE: Defines the FastAPI server to expose the LangGraph agent via LangServe.

from fastapi import FastAPI
from langserve import add_routes
import uvicorn
from pydantic import BaseModel, Field # Use Field for explicit descriptions/validation
from typing import Optional, Any
import logging
from pathlib import Path
import sys

# --- Add project root to path for module resolution ---
# This helps ensure 'src.agent' can be imported correctly, especially when running with `langgraph start`
# or `uvicorn`. Adjust the number of .parent calls if the server.py location changes relative to the root.
# app/server.py -> app -> finance-assistant -> assistant -> app-personal-finance (project root)
project_root = Path(__file__).resolve().parent.parent.parent.parent.parent
src_path = project_root / "assistant" / "finance-assistant" / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))
# ---

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    # 1. Import the compiled graph object and the AgentState
    from agent.graph import graph as finance_assistant_graph # Rename to avoid potential namespace conflicts
    from agent.state import AgentState # Needed for understanding the output structure
    logger.info("Successfully imported graph and AgentState from src.agent")
except ImportError as e:
    logger.error(f"CRITICAL ERROR: Failed to import graph or AgentState from src.agent. "
                 f"Ensure assistant/finance-assistant/src/agent/graph.py and state.py exist "
                 f"and PYTHONPATH is configured correctly. Error: {e}", exc_info=True)
    # Optionally raise the error to prevent server startup with missing core components
    raise ImportError("Could not import agent graph or state. Server cannot start.") from e
except Exception as e:
    logger.error(f"An unexpected error occurred during import: {e}", exc_info=True)
    raise

# --- Define API Input Schema ---
# This model defines the structure expected within the 'input' key of the request payload.
# It should contain the fields needed to initialize the AgentState for a new run.
class AssistantInput(BaseModel):
    """Input schema for the Finance Assistant Agent."""
    original_query: str = Field(..., description="The user's natural language query.")
    # If other fields need to be configurable at invocation time, add them here.
    # Example: session_id: Optional[str] = None

    # Add model config if needed, e.g., for example payloads in OpenAPI docs
    # class Config:
    #     schema_extra = {
    #         "example": {
    #             "original_query": "What was my total grocery spend last month?"
    #         }
    #     }

# --- Define API Output Schema ---
# This model defines the structure that will be nested within the 'output' key of the response payload.
# It selects and structures the relevant fields from the final AgentState.
class AssistantOutput(BaseModel):
    """Output schema for the Finance Assistant Agent."""
    final_response: Optional[str] = Field(None, description="The final text response generated for the user.")
    chart_json: Optional[str] = Field(None, description="Plotly chart JSON representation, if one was generated.")
    sql_results_str: Optional[str] = Field(None, description="SQL query results formatted as a string, if applicable.")
    error: Optional[str] = Field(None, description="Any error message captured during the agent's execution.")

    # class Config:
    #     schema_extra = {
    #         "example": {
    #             "final_response": "Your total grocery spend last month was INR 5,432.10.",
    #             "chart_json": "{...plotly json...}",
    #             "sql_results_str": "Category | Total Amount\nGrocery | 5432.10",
    #             "error": None
    #         }
    #     }


# --- Initialize FastAPI App ---
logger.info("Initializing FastAPI application...")
app = FastAPI(
    title="Personal Finance Assistant Agent API",
    version="0.1.0",
    description="API endpoint for interacting with the LangGraph-based Personal Finance Assistant.",
    # Add other FastAPI configurations like docs_url if needed
)

# --- Basic Health Check Endpoint ---
@app.get("/", tags=["Health"])
async def read_root():
    """Simple health check endpoint."""
    logger.debug("Root endpoint '/' accessed.")
    return {"status": "ok", "message": "Finance Assistant API is running."}

# --- Add LangServe Routes ---
# This function configures the necessary endpoints (/invoke, /stream, /batch, /stream_log, etc.)
# for the given LangGraph agent.
logger.info("Adding LangServe routes for the finance assistant graph...")
add_routes(
    app,
    finance_assistant_graph,    # The imported compiled LangGraph runnable
    path="/assistant",          # The base path for the agent endpoints (e.g., POST /assistant/invoke)
    input_type=AssistantInput,  # Specifies the Pydantic model for input validation (maps to AgentState start)
    output_type=AssistantOutput,# Specifies the Pydantic model for the output structure (maps to AgentState end)
    # enable_feedback_endpoint=True, # Uncomment to enable feedback endpoint (requires LangSmith)
    # config_keys=["configurable"] # Add this if your graph uses configurable fields via AgentState/RunnableConfig
    # tags=["Assistant Agent"]       # Tag for OpenAPI documentation grouping
)
logger.info(f"LangServe routes added successfully at path '/assistant'.")


# --- Run with Uvicorn (for direct execution, e.g., python app/server.py) ---
# Note: `langgraph start` is the recommended way to run this in development
# as it handles hot-reloading and environment setup based on langgraph.json.
if __name__ == "__main__":
    logger.info("Starting Uvicorn server directly (use 'langgraph dev' for development)...")
    # Use port 8000 as the default, matching LangServe's common practice
    # Host '0.0.0.0' makes it accessible on the network
    uvicorn.run(app, host="0.0.0.0", port=8000)

#FILE File: langgraph.json
@path: assistant/finance-assistant/langgraph.json
@summary: This configuration file specifies dependencies, a graph path for an agent in a Python script, and an environment file. It links the agent's graph to a specific Python file and uses a `.env` file for environment variables.
@code:
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env"
}


#PY File: __init__.py
@path: assistant/finance-assistant/src/agent/__init__.py
@summary: Full Python code included below.
@code:
"""New LangGraph Agent.

This module defines a custom graph.
"""

from agent.graph import graph

__all__ = ["graph"]


#PY File: configuration.py
@path: assistant/finance-assistant/src/agent/configuration.py
@summary: Full Python code included below.
@code:
"""Define the configurable parameters for the agent."""

from __future__ import annotations

from dataclasses import dataclass, fields
from typing import Optional

from langchain_core.runnables import RunnableConfig


@dataclass(kw_only=True)
class Configuration:
    """The configuration for the agent."""

    # Changeme: Add configurable values here!
    # these values can be pre-set when you
    # create assistants (https://langchain-ai.github.io/langgraph/cloud/how-tos/configuration_cloud/)
    # and when you invoke the graph
    my_configurable_param: str = "changeme"

    @classmethod
    def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> Configuration:
        """Create a Configuration instance from a RunnableConfig object."""
        configurable = (config.get("configurable") or {}) if config else {}
        _fields = {f.name for f in fields(cls) if f.init}
        return cls(**{k: v for k, v in configurable.items() if k in _fields})


#PY File: graph.py
@path: assistant/finance-assistant/src/agent/graph.py
@summary: Full Python code included below.
@code:
# assistant/finance-assistant/src/agent/graph.py
"""Define the graph for the finance assistant agent."""

import os
import json
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from langchain_google_genai import ChatGoogleGenerativeAI # Use Gemini
from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.pydantic_v1 import BaseModel, Field # Not needed right now
from langgraph.graph import StateGraph, END
from sqlalchemy import create_engine, text           # For database interaction
from pathlib import Path
import logging
from dotenv import load_dotenv
from typing import List, Dict, Any
import yaml # To load the metadata YAML

# Import the AgentState definition from the state.py file in the same directory
from agent.state import AgentState

# --- Basic Configuration ---
# Configure logging for better visibility
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__) # Use a specific logger for this module

# Load .env file from the *project root* (app-personal-finance/)
# Adjust the number of .parent calls based on the script's location
# src/agent/graph.py -> src/agent -> src -> finance-assistant -> assistant -> app-personal-finance
project_root = Path(__file__).resolve().parent.parent.parent.parent.parent
env_path = project_root / '.env'
if env_path.exists():
    load_dotenv(dotenv_path=env_path)
    logger.info(f"Loaded environment variables from: {env_path}")
else:
    logger.warning(f".env file not found at {env_path}. Relying on system environment variables.")

# --- Database Setup ---
# Construct the path relative to the project root
DB_PATH = project_root / "data" / "expenses.db"
if not DB_PATH.exists():
    logger.error(f"CRITICAL: DATABASE NOT FOUND at expected location: {DB_PATH}")
    raise FileNotFoundError(f"Database file not found at {DB_PATH}. Ensure data/expenses.db exists in the project root.")
DB_URI = f"sqlite:///{DB_PATH.resolve()}"
try:
    # connect_args might be needed for specific DB types or async operations later
    engine = create_engine(DB_URI) #, connect_args={"check_same_thread": False})
    logger.info(f"Database engine created for: {DB_URI}")
    # Simple connection test
    with engine.connect() as conn:
         logger.info("Database connection test successful.")
except Exception as e:
    logger.error(f"Failed to create database engine or connect: {e}", exc_info=True)
    raise # Stop execution if DB setup fails

# --- LLM Setup (Gemini) ---
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    logger.error("CRITICAL: GOOGLE_API_KEY not found in environment variables.")
    raise ValueError("GOOGLE_API_KEY environment variable must be set in the .env file.")

try:
    # Initialize Gemini LLM - Using flash for speed/cost
    LLM = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        google_api_key=google_api_key,
        temperature=0.1, # Lower temperature for more deterministic tasks initially
        convert_system_message_to_human=True # Important for Gemini compatibility
    )
    logger.info("ChatGoogleGenerativeAI model initialized (gemini-1.5-flash-latest).")
except Exception as e:
    logger.error(f"Failed to initialize ChatGoogleGenerativeAI: {e}", exc_info=True)
    raise # Stop execution if LLM setup fails

# --- Metadata Loading ---
# Load the detailed metadata YAML file
metadata_path = project_root / "metadata" / "expenses_metadata_detailed.yaml"
SCHEMA_METADATA = "" # Initialize as empty string
try:
    if metadata_path.exists():
        with open(metadata_path, 'r', encoding='utf-8') as f:
            # Load the YAML content - consider formatting it for the prompt
            metadata_content = yaml.safe_load(f)
            # Basic string conversion - could be refined for better LLM digestion
            SCHEMA_METADATA = json.dumps(metadata_content, indent=2)
            logger.info(f"Successfully loaded metadata from {metadata_path}")
    else:
        logger.warning(f"Metadata file not found at {metadata_path}. SQL generation accuracy may be reduced.")
        # Provide a minimal fallback schema description if file is missing
        SCHEMA_METADATA = """
         Fallback Schema:
         Table: expenses
         Columns: id(TEXT PK), date(DATE 'YYYY-MM-DD'), year(INT), month(TEXT 'YYYY-MM'), week(TEXT 'YYYY-Www'), day_of_week(TEXT), account(TEXT), category(TEXT), sub_category(TEXT), type(TEXT), user(TEXT 'Anirban'|'Puspita'), amount(REAL INR).
         """
except Exception as e:
    logger.error(f"Failed to load or parse metadata YAML: {e}", exc_info=True)
    # Proceed with fallback schema or raise error depending on desired robustness
    SCHEMA_METADATA = """
     Fallback Schema: Table: expenses. Columns: id, date, year, month, week, day_of_week, account, category, sub_category, type, user, amount.
     """


# ==========================================================================
#                       NODE FUNCTIONS Will Go Here
# ==========================================================================
def classify_query_node(state: AgentState) -> dict:
    """
    Classifies the user's original query into 'simple', 'advanced', or 'irrelevant'.

    Args:
        state (AgentState): The current state of the graph. Must contain 'original_query'.

    Returns:
        dict: A dictionary containing the 'classification' key with the determined category,
              or an 'error' key if classification fails.
    """
    logger.info("--- Executing Node: classify_query_node ---")
    query = state.get('original_query') # Use .get() for safety

    if not query:
        logger.error("Original query is missing in state for classification.")
        # Return error and default classification
        return {"error": "Missing user query.", "classification": "irrelevant"}

    logger.debug(f"Classifying query: '{query}'")

    # Define the prompt for the classification task
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Your primary task is to classify user questions about personal finance data into one of three categories. Respond ONLY with a single word: 'simple', 'advanced', or 'irrelevant'.

Definitions:
- simple: Directly answerable with a standard SQL query on the 'expenses' table. Examples: totals, averages, filtering by date/category/user, specific lookups like 'What was my total spend last month?', 'Show my grocery expenses in Jan 2024', 'List expenses over 1000 INR'.
- advanced: Requires complex analysis beyond direct SQL (e.g., forecasting, prediction, anomaly detection, clustering, complex multi-step calculations). Examples: 'Predict my spending next week', 'Cluster my spending habits', 'Find unusual spending patterns'. FOR THIS INITIAL IMPLEMENTATION, TREAT 'advanced' THE SAME AS 'simple' downstream (proceed to SQL generation).
- irrelevant: Unrelated to the user's personal finance data stored in the expenses table. Examples: 'What is the weather like?', 'Who won the game?', 'General knowledge questions'.
"""),
        # Explicitly tell the LLM what to do with the user's query
        ("user", f"Classify the following user query: {query}")
    ])

    # Create the chain: Prompt -> LLM
    chain = prompt | LLM

    try:
        # Invoke the LLM
        response = chain.invoke({}) # Provide empty input as context is in the prompt
        # Clean the LLM response: lower case, remove extra characters/whitespace
        classification = response.content.strip().lower().replace("'", "").replace('"', '').replace(".", "")
        logger.info(f"Gemini raw classification response: '{response.content}' -> Cleaned: '{classification}'")

        # Validate the classification output
        valid_classifications = ['simple', 'advanced', 'irrelevant']
        if classification not in valid_classifications:
            logger.warning(f"LLM returned an unexpected classification: '{classification}'. Defaulting to 'simple'.")
            # Fallback to a safe default if the LLM response is malformed
            classification = 'simple'

    except Exception as e:
        logger.error(f"LLM call failed during query classification for query '{query}': {e}", exc_info=True)
        # If the LLM call fails, set an error and default classification
        return {"error": f"Failed to classify query due to LLM error: {e}", "classification": "simple"}

    # Return the classification in the expected dictionary format to update the state
    return {"classification": classification}

def generate_sql_node(state: AgentState) -> dict:
    """
    Generates an SQLite query based on the user's original query and schema metadata.
    Uses prompt variables to safely inject schema and query.

    Args:
        state (AgentState): The current state graph. Must contain 'original_query'.
                           Uses the pre-loaded SCHEMA_METADATA constant.

    Returns:
        dict: A dictionary containing the 'sql_query' key with the generated query string,
              or an 'error' key if SQL generation fails.
    """
    logger.info("--- Executing Node: generate_sql_node ---")
    query = state.get('original_query')

    if state.get('error'):
         logger.warning(f"Skipping SQL generation due to previous error: {state['error']}")
         return {}
    if not query:
        logger.error("Original query is missing in state for SQL generation.")
        return {"error": "Missing user query.", "sql_query": None}

    logger.debug(f"Generating SQL for query: '{query}'")

    if not SCHEMA_METADATA:
        logger.error("Schema metadata (SCHEMA_METADATA) is empty or was not loaded.")
        return {"error": "Database schema metadata is unavailable.", "sql_query": None}

    template_string = """System: You are an expert SQLite query generator. Your task is to generate a precise and syntactically correct SQLite query for the 'expenses' table based ONLY on the user's question and the provided schema metadata.

Schema Metadata:
------
{schema_info}
------

Query Generation Instructions:
1.  Analyze the user's question and the detailed schema metadata.
2.  Identify the relevant columns, filters, aggregations (SUM, AVG, COUNT), and grouping clauses needed.
3.  Use the 'purpose_for_llm' descriptions in the metadata to understand column usage and handle potential ambiguities (e.g., prefer 'Household.Electricity Bill' for bill payments, consider context for 'Amazon').
4.  Construct a single, valid SQLite query.
5.  Interpret timeframes relative to a plausible current date (e.g., assume mid-2024 or later for 'last month', 'this year'). Use specific 'YYYY-MM-DD' date formats in WHERE clauses (e.g., `date BETWEEN '2024-01-01' AND '2024-01-31'`).
6.  Respond ONLY with the raw SQL query. Do NOT include any explanations, comments, or markdown formatting (like ```sql or ```).
7.  Ensure the query terminates correctly (no trailing semicolon needed typically for execution libraries).

User: Generate the SQLite query for the following question: {user_query}
"""
    prompt = ChatPromptTemplate.from_template(template_string)
    # --- Add logging to check prompt variables ---
    logger.debug(f"Prompt expected input variables: {prompt.input_variables}")

    chain = prompt | LLM

    try:
        # --- MODIFICATION START ---
        # Prepare the input dictionary with keys matching the template variables
        input_data = {"schema_info": SCHEMA_METADATA, "user_query": query}

        # --- Add logging to show the exact data being passed ---
        logger.debug(f"Invoking SQL generation chain with input_data keys: {list(input_data.keys())}")
        logger.debug(f"Value for 'user_query' (first 50 chars): {str(input_data.get('user_query', 'MISSING'))[:50]}")
        logger.debug(f"Value for 'schema_info' present: {bool(input_data.get('schema_info'))}")

        # Invoke the chain directly with the dictionary containing the required keys
        response = chain.invoke(input_data)
        # --- MODIFICATION END ---


        sql_query = response.content.strip().replace("```sql", "").replace("```", "").strip()
        if sql_query.endswith(';'):
            sql_query = sql_query[:-1].strip()

        logger.info(f"Gemini raw SQL response: '{response.content}' -> Cleaned: '{sql_query}'")

        if not sql_query or not sql_query.lower().startswith("select"):
             logger.error(f"Generated query is empty or does not start with SELECT: '{sql_query}'")
             return {"error": "Failed to generate a valid SELECT query.", "sql_query": sql_query or "Empty response"}

    except KeyError as e:
        # More specific logging for KeyError during invoke
        logger.error(f"KeyError during chain invocation in SQL generation. This usually means the input dict didn't match prompt variables. Error: {e}", exc_info=True)
        return {"error": f"Internal error matching input to prompt variables: {e}"}
    except Exception as e:
        logger.error(f"LLM call failed during SQL generation for query '{query}': {e}", exc_info=True)
        return {"error": f"Failed to generate SQL query due to LLM error: {e}", "sql_query": None}

    return {"sql_query": sql_query}

def execute_sql_node(state: AgentState) -> dict:
    """
    Executes the generated SQL query against the SQLite database using SQLAlchemy.
    Returns results as a string and a serializable list of dictionaries.

    Args:
        state (AgentState): The current graph state. Must contain 'sql_query' if no prior error.

    Returns:
        dict: A dictionary containing 'sql_results_list' (List[Dict]) and
              'sql_results_str' (string representation), or an 'error' key if execution fails.
    """
    logger.info("--- Executing Node: execute_sql_node ---")
    sql_query = state.get('sql_query')

    # --- MODIFICATION: Ensure results fields are set consistently on error ---
    if state.get('error'):
        logger.warning(f"Skipping SQL execution due to previous error: {state['error']}")
        # Ensure results state fields are appropriately empty/error-indicating
        return {"sql_results_list": [], "sql_results_str": f"Error: {state['error']}"}
    if not sql_query:
        logger.error("No SQL query found in state to execute.")
        return {"error": "SQL query generation failed or was missing.", "sql_results_list": [], "sql_results_str": "Error: No SQL query found."}
    # --- END MODIFICATION ---

    logger.info(f"Attempting to execute SQL query: [{sql_query}]")
    try:
        with engine.connect() as connection:
            df = pd.read_sql(sql=text(sql_query), con=connection)
        logger.info(f"SQL query executed successfully. Number of rows returned: {len(df)}")

        results_list: List[Dict[str, Any]] = [] # Initialize as empty list
        results_str: str = ""                   # Initialize as empty string

        if not df.empty:
            # Format data (optional but good) - Same logic as before
            for col in df.columns:
                if 'date' in col.lower():
                    try: df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d')
                    except Exception: pass # Ignore formatting errors
                elif 'amount' in col.lower() and pd.api.types.is_numeric_dtype(df[col]):
                    try: df[col] = df[col].round(2)
                    except Exception: pass # Ignore formatting errors

            # --- MODIFICATION: Convert to list of dicts and string ---
            results_list = df.to_dict('records') # Convert DataFrame to list of dictionaries
            results_str = df.to_string(index=False, na_rep='<NA>') # Create string version
            # --- END MODIFICATION ---
        else:
            results_list = [] # Explicitly set to empty list
            results_str = "Query returned no results."

        # --- MODIFICATION: Return the list and string ---
        return {"sql_results_list": results_list, "sql_results_str": results_str}
        # --- END MODIFICATION ---

    except Exception as e:
        logger.error(f"SQL execution failed for query [{sql_query}]: {e}", exc_info=True)
        error_msg = f"Failed to execute SQL query. Error: {e}. Query Attempted: [{sql_query}]"
        # --- MODIFICATION: Ensure state consistency on error ---
        return {"error": error_msg, "sql_results_list": [], "sql_results_str": f"Error executing SQL."}
        # --- END MODIFICATION ---

def generate_chart_node(state: AgentState) -> dict:
    """
    Generates a Plotly chart JSON based on the SQL query results (List of Dicts).
    Uses simple heuristics to determine the most appropriate chart type.

    Args:
        state (AgentState): The current graph state. Needs 'sql_results_list'.

    Returns:
        dict: A dictionary containing 'chart_json' (Plotly JSON string) if successful,
              otherwise None.
    """
    logger.info("--- Executing Node: generate_chart_node ---")
    # --- MODIFICATION: Get list instead of df ---
    results_list = state.get('sql_results_list')
    # --- END MODIFICATION ---
    error = state.get('error')
    query = state.get('original_query', 'Unknown query')

    # Pre-checks
    if error:
        logger.warning(f"Skipping chart generation due to previous error: {error}")
        return {"chart_json": None}
    # --- MODIFICATION: Check the list ---
    if results_list is None: # Check if None (might happen if execute_sql had severe issue before returning)
        logger.warning("Skipping chart generation as results list is None.")
        return {"chart_json": None}
    if not results_list: # Check if empty list
        logger.info("Skipping chart generation as results list is empty.")
        return {"chart_json": None}
    # --- END MODIFICATION ---

    try:
        # --- MODIFICATION: Convert list back to DataFrame for plotting ---
        df = pd.DataFrame(results_list)
        if df.empty: # Double check after conversion
            logger.warning("DataFrame created from results list is empty. Skipping chart generation.")
            return {"chart_json": None}
        logger.info(f"Reconstructed DataFrame for charting, shape: {df.shape}")
        logger.debug(f"DataFrame columns: {df.columns.tolist()}")
        # --- END MODIFICATION ---


        # --- Chart Generation Logic (using df) ---
        # THE REST OF THE CHARTING LOGIC USING 'df' REMAINS EXACTLY THE SAME AS BEFORE
        # --- from here down ---
        fig = None
        num_rows, num_cols = df.shape
        col_names_lower = df.columns.str.lower()

        numeric_cols = [
            col for col in df.columns
            if pd.api.types.is_numeric_dtype(df[col])
            and col.lower() not in ['id', 'year']
        ]
        logger.debug(f"Identified numeric columns for plotting: {numeric_cols}")

        date_col_present = 'date' in col_names_lower or 'month' in col_names_lower or 'yearmonth' in col_names_lower
        if date_col_present and len(numeric_cols) == 1:
            if 'date' in col_names_lower: time_col = df.columns[col_names_lower.tolist().index('date')]
            elif 'month' in col_names_lower: time_col = df.columns[col_names_lower.tolist().index('month')]
            else: time_col = df.columns[col_names_lower.tolist().index('yearmonth')]
            value_col = numeric_cols[0]
            logger.info(f"Attempting Line Chart: X='{time_col}', Y='{value_col}'")
            try:
                # Attempt conversion to datetime for robust sorting, handle failure gracefully
                try:
                    df[time_col] = pd.to_datetime(df[time_col])
                except Exception as time_conv_err:
                    logger.warning(f"Could not convert time column '{time_col}' to datetime: {time_conv_err}. Sorting may be string-based.")

                df_sorted = df.sort_values(by=time_col)
                fig = px.line(df_sorted, x=time_col, y=value_col, title=f"{value_col.capitalize()} Trend", markers=True)
                fig.update_layout(xaxis_title=time_col.capitalize(), yaxis_title=value_col.capitalize())
            except TypeError as sort_err:
                 logger.warning(f"TypeError during sorting/plotting line chart (column '{time_col}' might not be sortable): {sort_err}. Skipping line chart.")
            except Exception as line_err:
                 logger.warning(f"Could not generate line chart: {line_err}. Skipping line chart.")

        elif num_cols == 2 and len(numeric_cols) == 1:
            value_col = numeric_cols[0]
            cat_col = next((col for col in df.columns if col != value_col), None)
            if cat_col:
                logger.info(f"Attempting Bar Chart: Category='{cat_col}', Value='{value_col}'")
                try:
                    max_bars = 15
                    df_agg = df.sort_values(by=value_col, ascending=False)
                    df_chart = df_agg.head(max_bars)
                    title_suffix = f" (Top {max_bars})" if len(df_agg) > max_bars else ""
                    title = f"{value_col.capitalize()} by {cat_col.capitalize()}{title_suffix}"
                    fig = px.bar(df_chart, x=cat_col, y=value_col, title=title, text_auto='.2s')
                    fig.update_traces(textangle=0, textposition="outside")
                    fig.update_layout(xaxis_title=cat_col.capitalize(), yaxis_title=value_col.capitalize())
                except Exception as bar_err:
                    logger.warning(f"Could not generate bar chart: {bar_err}. Skipping bar chart.")
            else:
                logger.warning("Bar chart condition met but failed to identify categorical column.")

        elif num_rows == 1 and len(numeric_cols) == 1:
            logger.info("Single numeric value result. Skipping graphical chart generation.")
            return {"chart_json": None}

        if fig is None:
            logger.info("No specific chart type matched heuristics. Skipping chart generation.")
            return {"chart_json": None}

        fig.update_layout(
            margin=dict(l=40, r=20, t=60, b=40),
            title_x=0.5,
            legend_title_text=None
        )
        chart_json = fig.to_json()
        logger.info("Plotly chart JSON generated successfully.")
        return {"chart_json": chart_json}

    except Exception as e:
        logger.error(f"Unexpected error during chart generation: {e}", exc_info=True)
        return {"chart_json": None}


# Import PromptTemplate if not already imported at the top
from langchain_core.prompts import PromptTemplate # Or keep ChatPromptTemplate

def generate_response_node(state: AgentState) -> dict:
    """
    Generates the final natural language response. Uses PromptTemplate.
    Instructs the LLM to use INR as the currency.

    Args:
        state (AgentState): The current graph state. Needs 'original_query'.
                           Uses 'sql_results_str', 'error', 'classification',
                           and checks for 'chart_json'.

    Returns:
        dict: A dictionary containing the 'final_response' string.
    """
    logger.info("--- Executing Node: generate_response_node ---")
    query = state.get('original_query', "An unspecified query")
    # Use the potentially empty list from state if needed, but string is primary for summary
    sql_results_str = state.get('sql_results_str', '') # Use the string representation
    error = state.get('error')
    classification = state.get('classification')
    chart_available = state.get('chart_json') is not None

    final_response = ""

    # 1. Prioritize responding to errors
    if error:
        logger.warning(f"Generating response based on detected error: {error}")
        error_str = str(error)
        # Simple error mapping
        if "Failed to execute SQL" in error_str or "syntax error" in error_str.lower():
            final_response = f"I encountered an issue trying to retrieve the data ({error_str}). Perhaps try asking differently?"
        elif "Failed to generate SQL" in error_str:
             final_response = f"I had trouble understanding how to fetch the data ({error_str}). Could you please rephrase?"
        elif "Failed to classify query" in error_str:
             final_response = f"I had trouble understanding your request type ({error_str}). Could you clarify?"
        elif "Missing user query" in error_str or "metadata is unavailable" in error_str:
             final_response = f"There was an internal setup issue preventing me from processing your request: {error_str}"
        else: # Generic error
            final_response = f"I'm sorry, I encountered an issue: {error_str}. Please try again."

    # 2. Handle irrelevant classification
    elif classification == 'irrelevant':
         final_response = "This question doesn't seem related to your financial expenses. Could you ask something about your spending?"
         logger.info("Generated response for 'irrelevant' classification.")

    # 3. Handle no results (Check both empty string and specific message)
    elif not error and (not sql_results_str or sql_results_str == "Query returned no results."):
         final_response = "I looked through your expense records based on your query, but couldn't find any matching transactions."
         logger.info("Generated response for query with no results.")

    # 4. Generate a summary response based on successful SQL results
    elif not error and sql_results_str:
        logger.info("Generating summary response using PromptTemplate.")
        chart_mention = "I've also prepared a chart to visualize this." if chart_available else ""

        # --- MODIFICATION START: Added currency instruction ---
        template_string = """System: You are a friendly financial assistant summarizing financial data for users based in India.

Instructions:
*   Answer the user's question ({user_query}) using the key information from the data below.
*   Summarize findings/trends if data is tabular. State single numbers clearly. Do NOT just repeat the raw data.
*   {chart_mention_instruction} Mention the chart briefly if applicable.
*   Keep the tone conversational (use "You"/"Your"). Avoid jargon.
*   IMPORTANT: Always use 'INR' (Indian Rupees) when referring to monetary values. Do NOT use '$' or other currency symbols.

Retrieved Data:
{sql_data}

Assistant: """
        # --- MODIFICATION END ---

        prompt = PromptTemplate(
            template=template_string,
            input_variables=["user_query", "sql_data", "chart_mention_instruction"]
        )

        chain = prompt | LLM

        try:
            if LLM is None: raise ValueError("LLM client is not initialized.")

            input_dict = {
                "user_query": query,
                "sql_data": sql_results_str, # Pass the string representation
                "chart_mention_instruction": chart_mention
            }

            response = chain.invoke(input_dict)
            final_response = response.content.strip()
            logger.info("Successfully generated summary response from LLM.")

        except Exception as e:
            logger.error(f"LLM call failed during final response generation: {e}", exc_info=True)
            final_response = f"I retrieved the data:\n{sql_results_str}\nBut I had trouble summarizing it. Please note the currency is INR." # Add fallback currency note

    # 5. Fallback
    else:
        logger.error("Reached end of generate_response_node without generating a response. State: %s", {k:v for k,v in state.items() if k != 'sql_results_list'}) # Log state excluding list
        final_response = "I'm sorry, I wasn't able to determine a response for your query."

    if not final_response:
        logger.warning("Final response was empty after processing, providing default.")
        final_response = "I'm sorry, I couldn't process that request properly."

    return {"final_response": final_response}

# ==========================================================================
#                       EDGE LOGIC FUNCTION
# ==========================================================================

def should_continue(state: AgentState) -> str:
    """
    Determines the next node to execute based on the current state,
    specifically the query classification or presence of errors.

    Args:
        state (AgentState): The current graph state.

    Returns:
        str: The name of the next node to call ('generate_sql', 'generate_response'),
             or potentially END (though current logic routes errors to response).
    """
    logger.info("--- Evaluating Edge: should_continue ---")
    classification = state.get('classification')
    error = state.get('error') # Check if ANY previous node set an error

    # Priority 1: Handle errors immediately by routing to final response generation
    # The generate_response_node is responsible for formatting the error message.
    if error:
        logger.warning(f"Error detected in state ('{error}'), routing directly to 'generate_response'.")
        return "generate_response"

    # Priority 2: Handle irrelevant classification
    elif classification == 'irrelevant':
        logger.info("Classification is 'irrelevant', routing to 'generate_response'.")
        return "generate_response"

    # Priority 3: Proceed with data retrieval for simple/advanced queries
    elif classification in ['simple', 'advanced']:
        logger.info(f"Classification is '{classification}', routing to 'generate_sql'.")
        return "generate_sql"

    # Fallback: This case indicates an issue, likely with the classification node.
    # Route to response node to report the internal issue.
    else:
        logger.error(f"Unknown or missing classification ('{classification}') in state. Routing to generate_response.")
        # We don't set the error here, let generate_response handle the lack of valid path.
        # Alternatively, could set state['error'] = "Internal classification error" here.
        return "generate_response"

# ==========================================================================
#                       GRAPH DEFINITION
# ==========================================================================

logger.info("Defining LangGraph workflow structure...")

# Initialize the state graph with our AgentState definition
workflow = StateGraph(AgentState)
logger.debug("StateGraph initialized.")

# Add the nodes to the graph, associating a name with each function
workflow.add_node("classify_query", classify_query_node)
workflow.add_node("generate_sql", generate_sql_node)
workflow.add_node("execute_sql", execute_sql_node)
workflow.add_node("generate_chart", generate_chart_node)
workflow.add_node("generate_response", generate_response_node)
logger.info("Nodes added to the graph: classify_query, generate_sql, execute_sql, generate_chart, generate_response")

# Define the entry point of the graph
workflow.set_entry_point("classify_query")
logger.info("Graph entry point set to 'classify_query'.")

# Define the conditional edge after the classification node.
# The 'should_continue' function will determine which node to go to next.
workflow.add_conditional_edges(
    source="classify_query",  # keyword: source
    path=should_continue,     # keyword: path
    path_map={                # keyword: path_map
        "generate_sql": "generate_sql",
        "generate_response": "generate_response"
    }
)
logger.info("Conditional edge added from 'classify_query' based on 'should_continue' logic.")

# Define the linear flow for the main data processing path
# Connect nodes sequentially after the conditional split directs to 'generate_sql'
workflow.add_edge("generate_sql", "execute_sql")
workflow.add_edge("execute_sql", "generate_chart")
workflow.add_edge("generate_chart", "generate_response")
logger.info("Linear edges defined: generate_sql -> execute_sql -> generate_chart -> generate_response.")

# Define the final step: after generating the response, the graph ends.
workflow.add_edge("generate_response", END) # END is a special marker from langgraph.graph
logger.info("Final edge added from 'generate_response' to END.")

# ==========================================================================
#                       Compile & Assign Entry Point Variable ('graph')
# ==========================================================================

# Compile the workflow into a runnable application object
# The compiled object MUST be assigned to the variable name 'graph'
# as defined in langgraph.json
graph = workflow.compile()
logger.info("LangGraph workflow compiled successfully. Runnable 'graph' object created.")

# Optional: Add a simple test execution block (useful during development)
# This will only run if you execute graph.py directly (python src/agent/graph.py)
if __name__ == "__main__":
    logger.info("--- Running Direct Script Test ---")
    # Example test invocation (replace with relevant queries)
    test_queries = [
        "What was my total spend last month?",
        "Show my grocery expenses",
        "what is the weather?",
        "Compare spending between Anirban and Puspita for Rent",
        "Generate a query with syntax error deliberately" # Example for testing error path
    ]
    test_input = {"original_query": test_queries[0]} # Change index to test different queries

    logger.info(f"Test Input: {test_input}")
    try:
        # Use stream to see the steps
        for output_chunk in graph.stream(test_input, {"recursion_limit": 10}):
            # output is a dictionary where keys are node names, values are output dicts
            node_name = list(output_chunk.keys())[0]
            node_data = output_chunk[node_name]
            logger.info(f"--- Step Output: {node_name} ---")
            # Log relevant parts of the node output (avoid printing large dataframes directly)
            log_output = {k: (v[:100] + '...' if isinstance(v, str) and len(v) > 100 else v)
                          for k, v in node_data.items() if k != 'sql_results_df'} # Exclude df for brevity
            logger.info(f"{log_output}")

        # Optionally invoke again to get final state easily (can be large)
        # final_state = graph.invoke(test_input, {"recursion_limit": 10})
        # logger.info(f"--- Final State ---")
        # logger.info(json.dumps(final_state, indent=2, default=str)) # Use default=str for non-serializable like DataFrame

    except Exception as e:
        logger.error(f"Test execution failed: {e}", exc_info=True)

    logger.info("--- Direct Script Test Complete ---")

#PY File: state.py
@path: assistant/finance-assistant/src/agent/state.py
@summary: Full Python code included below.
@code:
# assistant/finance-assistant/src/agent/state.py
"""Define the state structures for the agent."""

from __future__ import annotations # Ensures compatibility with type hints

# Use typing.TypedDict for standard LangGraph state
from typing import TypedDict, Optional, List, Dict, Any
import pandas as pd

# Define the structure of the state that will be passed between nodes
class AgentState(TypedDict):
    """Represents the state shared across the agent graph."""
    original_query: str           # The initial question from the user
    classification: Optional[str]   # 'simple', 'advanced', 'irrelevant'
    sql_query: Optional[str]        # Generated SQL query
    sql_results_str: Optional[str]  # SQL results as a formatted string
    # sql_results_df: Any             # SQL results as a Pandas DataFrame (use Any for now, handle serialization if needed)
    sql_results_list: Optional[List[Dict[str, Any]]] # SQL results as a list of dictionaries, store results as serializable list of dicts
    chart_json: Optional[str]       # Plotly figure JSON representation
    final_response: Optional[str]   # Final text response for the user
    error: Optional[str]            # To capture errors during execution

#PY File: __init__.py
@path: assistant/finance-assistant/tests/integration_tests/__init__.py
@summary: Full Python code included below.
@code:
"""Define any integration tests you want in this directory."""


#PY File: test_graph.py
@path: assistant/finance-assistant/tests/integration_tests/test_graph.py
@summary: Full Python code included below.
@code:
import pytest
from langsmith import unit

from agent import graph


@pytest.mark.asyncio
@unit
async def test_agent_simple_passthrough() -> None:
    res = await graph.ainvoke({"changeme": "some_val"})
    assert res is not None


#PY File: __init__.py
@path: assistant/finance-assistant/tests/unit_tests/__init__.py
@summary: Full Python code included below.
@code:
"""Define any unit tests you may want in this directory."""


#PY File: test_configuration.py
@path: assistant/finance-assistant/tests/unit_tests/test_configuration.py
@summary: Full Python code included below.
@code:
from agent.configuration import Configuration


def test_configuration_empty() -> None:
    Configuration.from_runnable_config({})


#FILE File: expenses_metadata_detailed.yaml
@path: metadata/expenses_metadata_detailed.yaml
@summary: The file defines the structure for an "expenses" table, tracking financial transactions for users Anirban and Puspita. Key elements include primary key "id," date-related columns, user-account mapping, and categorized expense details. It supports detailed financial analysis, filtering, and pattern tracking based on user, date, category, and amount.
@code:
table_name: expenses
description: "Stores all recorded financial transactions for users Anirban and Puspita. Each row represents a single expense event. Used to track spending patterns, budget adherence, and answer financial queries."
primary_key: id # Assuming 'id' is a UUID added during DB creation

# Explicit relationship mapping derived from expense_metadata.json
relationships:
  - type: derived
    from_column: account
    to_column: user
    mapping:
      "Anirban-SBI": "Anirban"
      "Anirban-ICICI": "Anirban"
      "Puspita-SBI": "Puspita"
      "Puspita-Bandhan": "Puspita"
    description: "The 'user' is strictly determined by the 'account' used in the transaction according to this fixed mapping."

columns:
  - name: id
    data_type: TEXT # Or UUID if supported natively
    description: "Unique identifier for each transaction record. Auto-generated, not typically used in user queries."
    constraints: "Primary Key, Not Null, Unique"
    purpose_for_llm: "Internal database identifier. Avoid using in filters unless a specific transaction ID is provided."

  - name: date
    data_type: DATE # Store as ISO 8601 string 'YYYY-MM-DD' or DATE type
    description: "The exact date the transaction occurred (format: YYYY-MM-DD)."
    example_values: ["2023-10-26", "2024-01-15"]
    constraints: "Not Null"
    purpose_for_llm: "Primary column for filtering by time. Use for specific dates, date ranges (e.g., 'last month', 'this year', 'between Jan 1 2024 and Mar 31 2024'), or time-based aggregations. Always use this column for date conditions."

  - name: year
    data_type: INTEGER
    description: "The year the transaction occurred (extracted from the 'date' column)."
    example_values: [2023, 2024]
    constraints: "Derived from 'date'. Not Null."
    purpose_for_llm: "Use for filtering or grouping expenses by calendar year (e.g., 'total spend in 2023', 'compare 2023 vs 2024'). Less granular than 'month' or 'date'."

  - name: month
    data_type: TEXT # Format 
[...]

#PY File: __init__.py
@path: streamlit/__init__.py
@summary: Full Python code included below.
@code:


#PY File: db_utils.py
@path: streamlit/db_utils.py
@summary: Full Python code included below.
@code:
# streamlit/db_utils.py
import sqlite3
import pandas as pd
from uuid import uuid4
from pathlib import Path # Use pathlib
import logging
from typing import Optional, Dict, Any, List

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ---  Define DB Path relative to the project root ---
# This assumes db_utils.py is in 'app-personal-finance/streamlit/'
# Path(__file__) gives the path to db_utils.py
# .parent gives 'app-personal-finance/streamlit/'
# .parent gives 'app-personal-finance/'
# Then we navigate to 'data/expenses.db'
PROJECT_ROOT = Path(__file__).parent.parent
DB_PATH = PROJECT_ROOT / "data" / "expenses.db"

# --- Optional: Check if DB exists and log ---
if not DB_PATH.exists():
    logging.error(f"DATABASE NOT FOUND at expected location: {DB_PATH.resolve()}")
    # Indicate the expected path based on calculation
    logging.error(f"(Calculated from: {__file__})")
    # You might want to raise an error or handle this case differently in a real app
else:
    # Print statement removed as logging is now configured
    logging.info(f"Using database at: {DB_PATH.resolve()}")
# ---

def get_connection() -> Optional[sqlite3.Connection]:
    """
    Establishes a connection to the SQLite database.

    Returns:
        Optional[sqlite3.Connection]: A connection object or None if connection fails.
    """
    try:
        # Ensure the path is passed as a string or Path object
        conn = sqlite3.connect(DB_PATH, check_same_thread=False)
        conn.row_factory = sqlite3.Row  # Return rows as dictionary-like objects
        logging.debug(f"Database connection established to {DB_PATH.resolve()}")
        return conn
    except sqlite3.Error as e:
        # Use logging correctly
        logging.error(f"Database connection error to {DB_PATH.resolve()}: {e}", exc_info=True)
        return None

def fetch_all_expenses() -> pd.DataFrame:
    """Fetches all expenses, ordered by date descending."""
    conn = get_connection()
    if conn is None:
        logging.error("Cannot fetch expenses: Database connection failed.")
        return pd.DataFrame()
    try:
        # Explicitly select columns for clarity and potential future changes
        query = "SELECT id, date, year, month, week, day_of_week, account, category, sub_category, type, user, amount FROM expenses ORDER BY date DESC"
        df = pd.read_sql(query, conn)
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        logging.info(f"Fetched {len(df)} expenses.")
        return df
    except (sqlite3.Error, pd.errors.DatabaseError) as e:
        logging.error(f"Error fetching all expenses: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

def fetch_expense_by_id(expense_id: str) -> Optional[Dict[str, Any]]:
    """Fetches a single expense by its ID."""
    conn = get_connection()
    if conn is None:
        logging.error(f"Cannot fetch expense {expense_id}: Database connection failed.")
        return None
    try:
        cursor = conn.cursor()
        # Use parameter binding for safety
        cursor.execute("SELECT * FROM expenses WHERE id = ?", (expense_id,))
        record = cursor.fetchone()
        logging.debug(f"Fetched expense for ID {expense_id}: {'Found' if record else 'Not Found'}")
        # Convert sqlite3.Row to dict if found
        return dict(record) if record else None
    except sqlite3.Error as e:
        logging.error(f"Error fetching expense by ID {expense_id}: {e}", exc_info=True)
        return None
    finally:
        if conn: conn.close()

def insert_expense(data: Dict[str, Any]) -> bool:
    """
    Inserts a new expense record into the database.

    Args:
        data (Dict[str, Any]): Dictionary containing expense details.
                               Must include all required fields.

    Returns:
        bool: True if insertion was successful, False otherwise.
    """
    conn = get_connection()
    if conn is None:
        logging.error("Cannot insert expense: Database connection failed.")
        return False

    # Define expected columns explicitly based on the SQL statement below
    required_fields = ['date', 'year', 'month', 'week', 'day_of_week',
                       'account', 'category', 'sub_category', 'type',
                       'user', 'amount']

    # Check for missing fields *before* trying to insert
    missing_fields = [field for field in required_fields if field not in data]
    if missing_fields:
        logging.error(f"Missing required fields for inserting expense: {', '.join(missing_fields)}. Data provided: {list(data.keys())}")
        return False

    sql = """
    INSERT INTO expenses (
        id, date, year, month, week, day_of_week,
        account, category, sub_category, type, user, amount
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """

    try:
        cursor = conn.cursor()
        new_id = str(uuid4()) # Generate UUID here
        # Prepare values in the correct order, ensuring types
        values = (
            new_id,
            data['date'], # Assume already string in 'YYYY-MM-DD' format
            int(data['year']),
            data['month'],
            data['week'],
            data['day_of_week'],
            data['account'],
            data['category'],
            # Handle potentially missing/empty sub_category gracefully
            data.get('sub_category', ''), # Use .get for optional fields if applicable
            data['type'],
            data['user'],
            float(data['amount'])
        )
        cursor.execute(sql, values)
        conn.commit()
        logging.info(f" Expense inserted with ID: {new_id}")
        return True
    except (sqlite3.Error, ValueError, TypeError) as e: # Catch potential type errors too
        logging.error(f"Error inserting expense: {e}", exc_info=True)
        conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


def update_expense(expense_id: str, data: Dict[str, Any]) -> bool:
    """Updates an existing expense based on its ID."""
    conn = get_connection()
    if conn is None:
        logging.error(f"Cannot update expense {expense_id}: Database connection failed.")
        return False

    # Define fields allowed for update (excluding id, maybe others)
    updatable_fields = ['date', 'year', 'month', 'week', 'day_of_week', 'account', 'category', 'sub_category', 'type', 'user', 'amount']
    # Construct SET clause dynamically from provided data, only using allowed fields
    set_parts = []
    values = []
    for field in updatable_fields:
        if field in data:
            set_parts.append(f"{field} = ?")
            # Basic type conversion/validation (more robust needed for production)
            if field in ['year']:
                values.append(int(data[field]))
            elif field in ['amount']:
                values.append(float(data[field]))
            else:
                values.append(data[field])

    if not set_parts:
        logging.warning(f"No valid fields provided for updating expense ID {expense_id}.")
        return False

    set_clause = ", ".join(set_parts)
    sql = f"UPDATE expenses SET {set_clause} WHERE id = ?"
    values.append(expense_id) # Add the ID for the WHERE clause

    try:
        cursor = conn.cursor()
        cursor.execute(sql, tuple(values))
        conn.commit()
        if cursor.rowcount == 0:
            logging.warning(f"No expense found with ID {expense_id} to update.")
            return False
        logging.info(f"Expense {expense_id} updated successfully.")
        return True
    except (sqlite3.Error, ValueError, TypeError) as e:
        logging.error(f"Error updating expense {expense_id}: {e}", exc_info=True)
        conn.rollback()
        return False
    finally:
        if conn: conn.close()

def delete_expense(expense_id: str) -> bool:
    """Deletes an expense record by its ID."""
    conn = get_connection()
    if conn is None:
        logging.error(f"Cannot delete expense {expense_id}: Database connection failed.")
        return False

    sql = "DELETE FROM expenses WHERE id = ?"
    try:
        cursor = conn.cursor()
        cursor.execute(sql, (expense_id,))
        conn.commit()
        if cursor.rowcount == 0:
            logging.warning(f"No expense found with ID {expense_id} to delete.")
            return False
        logging.info(f"Expense {expense_id} deleted successfully.")
        return True
    except sqlite3.Error as e:
        logging.error(f"Error deleting expense {expense_id}: {e}", exc_info=True)
        conn.rollback()
        return False
    finally:
        if conn: conn.close()

def fetch_last_expenses(n: int = 10) -> pd.DataFrame:
    """Fetches the last N expenses, ordered by date then rowid descending."""
    conn = get_connection()
    if conn is None:
        logging.error(f"Cannot fetch last {n} expenses: Database connection failed.")
        return pd.DataFrame()
    try:
        # Order by date descending first, then rowid descending as a tie-breaker
        # Explicitly list columns
        query = f"""
            SELECT id, date, year, month, week, day_of_week, account, category, sub_category, type, user, amount
            FROM expenses
            ORDER BY date DESC, rowid DESC
            LIMIT ?
        """
        df = pd.read_sql(query, conn, params=(n,))
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        logging.info(f"Fetched last {len(df)} expenses (requested {n}).")
        return df
    except (sqlite3.Error, pd.errors.DatabaseError) as e:
        logging.error(f"Error fetching last {n} expenses: {e}", exc_info=True)
        return pd.DataFrame()
    finally:
        if conn: conn.close()

#PY File: main.py
@path: streamlit/main.py
@summary: Full Python code included below.
@code:
# streamlit/main.py
"""
Main Streamlit application file for the Personal Expense Tracker.
Handles page navigation and calls rendering functions for each tab.
"""
import streamlit as st
import pandas as pd
import logging
from pathlib import Path # Good practice for path handling

# ---  Relative Imports for modules within the 'streamlit' package ---
# Assumes main.py is in the 'streamlit' directory
# and the tabs are in a subdirectory 'tabs'
# and utils are directly in 'streamlit'
try:
    from tabs import add_expense, reports, visuals, assistant 
    from style_utils import load_css
    from db_utils import fetch_all_expenses  # For CSV download
    st.session_state['imports_successful'] = True
    logging.info("Successfully imported UI tabs and utils.")
except ImportError as e:
    # This error handling is crucial for debugging if imports fail
    st.error(f"Failed to import necessary application components: {e}. "
             f"Please check the file structure and ensure main.py is run from the correct directory "
             f"or that the 'streamlit' package is correctly installed/recognized.")
    logging.exception("ImportError during initial module loading.")
    st.session_state['imports_successful'] = False
    st.stop() # Stop execution if core modules fail

# --- Configure basic logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# --- Page Configuration ---
st.set_page_config(
    layout="wide",
    page_title="Personal Expense Tracker",
    page_icon=""
)

# --- Load CSS ---
if st.session_state.get('imports_successful', False):
    load_css()
else:
    st.warning("Could not load CSS due to import errors.")

# --- Optional: Banner ---
# Consider adding specific styling in styles.css if uncommented
# st.markdown(
#     '<div class="app-banner">My Personal Finance App</div>',
#     unsafe_allow_html=True
# )

# --- Sidebar Navigation ---
st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "Go to",
    # List of available tabs
    ["Add Expenses", "Reports", "Visualizations", "Assistant"], # Assistant is now a valid option
    label_visibility="collapsed",
    key="main_nav"
)

st.sidebar.markdown("---")

# --- Sidebar Data Management ---
st.sidebar.header("Data Management")
if st.session_state.get('imports_successful', False): # Check if db_utils import worked
    try:
        # Fetch data using the imported function
        df_all = fetch_all_expenses()
        if not df_all.empty:
            # Optional: drop UUID if not needed for export
            df_export = df_all.drop(columns=["id"], errors="ignore")
            csv_bytes = df_export.to_csv(index=False).encode("utf-8")

            st.sidebar.download_button(
                label="Download Data Backup (.csv)",
                data=csv_bytes,
                file_name="expenses_backup.csv",
                mime="text/csv",
                help="Download the full dataset as a CSV file"
            )
        else:
            st.sidebar.info("No expense data available to download.")
    except Exception as e:
        st.sidebar.error("Error loading data for CSV backup.")
        logging.exception("Sidebar CSV export error: %s", e)
else:
    st.sidebar.warning("Data management unavailable due to import errors.")


# --- Page Rendering ---
# Only attempt to render if imports were successful
if st.session_state.get('imports_successful', False):
    if page == "Add Expenses":
        # Call the imported module's render function
        add_expense.render()
    elif page == "Reports":
        # Call the imported module's render function
        reports.render()
    elif page == "Visualizations":
        # Call the imported module's render function
        visuals.render()
    elif page == "Assistant":
         # ---  Call the renamed Assistant tab's render function ---
         assistant.render() # **** CALL THE RENAMED MODULE'S FUNCTION ****
    else:
        st.error("Invalid page selected.")
else:
    # Error message already displayed during import failure
    pass

# Optional: Add a footer or other common elements here if needed
# st.markdown("---")
# st.caption("App v1.1")

#PY File: style_utils.py
@path: streamlit/style_utils.py
@summary: Full Python code included below.
@code:
# streamlit/style_utils.py
import streamlit as st
import logging
from pathlib import Path # Use pathlib for robust path handling

# --- Assume styles.css is in the same directory as this script ---
CSS_FILE = Path(__file__).parent / "styles.css"

def load_css():
    """Loads CSS from the styles.css file located in the same directory."""
    if CSS_FILE.is_file():
        try:
            with open(CSS_FILE, "r") as f:
                css = f.read()
            st.markdown(f"<style>{css}</style>", unsafe_allow_html=True)
            # logging.info(f"Successfully loaded CSS from {CSS_FILE}") # Optional: for debugging
        except Exception as e:
            logging.error(f"Error reading CSS file {CSS_FILE}: {e}")
            st.error("Failed to load page styles.")
    else:
        logging.warning(f"CSS file not found at expected location: {CSS_FILE}")
        # st.warning("Page styling may be incomplete (CSS not found).")

#PY File: __init__.py
@path: streamlit/tabs/__init__.py
@summary: Full Python code included below.
@code:


#PY File: add_expense.py
@path: streamlit/tabs/add_expense.py
@summary: Full Python code included below.
@code:
# streamlit/tabs/add_expense.py
import streamlit as st
import pandas as pd
from db_utils import insert_expense, fetch_last_expenses # Use direct import based on previous findings
import json
import datetime
from typing import Dict, Any, Optional
import logging
import time
from pathlib import Path

# Define Metadata Path relative to the project root
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
METADATA_FILE_PATH = PROJECT_ROOT / "metadata" / "expense_metadata.json"

@st.cache_data
def load_metadata() -> Optional[Dict[str, Any]]:
    """Loads metadata from the project's metadata directory."""
    if not METADATA_FILE_PATH.is_file():
        logging.error(f"Metadata file not found at: {METADATA_FILE_PATH}")
        st.error(f"Critical application error: Metadata configuration file not found at {METADATA_FILE_PATH}. Please ensure it exists.")
        return None
    try:
        with open(METADATA_FILE_PATH, "r") as f:
            metadata = json.load(f)
            logging.info(f"Metadata loaded successfully from {METADATA_FILE_PATH}")
            return metadata
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding JSON from {METADATA_FILE_PATH}: {e}", exc_info=True)
        st.error(f"Critical application error: Metadata file ({METADATA_FILE_PATH.name}) seems corrupted.")
        return None
    except Exception as e:
        logging.exception(f"Failed to load or parse metadata from {METADATA_FILE_PATH}: {e}")
        st.error("Critical application error: An unexpected error occurred while loading metadata.")
        return None

def render():
    """Renders the Add Expense page."""
    if "trigger_rerun" in st.session_state and time.time() > st.session_state["trigger_rerun"]:
        st.session_state.pop("trigger_rerun", None)
        st.rerun()

    st.subheader("Add New Expense")

    metadata = load_metadata()
    if metadata is None:
        return

    # Extract metadata components safely
    all_accounts = metadata.get("Account", [])
    category_map = metadata.get("categories", {})
    all_categories = sorted(list(category_map.keys()))
    user_map = metadata.get("User", {})

    if not all_accounts or not all_categories or not category_map or not user_map:
        st.error("Metadata structure is invalid or incomplete. Cannot proceed.")
        logging.error("Invalid metadata structure detected after loading.")
        return

    # --- Inputs outside the form ---
    expense_date = st.date_input("Date of Expense", value=datetime.date.today(), key="add_date")
    selected_category = st.selectbox("Category", options=all_categories, index=0, key="add_category")
    available_subcategories = sorted(category_map.get(selected_category, []))

    # --- Input Form ---
    with st.form("expense_form", clear_on_submit=True):
        # Use columns for side-by-side layout
        col1, col2 = st.columns(2)

        # --- Widgets in Columns ---
        # It's important that the order matches visually top-to-bottom
        with col1:
            selected_account = st.selectbox("Account", options=all_accounts, key="add_account")
            subcat_disabled = not bool(available_subcategories)
            selected_sub_category = st.selectbox(
                "Sub-category",
                options=available_subcategories,
                key="add_sub_category", # Key remains the same
                disabled=subcat_disabled,
                help="Select a sub-category if applicable." if not subcat_disabled else "No sub-categories for this category."
            )

        with col2:
            expense_type = st.text_input("Type (Description)", max_chars=60, key="add_type", help="Enter a brief description of the expense.")
            expense_amount = st.number_input("Amount (INR)", min_value=0.01, format="%.2f", step=10.0, key="add_amount") # Key remains the same

        # --- Form Submission Button ---
        submitted = st.form_submit_button("Add Expense")

        # --- Submission Logic ---
        if submitted:
            is_valid = True
            expense_user = user_map.get(selected_account, "Unknown") # Derive user here
            if not expense_type.strip():
                st.toast(" Please enter a Type/Description.", icon=""); is_valid = False
            if expense_amount <= 0:
                 st.toast(" Amount must be greater than zero.", icon=""); is_valid = False
            if available_subcategories and not selected_sub_category:
                st.toast(" Please select a Sub-category.", icon=""); is_valid = False

            if is_valid:
                final_sub_category = selected_sub_category if available_subcategories else ""
                dt = pd.to_datetime(expense_date)
                expense_data = {
                    "date": dt.strftime("%Y-%m-%d"), "year": dt.year,
                    "month": dt.to_period("M").strftime("%Y-%m"), "week": dt.strftime("%G-W%V"),
                    "day_of_week": dt.day_name(), "account": selected_account,
                    "category": selected_category, "sub_category": final_sub_category,
                    "type": expense_type.strip(), "user": expense_user, "amount": expense_amount
                }
                success = insert_expense(expense_data)
                if success:
                    st.toast(" Expense added successfully!", icon="")
                    st.session_state["last_added"] = expense_data
                    st.session_state["highlight_time"] = time.time()
                else:
                    st.toast(" Failed to save expense to the database.", icon="")

    # --- Display Recent Entries ---
    if "last_added" in st.session_state and "highlight_time" in st.session_state:
         # Check if highlight time has expired
         if time.time() - st.session_state["highlight_time"] <= 5:
              st.success("Entry saved successfully!") # Show success message briefly
         else:
              # Clear state after timeout
              st.session_state.pop("last_added", None)
              st.session_state.pop("highlight_time", None)

    st.markdown("---")
    st.subheader("Last 10 Expenses Added")
    try:
        df = fetch_last_expenses(10)
        if df.empty:
            st.info("No recent expenses recorded yet.")
        else:
            highlight_index = None
            last_added_data = st.session_state.get("last_added")
            highlight_start_time = st.session_state.get("highlight_time")

            if highlight_start_time and (time.time() - highlight_start_time > 5):
                 st.session_state.pop("last_added", None)
                 st.session_state.pop("highlight_time", None)
                 last_added_data = None

            if last_added_data:
                match = df[
                    (df["date"].dt.strftime('%Y-%m-%d') == last_added_data["date"]) &
                    (df["account"] == last_added_data["account"]) &
                    (df["category"] == last_added_data["category"]) &
                    (df["sub_category"].fillna("") == last_added_data["sub_category"]) &
                    (df["type"] == last_added_data["type"]) &
                    (df["user"] == last_added_data["user"]) &
                    (df["amount"].round(2) == round(float(last_added_data["amount"]), 2))
                ]
                if not match.empty:
                    highlight_index = match.index[0]

            display_df = df.drop(columns=["id", "year", "month", "week", "day_of_week"], errors="ignore").rename(columns={
                "date": "Date", "account": "Account", "category": "Category",
                "sub_category": "Sub Category", "type": "Type", "user": "User", "amount": "Amount (INR)"
            })

            def highlight_row_conditionally(row):
                is_highlighted = row.name == highlight_index
                return ['background-color: #d1ffd6' if is_highlighted else '' for _ in row]

            st.dataframe(
                display_df.style
                    .format({"Date": "{:%Y-%m-%d}", "Amount (INR)": "{:.2f}"})
                    .apply(highlight_row_conditionally, axis=1),
                use_container_width=True, height=380, hide_index=True
            )
    except Exception as e:
        logging.exception("Failed to display recent expenses table")
        st.error(f"Error loading recent expenses: {e}")

#PY File: assistant.py
@path: streamlit/tabs/assistant.py
@summary: Full Python code included below.
@code:
# streamlit/tabs/assistant_tab.py
import streamlit as st
import requests
import json
import plotly.io as pio
import pandas as pd # Keep for potential future use, even if not displaying df directly now
import logging

# Configure Logging (optional but good practice)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Configuration ---
# Make sure this matches the port where your uvicorn server is running
LANGSERVE_API_URL = "http://localhost:8000/assistant/invoke"

# --- Helper Functions ---
def call_assistant_api(query: str) -> dict | None:
    """Sends the query to the LangServe backend and returns the parsed response."""
    payload = {"input": {"original_query": query}}
    headers = {"Content-Type": "application/json", "Accept": "application/json"}
    try:
        logger.info(f"Sending request to API: {LANGSERVE_API_URL} with query: '{query}'")
        response = requests.post(LANGSERVE_API_URL, json=payload, headers=headers, timeout=120) # Increased timeout
        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)

        response_data = response.json()
        logger.info(f"API Raw Response: {response_data}") # Log the raw response

        # --- Extract the relevant 'output' part ---
        # LangServe wraps the graph output in an 'output' key
        if "output" in response_data:
            # Further extract based on AssistantOutput Pydantic model in server.py
            api_output = response_data.get("output", {})
            # Log extracted output for debugging
            logger.info(f"Extracted API Output: {api_output}")
            return api_output
        else:
            logger.error(f"API response missing 'output' key. Response: {response_data}")
            st.error("Received an unexpected response format from the assistant.")
            return None

    except requests.exceptions.RequestException as e:
        logger.error(f"API call failed: {e}", exc_info=True)
        st.error(f"Failed to connect to the assistant backend: {e}")
        return None
    except json.JSONDecodeError:
        logger.error(f"Failed to decode API JSON response: {response.text}")
        st.error("Received an invalid response from the assistant (not valid JSON).")
        return None
    except Exception as e:
         logger.error(f"An unexpected error occurred during API call: {e}", exc_info=True)
         st.error(f"An unexpected error occurred: {e}")
         return None


# --- Main Render Function ---
def render():
    """Renders the Assistant tab."""
    st.subheader(" Personal Finance Assistant")

    # --- Layout ---
    # Row 1: Chat Interface
    chat_col = st.container() # Use container for chat flow

    # Row 2: Visualization and Data (using columns)
    viz_col, data_col = st.columns([0.6, 0.4]) # Allocate 60% width to viz, 40% to data

    # --- Initialize Chat History ---
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Greetings! How can I help you with your finances today?"}
        ]

    # --- Display Prior Chat Messages ---
    with chat_col:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"]) # Display assistant/user text

                # Display chart/data associated with previous assistant messages
                if message["role"] == "assistant":
                    if "chart_json" in message and message["chart_json"]:
                        try:
                            chart_fig = pio.from_json(message["chart_json"])
                            # Use the viz_col from the *outer scope* to display charts below chat
                            with viz_col:
                                st.plotly_chart(chart_fig, use_container_width=True)
                        except Exception as e:
                            logger.error(f"Error rendering previous chart: {e}")
                            with viz_col:
                                st.warning("Could not render previous chart.")

                    if "sql_results_str" in message and message["sql_results_str"]:
                        # Use the data_col from the *outer scope*
                         with data_col:
                            with st.expander("View Raw Data"):
                                 st.text(message["sql_results_str"])


    # --- Chat Input ---
    if prompt := st.chat_input("Ask me about your expenses..."):
        logger.info(f"User input received: '{prompt}'")
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        # Display user message
        with chat_col:
            with st.chat_message("user"):
                st.markdown(prompt)

        # --- Call Backend API & Process Response ---
        with chat_col:
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("Thinking...") # Provide feedback

                # Call the backend
                api_response = call_assistant_api(prompt)

                if api_response:
                    final_response = api_response.get("final_response", "Sorry, I couldn't generate a response.")
                    chart_json = api_response.get("chart_json") # Can be None
                    sql_results_str = api_response.get("sql_results_str") # Can be None
                    error_msg = api_response.get("error") # Check for errors from the agent

                    if error_msg:
                        logger.error(f"Assistant API returned an error: {error_msg}")
                        final_response = f"An error occurred: {error_msg}" # Display error to user
                        message_placeholder.error(final_response) # Use error styling
                    else:
                        message_placeholder.markdown(final_response) # Display final text

                    # Store results with the message for potential redisplay if needed
                    assistant_message_data = {
                        "role": "assistant",
                        "content": final_response,
                        "chart_json": chart_json,
                        "sql_results_str": sql_results_str
                    }
                    st.session_state.messages.append(assistant_message_data)

                    # --- Display Chart and Data in Row 2 ---
                    # Clear previous row 2 content before displaying new results
                    with viz_col:
                        # st.empty() # Optional: Explicitly clear if needed
                        if chart_json:
                            try:
                                chart_fig = pio.from_json(chart_json)
                                st.plotly_chart(chart_fig, use_container_width=True)
                                logger.info("Chart displayed successfully.")
                            except Exception as e:
                                logger.error(f"Error rendering chart JSON: {e}")
                                st.warning("Could not display the generated chart.")
                        # else:
                        #     st.write("") # Keep the column, but empty if no chart

                    with data_col:
                        # st.empty() # Optional: Explicitly clear if needed
                        if sql_results_str and not error_msg : # Only show if no error and results exist
                            with st.expander("View Retrieved Data", expanded=False):
                                 st.text(sql_results_str)
                                 logger.info("SQL results string displayed.")
                        # else:
                        #     st.write("") # Keep the column, but empty if no data

                else:
                    # Handle API call failure (error already shown by call_assistant_api)
                     message_placeholder.error("Failed to get a response from the assistant.")
                     st.session_state.messages.append({
                         "role": "assistant",
                         "content": "Failed to get a response from the assistant."
                     })
                     # Clear row 2 as well
                     with viz_col: st.empty()
                     with data_col: st.empty()

#PY File: reports.py
@path: streamlit/tabs/reports.py
@summary: Full Python code included below.
@code:
# streamlit/tabs/reports.py
import streamlit as st
import pandas as pd
import datetime
import json
import logging
from typing import Dict, Any, Optional
# Assuming db_utils is importable from streamlit/
from db_utils import fetch_all_expenses, fetch_expense_by_id, update_expense, delete_expense
from pathlib import Path
import time # Keep for short sleep after successful edit/delete

# Define Metadata Path relative to the project root
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
METADATA_FILE_PATH = PROJECT_ROOT / "metadata" / "expense_metadata.json"

# Configure Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

@st.cache_data
def load_metadata() -> Optional[Dict[str, Any]]:
    """Loads metadata from the project's metadata directory."""
    if not METADATA_FILE_PATH.is_file():
        logging.error(f"Metadata file not found at: {METADATA_FILE_PATH}")
        st.error(f"Critical application error: Metadata configuration file not found at {METADATA_FILE_PATH}. Please ensure it exists.")
        return None
    try:
        with open(METADATA_FILE_PATH, "r") as f:
            metadata = json.load(f)
            logging.info(f"Metadata loaded successfully from {METADATA_FILE_PATH}")
            return metadata
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding JSON from {METADATA_FILE_PATH}: {e}", exc_info=True)
        st.error(f"Critical application error: Metadata file ({METADATA_FILE_PATH.name}) seems corrupted. Please check its format.")
        return None
    except Exception as e:
        logging.exception(f"Failed to load or parse metadata from {METADATA_FILE_PATH}: {e}")
        st.error("Critical application error: An unexpected error occurred while loading metadata.")
        return None

@st.cache_data
def convert_df_to_csv(df: pd.DataFrame) -> bytes:
    """Converts a DataFrame to CSV bytes."""
    try:
        if 'Date' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Date']):
             df_copy = df.copy()
             df_copy['Date'] = df_copy['Date'].dt.strftime('%Y-%m-%d')
             return df_copy.to_csv(index=False).encode("utf-8")
        else:
             return df.to_csv(index=False).encode("utf-8")
    except Exception as e:
        logging.error(f"CSV conversion failed: {e}")
        st.error("Failed to generate CSV data.")
        return b""

# ==============================================================================
# Main Rendering Function
# ==============================================================================
def render():
    """Renders the Reports page, handling view, edit, and delete modes."""
    st.session_state.setdefault("edit_mode", False)
    st.session_state.setdefault("delete_confirm", False)
    st.session_state.setdefault("selected_expense_id", None)
    st.session_state.setdefault("force_refresh", False)

    metadata = load_metadata()
    if metadata is None:
        return

    # ---  Handle Refresh Request at the Top ---
    # If flag is set from previous run (e.g., after edit/delete/button press)
    if st.session_state.get("force_refresh", False):
        st.session_state["force_refresh"] = False # Reset the flag immediately
        st.cache_data.clear() # Clear cache to ensure fresh data fetch
        # No explicit message needed, just let the page reload below
        # The rerun itself is triggered by button clicks or state changes that set the flag

    # --- Mode Handling ---
    if st.session_state.edit_mode:
        if st.session_state.selected_expense_id:
            expense = fetch_expense_by_id(st.session_state.selected_expense_id)
            if expense:
                display_edit_form(expense, metadata)
            else:
                st.error(f"Could not load expense with ID {st.session_state.selected_expense_id} to edit.")
                st.session_state.edit_mode = False
                st.session_state.selected_expense_id = None
                if st.button("Back to Report"): st.rerun()
            return

    elif st.session_state.delete_confirm:
        if st.session_state.selected_expense_id:
            expense = fetch_expense_by_id(st.session_state.selected_expense_id)
            if expense:
                display_delete_confirmation(expense)
            else:
                st.error(f"Could not load expense with ID {st.session_state.selected_expense_id} to delete.")
                st.session_state.delete_confirm = False
                st.session_state.selected_expense_id = None
                if st.button("Back to Report"): st.rerun()
            return

    # --- Default Mode: Render Report View ---
    render_report_view(metadata)

# ==============================================================================
# Report View Rendering Function
# ==============================================================================
def render_report_view(metadata: Dict[str, Any]):
    """Displays the main report view with filters and data table."""
    st.subheader("Expense Report")

    # --- Fetch Data ---
    # This fetch happens on initial load or after a rerun triggered by refresh/edit/delete
    df_all = fetch_all_expenses()

    if df_all.empty:
        st.info("No expense data available to display.")
        return

    # --- Prepare Data and Filter Options ---
    try:
        if not pd.api.types.is_datetime64_any_dtype(df_all['date']):
             df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')
             df_all.dropna(subset=['date'], inplace=True)

        if 'month' not in df_all.columns and 'date' in df_all.columns:
             df_all['month'] = df_all['date'].dt.strftime('%Y-%m')

        required_cols = ['date', 'month', 'account', 'category', 'sub_category', 'user', 'amount', 'id', 'type']
        if not all(col in df_all.columns for col in required_cols):
             missing = [col for col in required_cols if col not in df_all.columns]
             st.error(f"Database is missing required columns: {', '.join(missing)}. Cannot generate report.")
             logging.error(f"Missing columns in fetched data: {missing}")
             return

        all_months = ["All"] + sorted(df_all['month'].unique(), reverse=True)
        all_accounts = ["All"] + sorted(metadata.get("Account", []))
        all_categories = ["All"] + sorted(list(metadata.get("categories", {}).keys()))
        all_users = ["All"] + sorted(list(set(metadata.get("User", {}).values())))
        category_map = metadata.get("categories", {})
    except Exception as e:
         st.error(f"Error preparing data or filter options: {e}")
         logging.exception("Error during data preparation in reports tab.")
         return


    # --- Filter UI ---
    st.markdown("#### Filter Options")
    month_selected = st.selectbox(
        "Filter by Month", options=all_months, index=0, key="report_month_filter"
    )

    filter_col1, filter_col2 = st.columns(2)
    with filter_col1:
        accounts_selected = st.multiselect("Filter by Account(s)", options=all_accounts, default=["All"], key="report_account_filter")
        users_selected = st.multiselect("Filter by User(s)", options=all_users, default=["All"], key="report_user_filter")
    with filter_col2:
        categories_selected = st.multiselect("Filter by Category(s)", options=all_categories, default=["All"], key="report_category_filter")
        subcats_available = set()
        if "All" in categories_selected:
            for sublist in category_map.values(): subcats_available.update(sublist)
        else:
            for cat in categories_selected: subcats_available.update(category_map.get(cat, []))
        all_subcategories_options = ["All"] + sorted(list(subcats_available))
        subcategory_selected = st.selectbox(
            "Filter by Sub-category", options=all_subcategories_options, index=0, key="report_subcategory_filter",
            help="Available sub-categories depend on selected Categories."
        )

    # --- Apply Filters ---
    try:
        df_filtered = df_all.copy()
        if month_selected != "All": df_filtered = df_filtered[df_filtered['month'] == month_selected]
        if "All" not in accounts_selected: df_filtered = df_filtered[df_filtered['account'].isin(accounts_selected)]
        if "All" not in categories_selected: df_filtered = df_filtered[df_filtered['category'].isin(categories_selected)]
        if subcategory_selected != "All": df_filtered = df_filtered[df_filtered['sub_category'] == subcategory_selected]
        if "All" not in users_selected: df_filtered = df_filtered[df_filtered['user'].isin(users_selected)]
    except Exception as e:
         st.error(f"Error applying filters: {e}")
         logging.exception("Error occurred while filtering DataFrame.")
         df_filtered = pd.DataFrame()


    # --- Display Summary ---
    st.markdown("---")
    total_filtered_expense = df_filtered['amount'].sum() if not df_filtered.empty else 0
    st.markdown(f"### Total Expense (Filtered): {total_filtered_expense:,.2f}")

    if not df_filtered.empty:
        st.markdown("#### Summary Statistics (Filtered)")
        num_transactions = len(df_filtered)
        avg_transaction_amount = df_filtered['amount'].mean()
        top_category_series = df_filtered.groupby("category")["amount"].sum().nlargest(1)
        top_category_display = "N/A"
        if not top_category_series.empty:
             top_category_display = f"{top_category_series.index[0]} ({top_category_series.values[0]:,.0f})"
        stat_col1, stat_col2, stat_col3 = st.columns(3)
        stat_col1.metric("Transactions", f"{num_transactions:,}")
        stat_col2.metric("Avg. Transaction", f"{avg_transaction_amount:,.2f}")
        stat_col3.metric("Top Category", top_category_display)
    elif not df_all.empty:
        st.info("No transactions match the current filter criteria.")


    # --- Detailed Transactions Table ---
    st.markdown("---")
    col_title, col_refresh = st.columns([4, 1])
    with col_title:
         st.markdown("### Detailed Transactions (Filtered)")
    with col_refresh:
        # --- Refresh Button just sets the flag and triggers rerun ---
        if st.button(" Refresh Data", key="report_refresh_btn", help="Click to reload data from database"):
            st.session_state["force_refresh"] = True
            st.rerun() # Trigger rerun, flag will be checked at the top

    if not df_filtered.empty:
        display_columns = ["date", "account", "category", "sub_category", "type", "user", "amount"]
        existing_display_cols = [col for col in display_columns if col in df_filtered.columns]
        display_df = df_filtered[existing_display_cols + ['id']].copy()
        display_df = display_df.rename(columns={
            "date": "Date", "account": "Account", "category": "Category",
            "sub_category": "Sub Category", "type": "Type", "user": "User", "amount": "Amount (INR)"
        }).sort_values("Date", ascending=False)

        st.dataframe(
            display_df.drop(columns=['id']),
            column_config={
                 "Date": st.column_config.DateColumn("Date", format="YYYY-MM-DD"),
                 "Amount (INR)": st.column_config.NumberColumn("Amount (INR)", format="%.2f")
            },
            use_container_width=True, height=400, hide_index=True
        )

        # --- Edit / Delete Controls ---
        st.markdown("---")
        st.markdown("#### Edit / Delete Expense")
        df_selectable = display_df.copy().head(500)

        def create_display_label(row):
             date_str = row['Date'].strftime('%Y-%m-%d') if pd.notna(row['Date']) else 'N/A'
             amt_str = f"{row['Amount (INR)']:.0f}"
             return f"{date_str} | {row['Category']} | {row.get('Sub Category', '')[:15]} | {row.get('Type', '')[:20]} | {amt_str}"

        selector_map = {"-- Select expense to modify --": None}
        for idx, row in df_selectable.iterrows():
             label = create_display_label(row)
             unique_label = f"{label} (ID: ...{row['id'][-6:]})"
             selector_map[unique_label] = row['id']

        selected_label = st.selectbox("Select Expense", options=list(selector_map.keys()), key="report_select_expense")
        selected_id = selector_map.get(selected_label)

        edit_col, delete_col = st.columns([1, 1])
        edit_disabled = selected_id is None
        delete_disabled = selected_id is None
        with edit_col:
            if st.button("Edit Selected", key="report_edit_btn", disabled=edit_disabled):
                st.session_state.selected_expense_id = selected_id
                st.session_state.edit_mode = True
                st.rerun()
        with delete_col:
            if st.button("Delete Selected", key="report_delete_btn", disabled=delete_disabled):
                st.session_state.selected_expense_id = selected_id
                st.session_state.delete_confirm = True
                st.rerun()

        # --- CSV Download Button ---
        st.markdown("---")
        csv_export_df = display_df.drop(columns=['id'])
        csv_data = convert_df_to_csv(csv_export_df)
        if csv_data:
            st.download_button(
                label=" Download Filtered Data (.csv)", data=csv_data,
                file_name="filtered_expenses.csv", mime="text/csv", key="report_download_csv"
            )
    # No final else needed here

# ==============================================================================
# Edit Form Display Function
# ==============================================================================
def display_edit_form(expense_data: Dict[str, Any], metadata: Dict[str, Any]):
    """Displays the form for editing a selected expense with dynamic sub-categories and rearranged layout."""
    expense_id = expense_data.get("id", "UNKNOWN")
    expense_id_short = f"...{expense_id[-6:]}" if expense_id != "UNKNOWN" else "N/A"
    st.subheader(f"Edit Expense (ID: {expense_id_short})")

    all_categories = sorted(metadata.get("categories", {}).keys())
    all_accounts = metadata.get("Account", [])
    user_map = metadata.get("User", {})
    category_map = metadata.get("categories", {})

    # --- Session State Initialization (as before) ---
    session_key_category = f"edit_category_{expense_id}"
    session_key_subcat_options = f"edit_subcat_options_{expense_id}"
    session_key_subcat_index = f"edit_subcat_index_{expense_id}"
    if session_key_category not in st.session_state:
        st.session_state[session_key_category] = expense_data.get("category", all_categories[0] if all_categories else None)

    # --- Callback (as before) ---
    def category_change_callback():
        new_category = st.session_state[f"edit_category_widget_{expense_id}"]
        st.session_state[session_key_category] = new_category
        new_subcat_options = sorted(category_map.get(new_category, []))
        st.session_state[session_key_subcat_options] = new_subcat_options
        st.session_state[session_key_subcat_index] = 0 # Reset index on category change

    # --- Get current state (as before) ---
    current_edit_category = st.session_state[session_key_category]
    current_subcat_options = st.session_state.get(session_key_subcat_options, sorted(category_map.get(current_edit_category, [])))

    try:
        # --- Pre-populate initial values (as before) ---
        default_date = pd.to_datetime(expense_data["date"]).date()
        default_account_index = all_accounts.index(expense_data["account"]) if expense_data["account"] in all_accounts else 0
        initial_category_index = all_categories.index(current_edit_category) if current_edit_category in all_categories else 0
        initial_subcat = expense_data.get("sub_category", "")
        initial_subcat_index = 0
        if initial_subcat and initial_subcat in current_subcat_options:
             initial_subcat_index = current_subcat_options.index(initial_subcat)
        if session_key_subcat_index not in st.session_state:
             st.session_state[session_key_subcat_index] = initial_subcat_index
        default_type = expense_data.get("type", "")
        default_amount = float(expense_data.get("amount", 0.01))

        # --- Widgets ABOVE the Form ---
        st.markdown("---") # Separator
        # ---  Date Moved Here ---
        new_date_input = st.date_input("Date", value=default_date, key="edit_date_outside")
        # --- Category (triggers callback, stays outside form) ---
        st.selectbox(
            "Category",
            options=all_categories,
            index=initial_category_index,
            key=f"edit_category_widget_{expense_id}",
            on_change=category_change_callback
        )
        st.markdown("---") # Separator

        # --- Main Edit Form ---
        with st.form("edit_expense_form"):
            st.markdown("#### Modify Remaining Details")

            # ---  Row 1: Account & Type ---
            row1_col1, row1_col2 = st.columns(2)
            with row1_col1:
                new_account_input = st.selectbox( # Changed variable name
                    "Account",
                    options=all_accounts,
                    index=default_account_index,
                    key="edit_account"
                )
            with row1_col2:
                new_type_input = st.text_input( # Changed variable name
                    "Type",
                    value=default_type,
                    key="edit_type",
                    max_chars=60
                )

            # ---  Row 2: Sub-category & Amount ---
            row2_col1, row2_col2 = st.columns(2)
            with row2_col1:
                 subcat_disabled = not bool(current_subcat_options)
                 current_subcat_idx = st.session_state.get(session_key_subcat_index, 0)
                 if current_subcat_idx >= len(current_subcat_options): current_subcat_idx = 0
                 new_subcat_input = st.selectbox( # Changed variable name
                      "Sub-category",
                      options=current_subcat_options,
                      index=current_subcat_idx,
                      key="edit_subcat_widget",
                      disabled=subcat_disabled,
                      help="Options update based on Category selected above."
                 )
            with row2_col2:
                 new_amount_input = st.number_input( # Changed variable name
                     "Amount (INR)",
                     value=default_amount,
                     min_value=0.01,
                     format="%.2f",
                     key="edit_amount"
                 )

            # --- User Display (Optional, Placed After Grid) ---
            derived_user = user_map.get(new_account_input, "Unknown")
            st.text(f"User: {derived_user}") # Display derived user

            # --- Form Submission Buttons ---
            submit_col, cancel_col = st.columns([1, 1])
            with submit_col: save_changes = st.form_submit_button("Save Changes")
            with cancel_col: cancel_edit = st.form_submit_button("Cancel")

            # --- Submission Logic ---
            if save_changes:
                # --- Read final values from widgets/state ---
                final_category = st.session_state[session_key_category]
                final_subcat_options = sorted(category_map.get(final_category, []))
                final_subcat_selection = new_subcat_input # Read from widget
                final_date = new_date_input # Read from widget outside form
                final_account = new_account_input # Read from widget
                final_type = new_type_input # Read from widget
                final_amount = new_amount_input # Read from widget

                # Validation
                is_valid = True
                if not final_type.strip(): st.warning("Type cannot be empty."); is_valid = False
                if final_amount <= 0: st.warning("Amount must be positive."); is_valid = False
                if final_subcat_options and not final_subcat_selection:
                    st.warning(f"Sub-category required for '{final_category}'."); is_valid = False
                if final_subcat_selection and final_subcat_selection not in final_subcat_options:
                     st.warning(f"'{final_subcat_selection}' is not valid for '{final_category}'."); is_valid = False

                if is_valid:
                     final_dt = pd.to_datetime(final_date)
                     updated_data = {
                        "date": final_dt.strftime("%Y-%m-%d"), "year": final_dt.year,
                        "month": final_dt.strftime("%Y-%m"), "week": final_dt.strftime("%G-W%V"),
                        "day_of_week": final_dt.day_name(), "account": final_account,
                        "category": final_category,
                        "sub_category": final_subcat_selection if final_subcat_options else "",
                        "type": final_type.strip(), "user": derived_user, "amount": final_amount
                     }
                     success = update_expense(expense_data["id"], updated_data)
                     if success:
                        st.success("Expense updated successfully!")
                        # Clean up state
                        for key in [session_key_category, session_key_subcat_options, session_key_subcat_index, f"edit_category_widget_{expense_id}"]:
                            if key in st.session_state: del st.session_state[key]
                        st.session_state.edit_mode = False
                        st.session_state.selected_expense_id = None
                        st.session_state["force_refresh"] = True
                        time.sleep(0.5)
                        st.rerun()
                     else:
                         st.error("Failed to update expense in the database.")

            elif cancel_edit:
                 # Clean up state
                 for key in [session_key_category, session_key_subcat_options, session_key_subcat_index, f"edit_category_widget_{expense_id}"]:
                     if key in st.session_state: del st.session_state[key]
                 st.session_state.edit_mode = False
                 st.session_state.selected_expense_id = None
                 st.rerun()

    except (ValueError, IndexError, KeyError, TypeError) as e:
         st.error(f"Error preparing edit form: {e}. Data might be inconsistent or type mismatch.")
         logging.exception(f"Error preparing edit form for ID {expense_id}: {e}")
         if st.button("Back to Report"):
              st.session_state.edit_mode = False; st.session_state.selected_expense_id = None; st.rerun()

# ==============================================================================
# Delete Confirmation Display Function
# ==============================================================================
def display_delete_confirmation(expense_data: Dict[str, Any]):
    """Displays the confirmation dialog for deleting an expense."""
    st.subheader("Confirm Deletion")
    st.warning(f" Are you sure you want to permanently delete this expense?")

    details = {
        "Date": expense_data.get('date'), "Category": expense_data.get('category'),
        "Sub Category": expense_data.get('sub_category'), "Type": expense_data.get('type'),
        "Amount": f"{expense_data.get('amount', 0):,.2f}", "User": expense_data.get('user'),
        "Account": expense_data.get('account'), "ID": f"...{expense_data.get('id', '')[-6:]}"
    }
    st.json(details, expanded=True)

    confirm_col, cancel_col = st.columns(2)
    with confirm_col:
        if st.button("Yes, Delete Permanently", key="confirm_delete", type="primary"):
            success = delete_expense(expense_data["id"])
            if success:
                st.success("Expense deleted successfully.")
                st.session_state.delete_confirm = False
                st.session_state.selected_expense_id = None
                st.session_state["force_refresh"] = True # Trigger refresh
                time.sleep(0.5) # Brief pause
                st.rerun() # Rerun to show updated report
            else:
                 st.error("Failed to delete expense from the database.")
    with cancel_col:
        if st.button("Cancel", key="cancel_delete"):
            st.session_state.delete_confirm = False
            st.session_state.selected_expense_id = None
            st.rerun() # Go back to the report view

#PY File: visuals.py
@path: streamlit/tabs/visuals.py
@summary: Full Python code included below.
@code:
# streamlit/tabs/visuals.py
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json
import datetime
# Assuming db_utils is importable from streamlit/
from db_utils import fetch_all_expenses
from typing import Dict, Any, Optional
import logging
from pathlib import Path

# Define Metadata Path relative to the project root
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
METADATA_FILE_PATH = PROJECT_ROOT / "metadata" / "expense_metadata.json"

# Configure Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

@st.cache_data
def load_metadata() -> Optional[Dict[str, Any]]:
    """Loads metadata from the project's metadata directory."""
    if not METADATA_FILE_PATH.is_file():
        logging.error(f"Metadata file not found at: {METADATA_FILE_PATH}")
        st.error(f"Critical application error: Metadata configuration file not found at {METADATA_FILE_PATH}. Please ensure it exists.")
        return None
    try:
        with open(METADATA_FILE_PATH, "r") as f:
            metadata = json.load(f)
            logging.info(f"Metadata loaded successfully from {METADATA_FILE_PATH}")
            return metadata
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding JSON from {METADATA_FILE_PATH}: {e}", exc_info=True)
        st.error(f"Critical application error: Metadata file ({METADATA_FILE_PATH.name}) seems corrupted.")
        return None
    except Exception as e:
        logging.exception(f"Failed to load or parse metadata from {METADATA_FILE_PATH}: {e}")
        st.error("Critical application error: An unexpected error occurred while loading metadata.")
        return None

def get_common_layout_args(chart_title: str, show_legend: bool = False) -> Dict[str, Any]:
    """Generates common layout arguments for Plotly charts."""
    return {
        "title_text": chart_title,
        "title_font_size": 16, "title_x": 0.5,
        "margin": dict(l=20, r=20, t=50, b=80 if show_legend else 40),
        "legend": dict(orientation="h", yanchor="bottom", y=-0.3, xanchor="center", x=0.5),
        "hovermode": "closest",
        "showlegend": show_legend
    }

def render():
    """Renders the 'Visualizations' page with a 2x2 grid of charts."""
    st.subheader("Expense Visualizations")

    metadata = load_metadata()
    if metadata is None: return

    # --- Fetch Data ---
    df_all = fetch_all_expenses()
    if df_all.empty:
        st.info("No expense data available for visualization.")
        return

    # --- Prepare Data ---
    try:
        if not pd.api.types.is_datetime64_any_dtype(df_all['date']):
             df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')
             df_all.dropna(subset=['date'], inplace=True)

        if 'month' not in df_all.columns and 'date' in df_all.columns:
             df_all['month'] = df_all['date'].dt.strftime('%Y-%m') # Use 'month' consistently

        # Rename 'month' to 'YearMonth' for clarity if preferred, or just use 'month'
        if 'month' in df_all.columns and 'YearMonth' not in df_all.columns:
             df_all['YearMonth'] = df_all['month']

        # Check for required columns
        required_cols = ['YearMonth', 'category', 'amount', 'date', 'account', 'user', 'type', 'sub_category']
        if not all(col in df_all.columns for col in ['YearMonth', 'category', 'amount', 'date', 'account', 'user', 'type']):
             missing = [col for col in required_cols if col not in df_all.columns]
             st.error(f"Required columns missing for visualizations: {missing}")
             return

        min_date = df_all['date'].min().date()
        max_date = df_all['date'].max().date()
        all_months = ["All"] + sorted(df_all['YearMonth'].unique(), reverse=True)
        all_categories = ["All"] + sorted(list(metadata.get("categories", {}).keys()))
        all_users = ["All"] + sorted(list(set(metadata.get("User", {}).values())))
        all_accounts = ["All"] + sorted(metadata.get("Account", []))
    except Exception as e:
        st.error(f"Error preparing data or filter options: {e}")
        logging.exception("Error during data preparation in visuals tab.")
        return

    # --- Initialize Session State for Legends ---
    if 'legends' not in st.session_state:
        st.session_state.legends = {'pie': False, 'bar': False, 'line': False, 'top': False}

    # --- Layout for Charts ---
    st.markdown("#### Overview Charts")
    row1_col1, row1_col2 = st.columns(2)
    row2_col1, row2_col2 = st.columns(2)

    # --- Chart 1: Pie Chart ---
    with row1_col1:
        st.markdown("###### By Category (Proportion)")
        # ---  Updated Expander Label ---
        with st.expander("Pie Chart Filters", expanded=False):
            pie_month = st.selectbox("Month", all_months, 0, key="pie_month_filter")
            pie_cats = st.multiselect("Category", all_categories, ["All"], key="pie_cat_filter")
            pie_accounts = st.multiselect("Account", all_accounts, ["All"], key="pie_account_filter")
            pie_users = st.multiselect("User", all_users, ["All"], key="pie_user_filter")

        if st.button("Toggle Legend - Pie", key="pie_legend_btn"):
            st.session_state.legends['pie'] = not st.session_state.legends['pie']

        # Filter Data
        pie_df = df_all.copy()
        if pie_month != "All": pie_df = pie_df[pie_df['YearMonth'] == pie_month]
        if "All" not in pie_cats: pie_df = pie_df[pie_df['category'].isin(pie_cats)]
        if "All" not in pie_accounts: pie_df = pie_df[pie_df['account'].isin(pie_accounts)]
        if "All" not in pie_users: pie_df = pie_df[pie_df['user'].isin(pie_users)]

        # Aggregate and Plot
        pie_data = pie_df.groupby('category')['amount'].sum().reset_index()
        if not pie_data.empty and pie_data['amount'].sum() > 0:
            fig_pie = px.pie(pie_data, values='amount', names='category', hole=0.4)
            fig_pie.update_traces(textposition='inside', textinfo='percent+label', hoverinfo='label+percent+value')
            fig_pie.update_layout(**get_common_layout_args("Spending by Category", st.session_state.legends['pie']))
            st.plotly_chart(fig_pie, use_container_width=True)
        elif not pie_df.empty:
             st.info("No spending in selected categories/filters for Pie Chart.")
        else:
             st.info("No data matches filters for Pie Chart.")

    # --- Chart 2: Bar Chart ---
    with row1_col2:
        st.markdown("###### By Category (Absolute)")
        with st.expander("Bar Chart Filters", expanded=False):
            # ... (Filter widgets remain the same) ...
            bar_start = st.date_input("Start Date", min_date, key="bar_start_filter")
            bar_end = st.date_input("End Date", max_date, key="bar_end_filter")
            bar_accounts = st.multiselect("Account", all_accounts, ["All"], key="bar_account_filter")
            bar_users = st.multiselect("User", all_users, ["All"], key="bar_user_filter")


        if st.button("Toggle Legend - Bar", key="bar_legend_btn"):
            st.session_state.legends['bar'] = not st.session_state.legends['bar']

        # Filter Data (Remains the same)
        if bar_start > bar_end:
            st.warning("Start date cannot be after end date for Bar Chart.")
            bar_df = pd.DataFrame()
        else:
             bar_df = df_all[(df_all['date'].dt.date >= bar_start) & (df_all['date'].dt.date <= bar_end)]
             if "All" not in bar_accounts: bar_df = bar_df[bar_df['account'].isin(bar_accounts)]
             if "All" not in bar_users: bar_df = bar_df[bar_df['user'].isin(bar_users)]

        # Aggregate and Plot
        bar_data = bar_df.groupby('category')['amount'].sum().reset_index()
        if not bar_data.empty and bar_data['amount'].sum() > 0:
            fig_bar = px.bar(bar_data, x='category', y='amount', color='category', text_auto='.2s')

            # ---  Modify Layout Update ---
            layout_bar = get_common_layout_args("Total Spending by Category", st.session_state.legends['bar'])
            layout_bar["yaxis_title"] = "Amount (INR)"
            layout_bar["xaxis_title"] = "Category"
            layout_bar["xaxis"] = dict(
                categoryorder='total descending',
                tickangle=-90  # Force vertical labels
            )
            fig_bar.update_layout(**layout_bar)
            # --- End of Modification ---

            fig_bar.update_traces(textposition='outside')
            st.plotly_chart(fig_bar, use_container_width=True)
        elif not bar_df.empty:
             st.info("No spending in selected categories/filters for Bar Chart.")
        else:
             st.info("No data matches filters for Bar Chart (check dates?).")


    # --- Chart 3: Line Chart ---
    with row2_col1:
        st.markdown("###### Trend Over Time")
        # ---  Updated Expander Label ---
        with st.expander("Line Chart Filters", expanded=False):
            line_start = st.date_input("Start Date", min_date, key="line_start_filter")
            line_end = st.date_input("End Date", max_date, key="line_end_filter")
            line_cats = st.multiselect("Category", all_categories, ["All"], key="line_cat_filter")
            line_accounts = st.multiselect("Account", all_accounts, ["All"], key="line_account_filter")
            line_users = st.multiselect("User", all_users, ["All"], key="line_user_filter")
            line_mode = st.radio("View", ["Daily", "Cumulative"], 0, horizontal=True, key="line_mode_filter")

        if st.button("Toggle Legend - Line", key="line_legend_btn"):
            st.session_state.legends['line'] = not st.session_state.legends['line']

        # Filter Data
        if line_start > line_end:
             st.warning("Start date cannot be after end date for Line Chart.")
             line_df = pd.DataFrame()
        else:
            line_df = df_all[(df_all['date'].dt.date >= line_start) & (df_all['date'].dt.date <= line_end)]
            if "All" not in line_cats: line_df = line_df[line_df['category'].isin(line_cats)]
            if "All" not in line_accounts: line_df = line_df[line_df['account'].isin(line_accounts)]
            if "All" not in line_users: line_df = line_df[line_df['user'].isin(line_users)]

        # Aggregate and Plot
        trend_data = line_df.groupby('date')['amount'].sum().reset_index().sort_values('date')
        fig_line = go.Figure()
        trace_added = False
        if not trend_data.empty:
            if line_mode == "Daily":
                fig_line.add_trace(go.Scatter(x=trend_data['date'], y=trend_data['amount'], mode='lines+markers', name='Daily Spend'))
                trace_added = True
            elif line_mode == "Cumulative":
                trend_data['cumulative'] = trend_data['amount'].cumsum()
                fig_line.add_trace(go.Scatter(x=trend_data['date'], y=trend_data['cumulative'], mode='lines+markers', name='Cumulative Spend', line=dict(dash='dot')))
                trace_added = True

        if trace_added:
             layout_line = get_common_layout_args(f"{line_mode} Spending Trend", st.session_state.legends['line'])
             layout_line["yaxis_title"] = "Amount (INR)"
             layout_line["xaxis_title"] = "Date"
             layout_line["xaxis"] = dict(rangeslider=dict(visible=True), type="date")
             layout_line["hovermode"] = "x unified"
             fig_line.update_layout(**layout_line)
             st.plotly_chart(fig_line, use_container_width=True)
        elif not line_df.empty:
             st.info("No spending in selected categories/filters for Line Chart.")
        else:
             st.info("No data matches filters for Line Chart (check dates?).")

    # --- Chart 4: Top 10 Expense Types (Horizontal Bar) ---
    with row2_col2:
        st.markdown("###### Top 10 Expense Types")
        # ---  Updated Expander Label ---
        with st.expander("Top Expenses Filters", expanded=False): # Renamed for clarity
            top_start = st.date_input("Start Date", min_date, key="top_start_filter")
            top_end = st.date_input("End Date", max_date, key="top_end_filter")
            top_cats = st.multiselect("Category", all_categories, ["All"], key="top_cat_filter")
            top_accounts = st.multiselect("Account", all_accounts, ["All"], key="top_account_filter")
            top_users = st.multiselect("User", all_users, ["All"], key="top_user_filter")

        # Toggle Button (Optional, maybe less useful here)
        # if st.button("Toggle Legend##Top", key="top_legend_btn"):
        #    st.session_state.legends['top'] = not st.session_state.legends['top']

        # Filter Data
        if top_start > top_end:
             st.warning("Start date cannot be after end date for Top Expenses.")
             top_df = pd.DataFrame()
        else:
            top_df = df_all[(df_all['date'].dt.date >= top_start) & (df_all['date'].dt.date <= top_end)]
            if "All" not in top_cats: top_df = top_df[top_df['category'].isin(top_cats)]
            if "All" not in top_accounts: top_df = top_df[top_df['account'].isin(top_accounts)]
            if "All" not in top_users: top_df = top_df[top_df['user'].isin(top_users)]

        # Aggregate by 'Type' and get top 10
        if not top_df.empty and 'type' in top_df.columns:
             # Handle potential NaN/empty types before grouping
            top_df_cleaned = top_df.dropna(subset=['type'])
            top_df_cleaned = top_df_cleaned[top_df_cleaned['type'].str.strip() != '']
            if not top_df_cleaned.empty:
                top_data = top_df_cleaned.groupby('type')['amount'].sum().reset_index().nlargest(10, 'amount').sort_values('amount', ascending=True)
                if not top_data.empty:
                    fig_top = px.bar(top_data, y='type', x='amount', orientation='h', text='amount', color='type', color_discrete_sequence=px.colors.qualitative.Pastel) # Example color sequence
                    layout_top = get_common_layout_args("Top 10 Expense Types by Amount", show_legend=False)
                    layout_top["xaxis_title"] = "Total Amount (INR)"
                    layout_top["yaxis_title"] = ""
                    layout_top["yaxis"] = {'categoryorder':'total ascending'}
                    fig_top.update_layout(**layout_top)
                    fig_top.update_traces(texttemplate="%{x:,.0f}", textposition="outside")
                    st.plotly_chart(fig_top, use_container_width=True)
                else:
                     st.info("No spending data found for 'Type' aggregation with current filters.")
            else:
                 st.info("No valid 'Type' entries found after cleaning filters.")
        elif not top_df.empty:
             st.info("No 'type' column found or no data after filtering for Top Expenses.")
        else:
             st.info("No data matches filters for Top Expenses (check dates?).")
