INSTRUCTIONS FOR AI COLLABORATOR (AGENTIC AI PROJECT)

---------
INSTRUCTIONS FOR CREATING / UPDATING THIS FILE
1.  Goal of THIS DOCUMENT: To serve as the primary requirement tracker, development guideline, and context carrier for the Personal Finance Assistant (PFA) and DataWizard (DW) projects across development sessions. Guide on best practices (like Git workflow), but do not enforce rigidly.
2.  Update Focus: When updating, focus primarily on the current ongoing section (currently Section 4: PFA Phase 1.5). Ensure its status, goals, and next steps accurately reflect the end of the latest session. Other sections should remain relatively stable unless major strategic decisions are made. This section will always be the last project phase section.
3.  Detail Level: Explain concepts, requirements, and steps in as much detail as possible (DSBSI - Detailed Step-By-Step Instructions) to ensure context is carried over effectively. Assume the reader (me, the LLM, or you) might have forgotten specifics between sessions.
4.  Completeness: Include all relevant details discussed, even seemingly minor ones (e.g., specific file paths, configuration settings, workaround justifications, library choices, UI decisions, future considerations). Add anything relevant that might have been missed.
5.  Session Workflow: At the start of each session, you (Anirban) will upload the latest version of this document. At the end of each session, request an "end session" update, and I (the LLM) will generate the updated version incorporating the session's progress and decisions, focusing on updating Section 4.
6.  Scalability: This document will grow. That's expected. We will manage its structure as needed. Focus now is on comprehensive detail.

---------
ROADMAP
---------
1.  Personal Finance assistant - Phase 1 (Streamlit App) DONE
2.  Personal Finance assistant - Phase 1.5 (Agent Prototype Refactor & Prep) WE ARE HERE -> (Code modularization + Testing + Code Quality + UI refinements)
3.  Personal Finance assistant - Phase 2 (Multi-Agent Integration) NEXT -> (SA, DAA with RAG, DVA, DSA Implementation in Streamlit)
4.  Data Wizard - Phase 1 (Multi-Agent Foundation + React UI) FUTURE -> (Port PFA Phase 2 backend, build React UI foundation)
5.  Data Wizard - Phase 2 (Complex DS Workflows & RAG DAA) FUTURE -> (Implement advanced DSA, basic RAG DAA)
6.  Data Wizard - Phase 3 (Advanced RAG, Fine-tuning, UI Polish) FUTURE -> (Implement RAG re-ranking, fine-tuning, "Wow" factor UI)


(This roadmap clarifies the stepping stones towards the final DataWizard goal.)

---------
1. PROJECT REQUIREMENT - DATA WIZARD (DW): TARGET MILESTONES 1, 2, 3: FUTURE
---------

GOAL: The ultimate objective. An autonomous multi-agent data science assistant capable of performing complex analysis (EDA, Feature Engineering, Modeling, Evaluation) on large, multi-table relational datasets (e.g., AdventureWorksLT). The frontend will be a sophisticated, responsive single-page application built with React, designed to provide business users with an intuitive and engaging ("wow factor") experience. It will feature a three-part UI: (1) Interactive Chat, (2) Dynamic Visualizations, (3) Live Data Science Workflow/Output Streaming (showing steps like SQL, feature lists, model params, metrics).

1.1 Agentic AI Architecture:
*   A multi-agent system orchestrated using LangGraph.
*   Super Agent (SA): Manages user interaction, conversation history, task decomposition, routing to sub-agents, and final response aggregation.
*   Data Analyst Agent (DAA): Responsible for understanding the database schema, generating complex SQL queries (potentially involving multiple tables and joins), executing queries, and returning structured data.
    *   DW Phase 2 Goal: Implement RAG for schema understanding. Workflow: Chunk metadata (YAML/JSON) -> Generate Embeddings -> Perform Similarity Search between NER-extracted query entities and metadata chunks to identify relevant tables/columns -> Generate SQL using retrieved context.
    *   DW Phase 3 Goal: Enhance RAG with re-ranking (e.g., using Cohere ReRank) after similarity search to improve context relevance before SQL generation.
*   Data Scientist Agent (DSA): Handles advanced ML-based requests. Classifies task type (Regression, Forecasting, Classification, Segmentation, Clustering), requests data via DAA, executes the appropriate automated ML workflow, and returns results/summaries.
*   Data Visualization Agent (DVA): Generates appropriate visualizations (Plotly.js) based on data returned by DAA or DSA, potentially guided by SA/user query context.

1.2 Data Science Workflow for DSA:
*   Goal: Automate end-to-end workflows for the 5 ML categories (Reg, Fcst, Cls, Seg, Clus).
*   Recommended Approach: Start with a high-level AutoML library like PyCaret for its breadth and ease of implementation. If PyCaret proves insufficient for specific tasks (e.g., complex Forecasting, custom Segmentation), implement those workflows manually using standard libraries (Scikit-learn, Statsmodels, Sktime, etc.) creating a Hybrid Approach.
*   Tracking: Consider using MLflow programmatically to track experiments (parameters, metrics, artifacts) from both PyCaret and manual workflows, especially as complexity grows.
*   Output: DSA must provide results (predictions, assignments) and model summaries/metrics in a structured format (likely tabular/JSON) suitable for display in the React UI's "Process Output" section.

1.3 Backend:
*   Framework: Python backend using FastAPI to create API endpoints.
*   Agent Execution: LangGraph for defining and running the multi-agent workflows.
*   API Serving: LangServe for exposing the LangGraph agents via FastAPI.
*   Server: Uvicorn to run the FastAPI application.
*   LLMs: Primarily Google Gemini models (configurable mix of Pro and Flash versions depending on node requirements), potentially augmented by fine-tuned models (see 1.8) hosted externally (e.g., HF Inference Endpoints via HuggingFaceEndpoint) for specific tasks like Text-to-SQL.
*   Database: Connection to a relational database (see 1.5) via SQLAlchemy.

1.4 Frontend:
*   Framework: React.js (Recommended) or potentially Vue.js. Chosen for UI flexibility, large ecosystem, component libraries, and ability to create the desired "wow" factor.
*   Component Library: Material UI (MUI) or Ant Design (AntD) recommended for pre-built components and faster development.
*   UI Structure: Single-page application with three main interactive sections:
    *   Chat: For user queries and agent text responses. Needs state management (e.g., Zustand, Context API) for conversation history.
    *   Visualizations: Displaying Plotly.js charts generated by the backend (receiving Plotly JSON).
    *   Process Output: Dynamically displaying SQL queries, data snippets, feature lists, model parameters, metrics, etc., streamed or updated as the DSA executes its workflow. Needs careful design to handle diverse outputs clearly. Possible use of streaming endpoints from LangServe/FastAPI.
*   "Wow" Factor: Achieved through dynamic updates reflecting agent state/progress (visualizing active nodes/steps), real-time streaming of outputs (e.g., text generation, step completion), interactive visualizations (Plotly.js features), and a polished, professional look-and-feel.

1.5 Data and Metadata Strategy:
*   Database:
    *   Requirement: Multi-table relational dataset (e.g., AdventureWorksLT).
    *   Recommendation: While PostgreSQL/MySQL are standard for large deployments, **SQLite is a viable and significantly simpler option for DW development and local testing**, especially given potential setup constraints (no admin access on laptop). It handles multi-table schemas and avoids installation/service management hurdles. Performance should be adequate for single-user development; migration to PostgreSQL can be considered later if performance bottlenecks arise or deployment requires it. Use SQLAlchemy for database interaction, allowing easier switching if needed.
*   Metadata: Crucial for DAA's SQL generation.
    *   Format: Detailed YAML files (preferred for readability) or JSON describing tables, columns (name, type, detailed description), primary keys, foreign keys, and explicit relationship descriptions (how to join tables).
    *   Enhancements: Include table summaries, column value distributions/examples where helpful for LLM context.
    *   RAG Data Source: Chunked versions of this metadata will be embedded for DAA's similarity search.
*   Fine-tuning Data (for DW Phase 3): If fine-tuning Text-to-SQL models, training data needs to be in structured JSON or JSONL format, pairing schema representations, questions, and validated SQL queries. Store on HF Hub Datasets.

1.6 Development Considerations:
*   Complexity Management: Multi-agent systems require careful design of state, error handling, and control flow. LangGraph's state management and conditional edges are key.
*   Frontend Learning Curve: Significant effort required to learn JavaScript, React (or Vue), component lifecycle, state management, hooks, CSS, and frontend build tools (Node.js, npm/yarn, Vite/CRA).
*   Testing: Comprehensive testing (unit, integration, potentially E2E using Cypress/Playwright for React) is critical for reliability. Testing agent interactions and ML workflows needs specific strategies (mocking, validation).
*   Monitoring: LangSmith for agent tracing. MLflow for ML experiments. Frontend monitoring tools if deployed.
*   Cost: Factor in costs for cloud hosting (HF Inference Endpoints, cloud VMs/GPUs for training/hosting), powerful LLM APIs.

1.7 Business Considerations:
*   (Justification for Personal Project) Frame as a strategic skills development initiative, demonstrating proactive learning in AI/ML and full-stack development, directly enhancing capabilities applicable to future work projects. Highlight the use of personal time for foundational learning and the potential for reusable components or demonstrated patterns. Emphasize the project as a platform for mastering cutting-edge AI application techniques.

1.8 Fine-tuning Strategy (DW Phase 3):
*   Goal: Create a specialized Text-to-SQL model (e.g., Llama 3 8B) trained on AdventureWorksLT schema/metadata.
*   Platform: Hugging Face ecosystem (Hub, Spaces/Notebooks for training, Inference Endpoints for hosting). Avoid local training/hosting.
*   Data Prep: Generate high-quality Question-Schema-SQL pairs (manual + validated synthetic). Format for SFT (e.g., ChatML format). Store on HF Hub Datasets.
*   Technique: Start with PEFT (QLoRA) for efficiency using `trl`'s `SFTTrainer`. Use `bitsandbytes` for 4-bit quantization.
*   Evaluation: Focus on Execution Accuracy against a test DB. Use Exact Match and potentially LLM-as-Judge as secondary metrics.
*   Advanced: Consider DPO/RLHF using `trl`'s `DPOTrainer` and preference data if SFT is insufficient.
*   Integration: DAA node calls the deployed fine-tuned model via its API endpoint (e.g., HF Inference Endpoint using `HuggingFaceEndpoint`).

---------
2. PROJECT REQUIREMENT - PERSONAL FINANCE ASSISTANT (PFA) PHASE 1: TARGET MILESTONE 1 - DONE
---------

GOAL: A functional Streamlit application for personal expense tracking for two users (Anirban, Puspita). Allows manual expense entry, viewing reports with filtering, and basic visualizations. This phase did not involve any agentic AI components. The codebase serves as the foundation for Phase 1.5/2. Edit/Delete functionality was deferred.

2.1 Agentic AI: None.

2.2 Data Science workflow for DSA: None.

2.3 Backend: Not applicable (Direct Streamlit application logic). Database interaction handled via utility functions.

2.4 Frontend:
*   Framework: Streamlit.
*   UI Structure: Multi-tab application (main.py orchestrating tabs in tabs/ directory):
    *   Add Expenses Tab (add_expense.py): Input form (Date, Account, Category, Sub-category, Type, Amount; User derived from Account). Displays a table (st.dataframe) of the last 10 added expenses fetched via db_utils.fetch_last_expenses. Uses expense_metadata.json for dropdown population.
    *   Reports Tab (reports.py): Displays all expenses (db_utils.fetch_all_expenses) in a filterable table (st.dataframe). Filters (st.multiselect, st.selectbox) include Month, Account, Category, Sub-category, User ('All' option). Shows total expense (st.markdown) for the filtered selection. Filters are within an st.expander.
    *   Visualizations Tab (visuals.py): Displays two Plotly charts (st.plotly_chart) with filters within expanders:
        *   Pie Chart: Category distribution (px.pie). Filters: Month (single st.selectbox), Account, Category, User (multi-select).
        *   Line Chart: Daily expense trend (go.Figure with go.Scatter). Filters: Account, Category, User (multi-select). Includes options (st.radio) for daily/cumulative/both views.
*   Styling: Uses a custom styles.css file loaded via style_utils.load_css.

2.5 Data and Metadata Strategy:
*   Database: Single SQLite database (expenses.db) created from a CSV. db_utils.py handles connections (sqlite3) and data fetching (pd.read_sql). UUIDs are generated for new entries.
*   Metadata: Initial metadata stored in expense_metadata.json (defining Accounts, Categories, Sub-categories, User mapping). Used primarily for UI dropdowns in Phase 1. Phase 1.5 introduced metadata/expenses_metadata_detailed.yaml for agent use.
*   JSON vs YAML for Metadata: Keep using YAML (expenses_metadata_detailed.yaml) for detailed, human-readable schema descriptions for LLM context (agent prompts). Generate separate JSON/JSONL files formatted specifically for ML model fine-tuning only if/when that step is undertaken (likely for DW).

2.6 Development Considerations:
*   Modular structure using tabs/ directory for UI pages.
*   Utility modules for database (db_utils.py) and styling (style_utils.py).
*   Uses Pandas for data manipulation before displaying in Streamlit or Plotly.
*   Relative paths used for loading metadata/CSS.
*   Basic Streamlit state management (st.session_state).

---------
3. PROJECT REQUIREMENT - PERSONAL FINANCE ASSISTANT (PFA) PHASE 2: TARGET MILESTONE 2 - NEXT
---------

GOAL: Integrate an autonomous multi-agent Data Science Assistant into the existing PFA Streamlit application (on feature/agentic-ai branch). The assistant should handle both simple data retrieval and advanced analytical/ML queries (as defined in instruction_advanced_question_types.txt) related to the user's personal finance data in expenses.db, leveraging the architecture defined in requirement_v2_ds_assistant.txt.

3.1 Agentic AI:
*   Implement the multi-agent architecture using LangGraph: SA, DAA, DSA, DVA.
*   SA: Orchestrator. Receives user query from Streamlit UI, manages conversation history (session-based), classifies query (Simple, Advanced-ML, Irrelevant using LLM), potentially asks disambiguation questions, routes tasks to DAA/DSA based on classification using conditional edges, coordinates with DVA, formats final response for UI.
*   DAA: Receives requests from SA/DSA.
    *   MUST IMPLEMENT: RAG on metadata (`metadata/expenses_metadata_detailed.yaml`) for schema understanding. Workflow: Chunk metadata -> Generate Embeddings -> Perform Similarity Search between NER-extracted query entities (needs implementation) and metadata chunks to identify relevant context -> Generate SQL using retrieved context + LLM.
    *   Execute SQL (SQLAlchemy), validate results, return structured data (e.g., List[Dict]) to the caller.
*   DSA: Receives 'Advanced-ML' query from SA. Classifies it into one of the 5 ML types (Reg, Fcst, Cls, Seg, Clus - refer to instruction_advanced_question_types.txt) using LLM. Frames data request to DAA. Receives data. Executes the corresponding pre-defined ML workflow (using PyCaret/Hybrid approach). Generates tabular model output and tabular model summary. Returns results to SA.
*   DVA: Receives data (from DAA/DSA) and context (from SA/user query). Selects an appropriate chart type from a predefined list ({Vertical bar, Horizontal bar, Scatter, Histogram, Line, Bubble, Pie} - potentially using LLM for selection). Generates Plotly JSON with appropriate labels/titles (plotly.express or plotly.graph_objects). Returns chart JSON to SA. SA decides if a chart is needed.
*   Prompts: Requires carefully crafted prompts (likely in YAML files per node/agent) for each agent/node using best practices (CoT, few-shot where applicable).
*   State Management: Requires careful design of shared state (likely extending AgentState or using multiple state dicts) passed between agents in the LangGraph workflow.

3.2 Data Science Workflow for DSA:
*   Implement 5 distinct workflows corresponding to the ML categories defined in instruction_advanced_question_types.txt.
*   Approach: Start with PyCaret for automation. If needed, supplement or replace specific workflows with Manual (Scikit-learn, Statsmodels, etc.) + MLflow tracking (Hybrid approach).
*   Input: DataFrames derived from data provided by DAA (expenses.db content).
*   Output: Two main tabular outputs per task: (1) Model predictions/results (e.g., forecasts, classifications), (2) Model performance summary/metrics (e.g., accuracy, RMSE, cluster descriptions). Format for Streamlit display (e.g., st.dataframe, st.table).

3.3 Backend:
*   Stack: LangGraph, LangServe, FastAPI, Uvicorn.
*   LLM: Google Gemini (currently gemini-2.0-flash set as default in graph.py, should be easily configurable per agent/node).
*   Database: SQLAlchemy connects to expenses.db.
*   API: Exposes the Super Agent graph via LangServe on FastAPI. Will require more complex input/output models to handle multi-agent state and conversation history.

3.4 Frontend:
*   Framework: Existing Streamlit application (streamlit/).
*   UI: The 'Assistant' tab (streamlit/tabs/assistant.py) needs significant enhancement:
    *   Implement robust conversation history management (storing multiple turns in st.session_state.messages).
    *   Modify call_assistant_api to potentially send relevant history context to the backend SA.
    *   Refine the UI layout for displaying chat, viz, and SQL/DSA outputs clearly (Consider Alternative 1: Tabs/Expanders Below Each Message from UI/UX discussion).
    *   Add components to display tabular DSA outputs (st.dataframe or st.table).

3.5 Data and Metadata Strategy:
*   Database: expenses.db.
*   Metadata: Rely heavily on metadata/expenses_metadata_detailed.yaml for DAA's context. Implement RAG using this metadata for DAA in this phase.

3.6 Development Considerations:
*   Complexity: Managing state, control flow, and error propagation between multiple asynchronous agents in LangGraph.
*   Tracing: Extensive use of LangSmith is critical for debugging multi-agent interactions.
*   Error Handling: Implement specific error handling within each agent and for communication failures between agents. SA needs to manage failures gracefully.
*   Testing: Develop integration tests specifically for agent handoffs (e.g., SA -> DAA -> SA, SA -> DSA -> DAA -> DSA -> SA). Test DSA workflows with mock data. Implement RAG testing.
*   Performance: Monitor latency, especially with sequential agent calls and potential ML model execution time in DSA. RAG step adds latency.

---------
5. DEVELOPMENT REQUIREMENT - Guidelines Summary
---------

GOAL: To establish and follow robust software development lifecycle (SDLC) best practices throughout the PFA and DW projects, facilitating learning and development towards becoming a Full Stack AI Architect. This section summarizes the guidelines previously detailed in agentic_ai_guidelines.txt.

5.1 Key Practice Areas:

*   1. Modular Design & Separation of Concerns: Break down backend (agents, nodes, prompts, utils, server) and frontend (components, pages, services, state) into independent, logical units. (Status: Refactoring planned for PFA 1.5)
*   2. Testing Strategy & Automation: Implement unit tests (mocking externals like LLMs, DBs), integration tests (component/agent interaction, API endpoints), and potentially E2E tests later. Use pytest. Automate execution via CI/CD (GitHub Actions). Focus on testing logic around LLMs. (Status: Next Step - Setup & Implementation)
*   3. Git Workflow Best Practices: Use main for stable code. Develop on feature branches (feature/agentic-ai). Use atomic, descriptive commits (Conventional Commits). Use Pull Requests for merging. Maintain a comprehensive .gitignore. (Status: Branching/Commits ongoing, needs PRs & .gitignore review)
*   4. Coding Best Practices (Quality & Automation):
    *   Tools: Use Ruff (linting/formatting), Black (formatting), mypy (type checking). Configure in pyproject.toml. (Status: Next Step - Setup)
    *   Automation: Use pre-commit framework to run quality tools before commits. (Status: Next Step - Setup)
    *   Logging: Use Python's logging module with appropriate levels and contextual messages. Configure centrally. (Status: Basic usage exists, needs refinement)
    *   Docstrings: Write comprehensive docstrings (Google/NumPy style) for modules, classes, functions (Args, Returns, Raises). (Status: Next Step - Add/Improve)
    *   Configuration: Avoid hardcoding. Load secrets/configs from .env or config files centrally. (Status: Partially done, needs consolidation)
*   5. UI/UX Best Practices: Design for responsiveness (adapting to screen size). Provide clear user feedback (loading states, errors). Maintain consistency. Consider accessibility. Optimize performance. For DW (React), focus on visualizing agent state and streaming outputs for "wow" factor. (Status: Basic feedback exists in Streamlit, UI layout refinement discussed)

---------
6. LLM INSTRUCTIONS - Collaboration Guidelines
---------

GOAL: Define specific instructions for me, the LLM assistant, to ensure effective collaboration, consistent code generation, and adherence to project standards.

6.1 Instructions:

*   6.1.1. Understand Current Code: ALWAYS first gain a detailed understanding of the currently implemented code (based on uploaded files/context). Ensure generated code is compatible and integrates correctly. CRITICAL.
*   6.1.2. Library Compatibility: Ensure generated code is compatible with relevant library versions (e.g., langchain, langgraph, fastapi, pydantic, openai >1.0.0 if used). Note potential version conflicts.
*   6.1.3. Recommend Efficient Approaches: Critically evaluate requests. Recommend the most efficient approach (base Python, ML libs, GenAI, Agentic AI) for the task. Don't automatically follow suggestions; propose better alternatives first.
*   6.1.4. Full Code Blocks: Always generate complete code blocks for requested .py files or code modifications.
*   6.1.5. Code Explanation: Include detailed comments (#) within the code explaining logic, purpose, and complex sections.
*   6.1.6. Python Standards: Adhere to proper Python structure, use type hints (typing module) consistently, and follow PEP 8 guidelines (enforced by linters/formatters).
*   6.1.7. Dependency Checks: Assume required libraries are installed but mention any *new* dependencies needed for generated code.
*   6.1.8. Chart Quality: Ensure generated code for Plotly charts includes appropriate titles, axis labels, and legends.
*   6.1.9. Path Handling: Use pathlib and relative paths correctly (e.g., Path(__file__).parent / "...") based on the project structure.
*   6.1.10. Variable Usage: Avoid undefined variables or misuse of constants.
*   6.1.11. Data Validation: Use Pydantic for data validation where applicable (e.g., API request/response models, configuration).
*   6.1.12. File Output: Ensure files are created correctly with headers/expected structure, even if fallback data is used.
*   6.1.13. Directory/Path Validation: Verify all mentioned file paths and directory structures align with the provided instructions_project_structure.txt. Ensure input/output directories exist or are handled correctly.
*   6.1.14. Prompt File Referencing: Ensure code correctly references and loads prompts from their designated files (e.g., YAML files in prompts/).
*   6.1.15. LLM Choice: Use gpt-4o (or specified powerful model) primarily, falling back to gpt-3.5-turbo if necessary. Inform if fallback occurs.
*   6.1.16. DSBSI: Provide Detailed Step-By-Step Instructions for implementation tasks.

6.2 Required Documents for Context:

*At the start of each session, please ensure the following documents are provided or confirm their contents are up-to-date within instructions_agentic_ai.txt. Remind if any are missing.*

*   6.2.1. AGENTIC AI INSTRUCTIONS: instructions_agentic_ai.txt (This file)
*   6.2.2. PROJECT STRUCTURE: instructions_code_details.txt (File tree showing current project layout)
*   6.2.3. METADATA REFERENCE: instructions_metadata.txt (Content or reference to metadata/expenses_metadata_detailed.yaml and potentially expense_metadata.json)
*   6.2.4. DSA GUIDELINES: instructions_advanced_question_types.txt (Definitions and examples for DSA ML task classification - Needed for Phase 2/DW)

---------
4. PROJECT REQUIREMENT - PERSONAL FINANCE ASSISTANT (PFA) PHASE 1.5: BETWEEN TARGET MILESTONE 1 and 2 - ONGOING
---------
(This section reflects the status and immediate goals as of the end of the last session)

GOAL: Refine the working single-agent prototype (PFA Phase 1 + basic agent integration) to establish a solid foundation before implementing the full multi-agent architecture (Phase 2). Focus on code quality, modularity, testing, and minor UI/UX improvements. Ensure adherence to SDLC best practices outlined in Section 5.

Status: Single-agent prototype functional end-to-end on main branch. Development switched to feature/agentic-ai branch.

4.1 Agentic AI:
*   Current: Single LangGraph agent workflow defined in src/agent/graph.py using Google Gemini gemini-2.0-flash (switched from 1.5 Flash). Performs classification -> SQL generation -> SQL execution -> Chart generation (basic heuristics) -> Response generation. State managed in AgentState (src/agent/state.py), using List[Dict] for SQL results to ensure serializability.
*   Next Steps:
    *   Modularization: Refactor graph.py by moving node logic into separate files within src/agent/nodes/ (e.g., classify_query_node.py, generate_sql_node.py, etc.).
    *   Prompt Externalization: Move LLM prompts from node functions into corresponding YAML files within src/agent/prompts/ (e.g., classify_query_prompts.yaml). Implement logic in node files to load prompts from YAML.
    *   LLM Configuration: Make the Gemini model name easily configurable (e.g., load from .env or a config file) instead of hardcoding in graph.py.
    *   Refine Error Handling: Improve specificity and propagation of errors within the graph state.

4.2 Data Science workflow for DSA:
*   Current: Not applicable. The current single agent classifies 'simple'/'advanced' but treats 'advanced' the same as 'simple' (routes to SQL generation). No distinct ML workflows are executed.
*   Next Steps: No action in Phase 1.5. DSA implementation belongs to Phase 2.

4.3 Backend:
*   Current: FastAPI server (app/server.py) uses LangServe (add_routes) to expose the single agent graph at /assistant/invoke. Pydantic models (AssistantInput, AssistantOutput) define the API contract. Runs successfully using uvicorn app.server:app --reload.
*   Known Issue Workaround: The langgraph dev command error persists (TypeError: 'async for'...). Current workaround is using uvicorn directly.
*   Next Steps:
    *   Implement API-level integration tests (using httpx or pytest-fastapi-client) to verify the /assistant/invoke endpoint contract.

4.4 Frontend:
*   Current: Streamlit 'Assistant' tab (streamlit/tabs/assistant.py) connects to the backend API via requests. Displays chat history (st.session_state.messages), renders assistant text response (st.markdown), Plotly chart JSON (st.plotly_chart), and SQL results string (st.text within st.expander). Layout uses basic st.columns.
*   Next Steps:
    *   UI Layout Refinement: Consider implementing Alternative 1 (Expanders Below Each Message) from UI/UX discussion to improve correlation between chat messages and their outputs (chart/SQL). This involves modifying the chat rendering loop in assistant.py.
    *   Conversation History: Enhance assistant.py to properly store and display multiple turns of conversation history in st.session_state.messages. (Defer sending history to backend until SA implementation in Phase 2).

4.5 Data and Metadata Strategy:
*   Current: Uses expenses.db (SQLite) via SQLAlchemy (graph.py) and sqlite3 (db_utils.py). Uses metadata/expenses_metadata_detailed.yaml loaded in graph.py for LLM context during SQL generation. AgentState uses sql_results_list: List[Dict] for results.
*   Next Steps: No major changes planned for Phase 1.5.

4.6 Development Considerations:
*   Current Focus: Code refactoring (modularization), setting up testing infrastructure, implementing code quality tools, and minor Streamlit UI improvements.
*   Key Learnings Applied: Handling non-serializable state (DataFrames), correct prompt formatting for templates with variables, debugging LangServe/Streamlit interactions, LLM configuration, using uvicorn directly.
*   Git: Currently on feature/agentic-ai branch. Need to commit refactoring/testing changes incrementally.
*   Immediate Tasks:
    1.  Implement Modularization (nodes/, prompts/).
    2.  Set up Testing (pytest, mocks, basic unit/integration tests for current nodes/API).
    3.  Set up Code Quality (Ruff, Black/Ruff format, mypy, pre-commit).
    4.  Improve Docstrings/Logging.
    5.  Refine Streamlit UI Layout (e.g., Expanders).
    6.  Update .gitignore.